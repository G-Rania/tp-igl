{"took": 90, "timed_out": false, "_shards": {"total": 1, "successful": 1, "skipped": 0, "failed": 0}, "hits": {"total": {"value": 21, "relation": "eq"}, "max_score": 1.0, "hits": [{"_index": "articles", "_id": "JutqCI0BUzn7PtN-Fouu", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Advances in Engineering Software", "abstract": "The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0 , a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.", "authors": ["Paul Savesa", "R\u00e9mi Lafagea", "Nathalie Bartolia", "Youssef Diouanec", "Jasper Bussemakerd", "Thierry Lefebvrea", "John T. Hwange", "Joseph Morlierf", "Joaquim R.R.A. Martinsg"], "institutions": ["ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France", "ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, France", "Polytechnique Montr\u00e9al, Montreal, QC, Canada", "German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany", "University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA", "ICA, Universit\u00e9 de Toulouse, ISAE\u2013SUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France", "University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA"], "keywords": ["Surrogate modeling", "Gaussian process", "Kriging", "Hierarchical problems", "Hierarchical and mixed-categorical inputs", "Meta variables"], "full_text": "                                                              Advances in Engineering Software 188 (2024) 103571\n\n\n                                                                    Contents lists available at ScienceDirect\n\n\n                                                         Advances in Engineering Software\n                                                        journal homepage: www.elsevier.com/locate/advengsoft\n\n\nResearch paper\n\nSMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and\nmixed variables Gaussian processes\nPaul Saves a,b ,\u2217,1 , R\u00e9mi Lafage a ,1 , Nathalie Bartoli a ,1 , Youssef Diouane c ,1 , Jasper Bussemaker d ,1 ,\nThierry Lefebvre a ,1 , John T. Hwang e ,1 , Joseph Morlier f ,1 , Joaquim R.R.A. Martins g ,1\na\n    ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France\nb ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, France\nc Polytechnique Montr\u00e9al, Montreal, QC, Canada\nd German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany\ne\n    University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA\nf\n    ICA, Universit\u00e9 de Toulouse, ISAE\u2013SUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France\ng\n    University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA\n\n\n\nARTICLE                  INFO                              ABSTRACT\n\nDataset link: https://colab.research.google.com            The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate\n/github/SMTorg/smt/blob/master/tutorial/No                 modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major\ntebookRunTestCases_Paper_SMT_v2.ipynb                      new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds\nKeywords:\n                                                           the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables\nSurrogate modeling                                         are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT\nGaussian process                                           by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives\nKriging                                                    for Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of\nHierarchical problems                                      our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical\nHierarchical and mixed-categorical inputs                  and mixed inputs. This open-source software is distributed under the New BSD license.2\nMeta variables\n\n\n\n1. Motivation and significance                                                                  Kriging models (also known as Gaussian processes) that take advantage\n                                                                                                of derivative information are one of SMT\u2019s key features [6]. Numerical\n    With the increasing complexity and accuracy of numerical models, it                         experiments have shown that SMT achieved lower prediction error\nhas become more challenging to run complex simulations and computer                             and computational cost than Scikit-learn [7] and UQLab [8] for a\ncodes [1,2]. As a consequence, surrogate models have been recognized                            fixed number of points [9]. SMT has been applied to rocket engine\nas a key tool for engineering tasks such as design space exploration,                           coaxial-injector optimization [10], aircraft engine consumption mod-\nuncertainty quantification, and optimization [3]. In practice, surrogate                        eling [11], numerical integration [12], multi-fidelity sensitivity analy-\nmodels are used to reduce the computational effort of these tasks by                            sis [13], high-order robust finite elements methods [14,15], planning\nreplacing expensive numerical simulations with closed-form approxi-                             for photovoltaic solar energy [16], wind turbines design optimiza-\nmations [4, Ch. 10]. To build such a model, we start by evaluating                              tion [17], porous material optimization for a high pressure turbine\nthe original expensive simulation at a set of points through a Design                           vane [18], chemical process design [19] and many other applications.\nof Experiments (DoE). Then, the corresponding evaluations are used to                               In systems engineering, architecture-level choices significantly in-\nbuild the surrogate model according to the chosen approximation, such                           fluence the final system performance, and therefore, it is desirable to\nas Kriging, quadratic interpolation, or least squares regression.                               consider such choices in the early design phases [20]. Architectural\n    The Surrogate Modeling Toolbox (SMT) is an open-source frame-                               choices are parameterized with discrete design variables; examples in-\nwork that provides functions to efficiently build surrogate models [5].                         clude the selection of technologies, materials, component connections,\n\n\n     \u2217 Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France.\n     E-mail addresses: paul.saves@onera.fr (P. Saves), remi.lafage@onera.fr (R. Lafage), nathalie.bartoli@onera.fr (N. Bartoli), youssef.diouane@polymtl.ca\n(Y. Diouane), jasper.bussemaker@dlr.de (J. Bussemaker), thierry.lefebvre@onera.fr (T. Lefebvre), jhwang@eng.ucsd.edu (J.T. Hwang),\njoseph.morlier@isae-supaero.fr (J. Morlier), jrram@umich.edu (J.R.R.A. Martins).\n   1\n     All authors contributed to this work, research and manuscript.\n   2\n     https://github.com/SMTorg/SMT\n\nhttps://doi.org/10.1016/j.advengsoft.2023.103571\nReceived 22 August 2023; Received in revised form 23 October 2023; Accepted 26 November 2023\nAvailable online 7 December 2023\n0965-9978/\u00a9 2023 Elsevier Ltd. All rights reserved.\n\fP. Saves et al.                                                                                                        Advances in Engineering Software 188 (2024) 103571\n\nTable 1\nComparison of software packages for hierarchical and mixed Kriging models. \u2713= implemented. * = user-defined.\n Package                  BOTorch            Dakota            DiceKriging              KerGP           LVGP       Parmoo             Spearmint             SMT 2.0\n Reference                [25]               [26]              [27]                     [32]            [28]       [29]               [30]                  This paper\n License                  MIT                EPL               GPL                      GPL             GPL        BSD                GNU                   BSD\n Language                 Python             C                 R                        R               R          Python             Python                Python\n Mixed var.               \u2713                  \u2713                 \u2713                        \u2713               \u2713          \u2713                  \u2713                     \u2713\n GD kernel                \u2713                  \u2713                 \u2713                        *                                                                   \u2713\n CR kernel                                                                                              \u2713          \u2713                  \u2713                     \u2713\n HH kernel                                                                              \u2713                                                                   \u2713\n EHH kernel                                                                             *                                                                   \u2713\n Hierarchical var.                                                                                                                                          \u2713\n\n\n\nand number of instantiated elements. When design problems include                       optimization (BO) with hierarchical and mixed variables or noisy co-\nboth discrete variables and continuous variables, they are said to have                 Kriging that have been successfully applied to aircraft design [39], data\nmixed variables.                                                                        fusion [40], and structural design [41]. The SMT 2.0 interface is more\n    When architectural choices lead to different sets of design variables,              user-friendly and offers an improved and more detailed documentation\nwe have hierarchical variables [21,22]. For example, consider differ-                   for users and developers.3 SMT 2.0 is hosted publicly4 and can be\nent aircraft propulsion architectures [23]. A conventional gas turbine                  directly imported within Python scripts. It is released under the New\nwould not require a variable to represent a choice in the electrical                    BSD License and runs on Linux, MacOS, and Windows operating sys-\npower source, while hybrid or pure electric propulsion would require                    tems. Regression tests are run automatically for each operating system\nsuch a variable. The relationship between the choices and the sets of                   whenever a change is committed to the repository. In short, SMT 2.0\nvariables can be represented by a hierarchy.                                            builds on the strengths of the original SMT package while adding new\n    Handling hierarchical and mixed variables requires specialized sur-                 features. On one hand, the emphasis on derivatives (including predic-\nrogate modeling techniques [24]. To address these needs, SMT 2.0\n                                                                                        tion, training and output derivatives) is maintained and improved in\nis offering researchers and practitioners a collection of cutting-edge\n                                                                                        SMT 2.0. On the other hand, this new release includes support for\ntools to build surrogate models with continuous, mixed and hierarchical\n                                                                                        hierarchical and mixed variables Kriging based models. For the sake\nvariables. The main objective of this paper is to detail the new enhance-\n                                                                                        of reproducibility, an open-source notebook is available that gathers\nments that have been added in this release compared to the original\n                                                                                        all the methods and results presented on this paper.5\nSMT 0.2 release [5].\n    There are two new major capabilities in SMT 2.0: the ability                            The remainder of the paper is organized as follows. First, we in-\nto build surrogate models involving mixed variables and the support                     troduce the organization and the main implemented features of the\nfor hierarchical variables within Kriging models. To handle mixed                       release in Section 2. Then, we describe the mixed-variable Kriging\nvariables in Kriging models, existing libraries such as BoTorch [25],                   model with an example in Section 3. Similarly, we describe and provide\nDakota [26], DiceKriging [27], LVGP [28], Parmoo [29], and Spearmint                    an example for a hierarchical-variable Kriging model in Section 4.\n[30] implement simple mixed models by using either continuous relax-                    The Bayesian optimization models and applications are described in\nation (CR), also known as one-hot encoding [30], or a Gower distance                    Section 5. Finally, we describe the other relevant contributions in\n(GD) based correlation kernel [31]. KerGP [32] (developed in R) imple-                  Section 6 and conclude in Section 7.\nments more general kernels but there is no Python open-source toolbox\nthat implements more general kernels to deal with mixed variables,\nsuch as the homoscedastic hypersphere (HH) [33] and exponential                         2. SMT 2.0 : an improved surrogate modeling toolbox\nhomoscedastic hypersphere (EHH) [34] kernels. Such kernels require\nthe tuning of a large number of hyperparameters but lead to more                            From a software point of view, SMT 2.0 maintains and improves\naccurate Kriging surrogates than simpler mixed kernels [34,35]. SMT                     the modularity and generality of the original SMT version [5]. In this\n2.0 implements all these kernels (CR, GD, HH, and EHH) through a                        section, we describe the software as follows. Section 2.1 describes the\nunified framework and implementation. To handle hierarchical vari-                      legacy of SMT 0.2. Then, Section 2.2 describes the organization of the\nables, no library in the literature can build peculiar surrogate models                 repository. Finally, Section 2.3 shows the new capabilities implemented\nexcept SMT 2.0, which implements two Kriging methods for these\n                                                                                        in the SMT 2.0 update.\nvariables. Notwithstanding, most softwares are compatible with a na\u00efve\nstrategy called the imputation method [24] but this method lacks depth\nand depends on arbitrary choices. This is why Hutter and Osborne                        2.1. Background on SMT former version: SMT 0.2\n[21] proposed a first kernel, called Arc-Kernel which in turn was\ngeneralized by Horn et al. [36] with a new kernel called the Wedge-\nKernel [37]. None of these kernels are available in any open-source                        SMT [5] is an open-source collaborative work originally developed\nmodeling software. Furthermore, thanks to the framework introduced                      by ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of\nin Audet et al. [38], our proposed kernels are sufficiently general so                  Michigan. Now, both Polytechnique Montr\u00e9al and the University of\nthat all existing hierarchical kernels are included within it. Section 4                California San Diego are also contributors. SMT 2.0 updates and ex-\ndescribes the two kernels implemented in SMT 2.0 that are referred                      tends the original SMT repository capabilities among which the original\nas SMT Arc-Kernel and SMT Alg-Kernel. In particular, Alg-                               publication [5] focuses on different types of derivatives for surrogate\nKernel is a novel hierarchical kernel introduced in this paper. Table 1                 models detailed hereafter.\noutlines the main features of the state-of-the-art modeling software that\ncan handle hierarchical and mixed variables.\n    SMT 2.0 introduces other enhancements, such as additional sam-                          3\n                                                                                            http://smt.readthedocs.io/en/latest\npling procedures, new surrogate models, new Kriging kernels (and their                      4\n                                                                                            https://github.com/SMTorg/smt\nderivatives), Kriging variance derivatives, and an adaptive criterion for                 5\n                                                                                            https://github.com/SMTorg/smt/tree/master/tutorial/\nhigh-dimensional problems. SMT 2.0 adds applications of Bayesian                        NotebookRunTestCases_Paper_SMT_v2.ipynb\n\n\n                                                                                    2\n\fP. Saves et al.                                                                                                               Advances in Engineering Software 188 (2024) 103571\n\nTable 2                                                                                         highlighted in blue and detailed on Fig. 1. The new major features\nImpact of using Numba on training time of the hierarchical Goldstein problem. Speedup\n                                                                                                implemented in SMT 2.0 are highlighted in lavender whereas the\nis calculated excluding the JIT compilation table, as this step is only needed once after\nSMT installation.                                                                               legacy features that were already in present in the original publication\n Training set       Without numba           Numba         Speedup         JIT overhead          for SMT 0.2 [5] are in black.\n 15 points          1.3 s                   1.1 s         15%             24 s\n 150 points         38 s                    7.4 s         80%             23 s\n                                                                                                2.3. New features within SMT 2.0\n\n\n                                                                                                    The main objective of this new release is to enable Kriging surrogate\nA Python surrogate modeling framework with derivatives. One of the                              models for use with both hierarchical and mixed variables. Moreover,\noriginal main motivations for SMT was derivative support. In fact, none\n                                                                                                for each of these five sub-modules described in Section 2.2, several\nof the existing packages for surrogate modeling such as Scikit-learn in\n                                                                                                improvements have been made between the original version and the\nPython [7], SUMO in Matlab [42] or GPML in Matlab and Octave [43]\n                                                                                                SMT 2.0 release.\nfocuses on derivatives. Three types of derivatives are distinguished:\nprediction derivatives, training derivatives, and output derivatives.                           Hierarchical and mixed design space. A new design space definition\nSMT also includes new models with derivatives such as Kriging with                              class DesignSpace has been added that implements hierarchical\nPartial Least Squares (KPLS) [44] and Regularized Minimal-energy                                and mixed functionalities. Design variables can either be continu-\nTensor-product Spline (RMTS) [3]. These developed derivatives were                              ous (FloatVariable), ordered (OrdinalVariable) or categorical\neven used in a novel algorithm called Gradient-Enhanced Kriging with\n                                                                                                (CategoricalVariable). The integer type (IntegerVariable) rep-\nPartial Least Squares (GEKPLS) [6] to use with adjoint methods, for\n                                                                                                resents a special case of the ordered variable, specified by bounds\nexample [45].\n                                                                                                (inclusive) rather than a list of possible values. The hierarchical struc-\nSoftware architecture, documentation, and automatic testing. SMT is orga-                       ture of the design space can be defined using declare_decreed_var:\nnized along three main sub-modules that implement a set of sampling                             this function declares that a variable is a decreed variable that is\ntechniques (sampling_methods), benchmarking functions (problems),                               activated when the associated meta variable takes one of a set of\nand surrogate modeling techniques (surrogate_models). The toolbox                               specified values, see Section 4 for background. The DesignSpace\ndocumentation6 is created using reStructuredText and Sphinx, a doc-                             class also implements mechanisms for sampling valid design vectors\numentation generation package for Python, with custom extensions.                               (i.e. design vectors that adhere to the hierarchical structure of the\nCode snippets in the documentation pages are taken directly from                                design space) using any of the below-mentioned samplers, for cor-\nactual tests in the source code and are automatically updated. The                              recting and imputing design vectors, and for requesting which design\noutput from these code snippets and tables of options are generated\n                                                                                                variables are acting in a given design vector. Correction ensures that\ndynamically by custom Sphinx extensions. This leads to high-quality\n                                                                                                variables have valid values (e.g. integers for discrete variables) [24],\ndocumentation with minimal effort. Along with user documentation,\n                                                                                                and imputation replaces non-acting variables by some default value\ndeveloper documentation is also provided to explain how to contribute\n                                                                                                (0 for discrete variables, mid-way between the bounds for continuous\nto SMT. This includes a list of API methods for the SurrogateModel,\n                                                                                                variables in SMT 2.0) [47].\nSamplingMethod, and Problem classes, that must be implemented\nto create a new surrogate modeling method, sampling technique, or                               Sampling. SMT implements three methods for sampling. The first one\nbenchmarking problem. When a developer submits a pull request, it is                            is a na\u00efve approach, called Random that draws uniformly points along\nmerged only after passing the automated tests and receiving approval\n                                                                                                every dimension. The second sampling method is called Full Fac-\nfrom at least one reviewer. The repository on GitHub7 is linked to\n                                                                                                torial and draws a point for every cross combination of variables,\ncontinuous integration tests (GitHub Actions) for Windows, Linux and\n                                                                                                to have an \u2018\u2018exhaustive\u2019\u2019 design of experiments. The last one is the\nMacOS, to a coverage test on coveralls.io and to a dependency version\ncheck for Python with DependaBot. Various parts of the source code\n                                                                                                Latin Hypercube Sampling (LHS) [48] that draws a point in\n                                                                                                every Latin square parameterized by a certain criterion. For LHS, a\nhave been accelerated using Numba [46], an LLVM-based just-in-time\n(JIT) compiler for numpy-heavy Python code. Numba is applied to con-                            new criterion to manage the randomness has been implemented and\nventional Python code using function decorators, thereby minimizing                             the sampling method was adapted for multi-fidelity and mixed or\nits impact on the development process and not requiring an additional                           hierarchical variables. More details about the new sampling techniques\nbuild step. For a mixed Kriging surrogate with 150 training points, a                           are given in Section 6.1.\nspeedup of up to 80% is observed, see Table 2. The JIT compilation\n                                                                                                Problems. SMT implements two new engineering problems: a mixed\nstep only needs to be done once when installing or upgrading SMT\n                                                                                                variant of a cantilever beam described in Section 3 and a hierarchical\nand adds an overhead of approximately 24 s on a typical workstation\nIn this paper, all results are obtained using an Intel\u00ae Xeon\u00ae CPU                               neural network described in Section 4.\nE5-2650 v4 @ 2.20 GHz core and 128 GB of memory with a Broadwell-                               Surrogate models. In order to keep up with state-of-art, several re-\ngeneration processor front-end and a compute node of a peak power of\n                                                                                                leases done from the original version developed new options for the\n844 GFlops.\n                                                                                                already existing surrogates. In particular, compared to the original\n                                                                                                publication [5], SMT 2.0 adds gradient-enhanced neural networks [45]\n2.2. Organization of SMT 2.0\n                                                                                                and marginal Gaussian process [49] models to the list of available\n                                                                                                surrogates. More details about the new models are given in Section 6.2.\n   The main features of the open-source repository SMT 2.0 are\ndescribed in Fig. 1. More precisely, Sampling Methods, Problems                                 Applications. Several applications have been added to the toolbox to\nand Surrogate models are kept from SMT 0.2 and two new                                          demonstrate the surrogate models capabilities. The most relevant ap-\nsections Models applications and Interactive notebooks\n                                                                                                plication is efficient global optimization (EGO), a Bayesian optimiza-\nhave been added to the architecture of the code. These sections are\n                                                                                                tion algorithm [50,51]. EGO optimizes expensive-to-evaluate black-box\n                                                                                                problems with a chosen surrogate model and a chosen optimization\n  6\n      https://smt.readthedocs.org                                                               criterion [52]. The usage of EGO with hierarchical and mixed variables\n  7\n      https://github.com/SMTorg/smt                                                             is described in Section 5.\n\n                                                                                            3\n\fP. Saves et al.                                                                                                          Advances in Engineering Software 188 (2024) 103571\n\n\n\n\n              Fig. 1. Functionalities of SMT 2.0. The new major features implemented in SMT 2.0 compared to SMT 0.2 are highlighted with the lavender color.\n\n\n\n\nInteractive notebooks. These tutorials introduce and explain how to use                 3. Surrogate models with mixed variables in SMT 2.0\nthe toolbox for different surrogate models and applications.8 Every\ntutorial is available both as a .ipynb file and directly on Google                          As mentioned in Section 1, design variables can be either of continu-\ncolab.9 In particular, a hierarchical and mixed variables dedicated                     ous or discrete type, and a problem with both types is a mixed-variable\nnotebook is available to reproduce the results presented on this paper.10               problem. Discrete variables can be ordinal or categorical. A discrete\n    In the following, Section 3 details the Kriging based surrogate                     variable is ordinal if there is an order relation within the set of possible\nmodels for mixed variables, and Section 4 presents our new Kriging                      values. An example of an ordinal design variable is the number of\nsurrogate for hierarchical variables. Section 5 details the EGO applica-                engines in an aircraft. A possible set of values in this case could be\ntion and the other new relevant features aforementioned are described                   2, 4, 8. A discrete variable is categorical if no order relation is known\nsuccinctly in Section 6.                                                                between the possible choices the variable can take. One example of a\n                                                                                        categorical variable is the color of a surface. A possible example of a\n                                                                                        set of choices could be blue, red, green. The possible choices are called\n  8\n    https://github.com/SMTorg/smt/tree/master/tutorial                                  the levels of the variable.\n  9\n    https://colab.research.google.com/github/SMTorg/smt/                                    Several methods have been proposed to address the recent increase\n 10\n    https://github.com/SMTorg/smt/tree/master/tutorial/                                 interest in mixed Kriging based models [30\u201333,35,39,53,54]. The main\nNotebookRunTestCases_Paper_SMT_v2.ipynb                                                 difference from a continuous Kriging model is in the estimation of\n\n                                                                                    4\n\fP. Saves et al.                                                                                                                                 Advances in Engineering Software 188 (2024) 103571\n\nTable 3\nCategorical kernels implemented in SMT 2.0.\n  Name                                \ud835\udf05(\ud835\udf19)                                  \ud835\udef7(\ud835\udee9\ud835\udc56 )                                                                                         # of hyperparam.\n  SMT GD                              exp(\u2212\ud835\udf19)                               [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57,\ud835\udc57 \u2236= 12 \ud835\udf03\ud835\udc56 ; [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57\u2260\ud835\udc57 \u2032 \u2236= 0                                                      1\n  SMT CR                              exp(\u2212\ud835\udf19)                               [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57,\ud835\udc57 \u2236= [\ud835\udee9\ud835\udc56 ]\ud835\udc57,\ud835\udc57 ; [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57\u2260\ud835\udc57 \u2032 \u2236= 0                                                   \ud835\udc3f\ud835\udc56\n                                                                                                                   log \ud835\udf16                                                   1\n  SMT EHH                             exp(\u2212\ud835\udf19)                               [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57,\ud835\udc57 \u2236= 0 ; [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57\u2260\ud835\udc57 \u2032 \u2236= 2 ([\ud835\udc36(\ud835\udee9\ud835\udc56 )\ud835\udc36(\ud835\udee9\ud835\udc56 )\u22a4 ]\ud835\udc57,\ud835\udc57 \u2032 \u2212 1)                              2\n                                                                                                                                                                             (\ud835\udc3f\ud835\udc56 )(\ud835\udc3f\ud835\udc56 \u2212 1)\n                                                                                                                                                                           1\n  SMT HH                              \ud835\udf19                                     [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57,\ud835\udc57 \u2236= 1 ; [\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udc57\u2260\ud835\udc57 \u2032 \u2236= [\ud835\udc36(\ud835\udee9\ud835\udc56 )\ud835\udc36(\ud835\udee9\ud835\udc56 )\u22a4 ]\ud835\udc57,\ud835\udc57 \u2032                                      2\n                                                                                                                                                                             (\ud835\udc3f\ud835\udc56 )(\ud835\udc3f\ud835\udc56 \u2212 1)\n\n\n\nthe categorical correlation matrix, which is critical to determine the                                  Table 4\nmean and variance predictions. As mentioned in Section 1, approaches                                    Results of the cantilever beam models [34, Table 4].\n\nsuch as CR [30,39], continuous latent variables [54], and GD [31]                                         Categorical               Displacement         Likelihood            # of\n                                                                                                          kernel                    error (cm)                                 hyperparam.\nuse a kernel-based method to estimate the correlation matrix. Other\nmethods estimate the correlation matrix by modeling the correlation                                       SMT GD                    1.3861               111.13                3\nentries directly [32,35,53], such as HH [33] and EHH [34]. The HH                                         SMT CR                    1.1671               155.32                14\n                                                                                                          SMT EHH                   0.1613               236.25                68\ncorrelation kernel is of particular interest because it generalizes simpler                               SMT HH                    0.2033               235.66                68\nkernels such as CR and GD [34]. In SMT 2.0, the correlation kernel is\nan option that can be set to either CR (CONT_RELAX_KERNEL), GD\n(GOWER_KERNEL), HH (HOMO_HSPHERE_KERNEL) or EHH\n(EXP_HOMO_HSPHERE_KERNEL).                                                                              3.2. An engineering design test-case\n\n3.1. Mixed Gaussian processes                                                                               A classic engineering problem commonly used for model validation\n                                                                                                        is the beam bending problem [32,58]. This problem is illustrated\n    The continuous and ordinal variables are both treated similarly\n                                                                                                        on Fig. 2(a) and consists of a cantilever beam in its linear range loaded\nin SMT 2.0 with a continuous kernel, where the ordinal values are\n                                                                                                        at its free end with a force \ud835\udc39 . As in Cheng et al. [58], the Young\nconverted to continuous through relaxation. For categorical variables,\n                                                                                                        modulus is \ud835\udc38 = 200 GPa and the chosen load is \ud835\udc39 = 50 kN. Also, as\nfour models (GD, CR, EHH and HH) can be used in SMT 2.0 if\n                                                                                                        in Roustant et al. [32], 12 possible cross-sections can be used. These\nspecified by the API. This is why we developed a unified mathematical\n                                                                                                        12 sections consist of 4 possible shapes that can be either hollow, thick\nformulation that allows a unique implementation for any model.\n    Denote \ud835\udc59 the number of categorical variables. For a given \ud835\udc56 \u2208                                       or full as illustrated in Fig. 2(b).\n{1, \u2026 , \ud835\udc59}, the \ud835\udc56th categorical variable is denoted \ud835\udc50\ud835\udc56 and its number                                       To compare the mixed Kriging models of SMT 2.0, we draw a 98\nof levels is denoted \ud835\udc3f\ud835\udc56 . The hyperparameter matrix peculiar to this                                    point LHS as training set and the validation set is a grid of 12 \u00d7 30 \u00d7\nvariable \ud835\udc50\ud835\udc56 is                                                                                          30 = 10800 points. For the four implemented methods, displacement\n                                                                                                        error (computed with a root-mean-square error criterion), likelihood,\n     \u23a1 [\ud835\udee9\ud835\udc56 ]1,1                              \ud835\udc7a\ud835\udc9a\ud835\udc8e.                  \u23a4\n     \u23a2 [\ud835\udee9 ]                                                        \u23a5                                    number of hyperparameters and computational time for every model\n                          [\ud835\udee9\ud835\udc56 ]2,2\n\ud835\udee9\ud835\udc56 = \u23a2 \ud835\udc56 1,2                                                       \u23a5,                                   are shown in Table 4. For the continuous variables, we use the square\n     \u23a2 \u22ee                    \u22f1              \u22f1                       \u23a5                                    exponential kernel. More details are found in [34]. As expected, the\n     \u23a2[\ud835\udee9 ]                  \u2026         [\ud835\udee9\ud835\udc56 ]\ud835\udc3f\ud835\udc56 \u22121,\ud835\udc3f\ud835\udc56   [\ud835\udee9\ud835\udc56 ]\ud835\udc3f\ud835\udc56 ,\ud835\udc3f\ud835\udc56 \u23a5\u23a6\n     \u23a3 \ud835\udc56 1,\ud835\udc3f\ud835\udc56                                                                                           complex EHH and HH models lead to a lower displacement error and a\nand the categorical parameters are defined as \ud835\udf03 \ud835\udc50\ud835\udc4e\ud835\udc61 = {\ud835\udee91 , \u2026 , \ud835\udee9\ud835\udc59 }. For                               higher likelihood value, but use more hyperparameters and increase\ntwo given inputs in the DoE, for example, the \ud835\udc5fth and \ud835\udc60th points, let                                   the computational cost compared to GD and CR. On this test case,\n\ud835\udc50\ud835\udc56\ud835\udc5f and \ud835\udc50\ud835\udc56\ud835\udc60 be the associated categorical variables taking respectively                                 the kernel EHH is easier to optimize than HH but in general, they\nthe \ud835\udcc1\ud835\udc5f\ud835\udc56 and the \ud835\udcc1\ud835\udc60\ud835\udc56 level on the categorical variable \ud835\udc50\ud835\udc56 . The categorical                              are similar in terms of performance. Also, by default SMT 2.0 uses\ncorrelation kernel is defined by                                                                        CR as it is known to be a good trade-off between complexity and\n\ud835\udc58\ud835\udc50\ud835\udc4e\ud835\udc61 (\ud835\udc50 \ud835\udc5f , \ud835\udc50 \ud835\udc60 , \ud835\udf03 \ud835\udc50\ud835\udc4e\ud835\udc61 ) =                                                                             performance [59].\n\n    \u220f\n    \ud835\udc59                                                                                        (1)\n           \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udcc1\ud835\udc5f ,\ud835\udcc1\ud835\udc60 ) \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udcc1\ud835\udc60 ,\ud835\udcc1 \ud835\udc5f )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udcc1\ud835\udc5f ,\ud835\udcc1\ud835\udc5f )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56 )]\ud835\udcc1\ud835\udc60 ,\ud835\udcc1\ud835\udc60 )                   4. Surrogate models with hierarchical variables in SMT 2.0\n                              \ud835\udc56   \ud835\udc56           \ud835\udc56   \ud835\udc56              \ud835\udc56      \ud835\udc56        \ud835\udc56   \ud835\udc56\n     \ud835\udc56=1\nwhere \ud835\udf05 is either a positive definite kernel or identity and \ud835\udef7(.) is a                                      To introduce the newly developed Kriging model for hierarchical\nsymmetric positive definite (SPD) function such that the matrix \ud835\udef7(\ud835\udee9\ud835\udc56 )                                  variables implemented in SMT 2.0, we present the general mathe-\nis SPD if \ud835\udee9\ud835\udc56 is SPD. For an exponential kernel, Table 3 gives the                                       matical framework for hierarchical and mixed variables established\nparameterizations of \ud835\udef7 and \ud835\udf05 that correspond to GD, CR, HH, and                                         by Audet et al. [38]. In SMT 2.0, two variants of our new method\nEHH kernels. The complexity of these different kernels depends on                                       are implemented, namely SMT Alg-Kernel and SMT Arc-Kernel.\nthe number of hyperparameters that characterizes them. As defined                                       In particular, the SMT Alg-Kernel is a novel correlation kernel\nby Saves et al. [34], for every categorical variable \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc59}, the                                introduced in this paper.\nmatrix \ud835\udc36(\ud835\udee9\ud835\udc56 ) \u2208 R\ud835\udc3f\ud835\udc56 \u00d7\ud835\udc3f\ud835\udc56 is lower triangular and built using a hypersphere\ndecomposition [55,56] from the symmetric matrix \ud835\udee9\ud835\udc56 \u2208 R\ud835\udc3f\ud835\udc56 \u00d7\ud835\udc3f\ud835\udc56 of\n                                                                                                        4.1. The hierarchical variables framework\nhyperparameters. The variable \ud835\udf16 is a small positive constant and the\nvariable \ud835\udf03\ud835\udc56 denotes the only positive hyperparameter that is used for\n                                                                                                            A problem structure is classified as hierarchical when the sets of\nthe Gower distance kernel.\n   Another Kriging based model that can use mixed variables is Kriging                                  active variables depend on architectural choices. This occurs frequently\nwith partial least squares (KPLS) [57]. KPLS adapts Kriging to high                                     in industrial design problems. In hierarchical problems, we can classify\ndimensional problems by using a reduced number of hyperparameters                                       variables as neutral, meta (also known as dimensional) or decreed\nthanks to a projection into a smaller space. Also, for a general surrogate,                             (also known as conditionally active) as detailed in Audet et al. [38].\nnot necessarily Kriging, SMT 2.0 uses continuous relaxation to allow                                    Neutral variables are the variables that are not affected by the hierarchy\nwhatever model to handle mixed variables. For example, we can use                                       whereas the value assigned to meta variables determines which decreed\nmixed variables with least squares (LS) or quadratic polynomial (QP)                                    variables are activated. For example, a meta variable could be the\nmodels. We now illustrate the abilities of the toolbox in terms of mixed                                number of engines. If the number of engines changes, the number of\nmodeling over an engineering test case.                                                                 decreed bypass ratios that every engine should specify also changes.\n\n                                                                                                    5\n\fP. Saves et al.                                                                                                    Advances in Engineering Software 188 (2024) 103571\n\n\n\n\n                                                        Fig. 2. Cantilever beam problem [34, Figure 6].\n\n\n\n\n                                                      Fig. 3. Variables classification as used in SMT 2.0.\n\n\nHowever, the wing aspect ratio being neutral, it is not affected by this               2. The number of neurons in the hidden layer number \ud835\udc58 is either\nhierarchy.                                                                                included or excluded. For example, the \u2018\u2018# of neurons in the 3rd\n    Problems involving hierarchical variables are generally dependant                     layer\u2019\u2019 would be excluded for an input that only has 2 hidden\non discrete architectures and as such involve mixed variables. Hence,                     layers. Therefore, \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are decreed\nin addition to their role (neutral, meta or decreed), each variable also                  variables.\nhas a variable type amongst categorical, ordinal or continuous. For the                3. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and\nsake of simplicity and because both continuous and ordinal variables                      \u2018\u2018Batch size\u2019\u2019 are not affected by the hierarchy choice. Therefore,\nare treated similarly [34], we chose to regroup them as quantitative                      they are neutral variables.\nvariables. For instance, the neutral variables \ud835\udc65neu may be partitioned\n                                                           qnt\ninto different variable types, such that \ud835\udc65neu = (\ud835\udc65cat                   cat\n                                                    neu , \ud835\udc65neu ) where \ud835\udc65neu\n                                                                                   According to their types, the MLP input variables can be classified as\n                                           qnt\nrepresents the categorical variables and \ud835\udc65neu are the quantitative ones.           follows:\nThe variable classification scheme in SMT 2.0 is detailed in Fig. 3.\n                                                                                       4. The meta variable \u2018\u2018# of hidden layers\u2019\u2019 is an integer and, as\n    To explain the framework and the new Kriging model, we illustrate                                                              qnt\nthe inputs variables of the model using a classical machine learn-                        such, is represented by the component \ud835\udc65met .\ning problem related to the hyperparameters optimization of a fully-                    5. The decreed variables \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are integers\n                                                                                                                                            qnt\nconnected Multi-Layer Perceptron (MLP) [38] on Fig. 4. In Table 5,                        and, as such, are represented by the component \ud835\udc65dec .\nwe detail the input variables of the model related to the MLP problem                  6. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and\n(i.e., the hyperparameters of the neural network, together with their                     \u2018\u2018Batch size\u2019\u2019 are, respectively, continuous, for the first two (ev-\ntypes and roles). To keep things clear and concise, the chosen problem                    ery value between two bounds), categorical (qualitative between\nis a simplification of the original problem developed by Audet et al.                     three choices) and integer (quantitative between 6 choices).\n[38]. Regarding the MLP problem of Fig. 4 and following the classi-                       Therefore, the \u2018\u2018Activation function\u2019\u2019 and the \u2018\u2018Momentum\u2019\u2019 are\nfication scheme of Fig. 3, we start by separating the input variables                     represented by the component \ud835\udc65cat  neu . The \u2018\u2018Learning rate\u2019\u2019 and the\naccording to their role. In fact,                                                         \u2018\u2018Batch size\u2019\u2019 are represented by the component \ud835\udc65neu .\n                                                                                                                                                qnt\n\n     1. changing the number of hidden layers modifies the number                      To model hierarchical variables, as proposed in [38],\n        of inputs variables. Therefore, \u2018\u2018# of hidden layers\u2019\u2019 is a meta                                                                   \u22c3 we separate\n                                                                                   the input space \ue244 as (\ue244neu , \ue244met , \ue244dec ) where \ue244dec =    \ue244inc (\ud835\udc65met ).\n        variable.                                                                                                                             \ud835\udc65met \u2208\ue244met\n\n\n                                                                               6\n\fP. Saves et al.                                                                                                          Advances in Engineering Software 188 (2024) 103571\n\n\n\n\n                                                              Fig. 4. The Multi-Layer Perceptron (MLP) problem.\n                                                              Source: Figure adapted from [38, Figure 1].\n\n\nTable 5                                                                                   4.2. A Kriging model for hierarchical variables\nA detailed description of the variables in the MLP problem.\n MLP                  Variable    Domain                      Type     Role\n                                                                                             In this section, a new method to build a Kriging model with hierar-\n Hyperparameters\n                                                                                          chical variables is introduced based on the framework aforementioned.\n Learning rate        \ud835\udc5f           [10\u22125 , 10\u22122 ]              FLOAT    NEUTRAL\n                                                                                          The proposed methods are included in SMT 2.0.\n Momentum             \ud835\udefc           [0, 1]                      FLOAT    NEUTRAL\n Activation           \ud835\udc4e           {\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 , \ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51, \ud835\udc47 \ud835\udc4e\ud835\udc5b\u210e}     ENUM     NEUTRAL\n function                                                                                 4.2.1. Motivation and state-of-the-art\n Batch size           \ud835\udc4f           {8, 16, \u2026 , 128, 256}       ORD      NEUTRAL                Assuming that the decreed variables are quantitative, Hutter and\n # of hidden          \ud835\udc59           {1, 2, 3}                   ORD      META\n                                                                                          Osborne [21] proposed several kernels for the hierarchical context. A\n layers\n # of neurons         \ud835\udc5b\ud835\udc58          {50, 51, \u2026 , 55}            ORD      DECREED            classic approach, called the imputation method (Imp-Kernel) leads\n hidden layer \ud835\udc58                                                                           to an efficient paradigm in practice that can be easily integrated into\n                                                                                          a more general framework as proposed by Bussemaker et al. [24].\n                                                                                          However, this kernel lacks depth and depends on arbitrary choices.\n                                                                                          Therefore, Hutter and Osborne [21] also proposed a more general\nHence, for a given point \ud835\udc65 \u2208 \ue244, one has \ud835\udc65 = (\ud835\udc65neu , \ud835\udc65met , \ud835\udc65inc (\ud835\udc65met )), where           kernel, called Arc-Kernel and Horn et al. [36] generalized this\n\ud835\udc65neu \u2208 \ue244neu , \ud835\udc65met \u2208 \ue244met and \ud835\udc65inc (\ud835\udc62met ) \u2208 \ue244inc (\ud835\udc62met ) are defined as follows:         kernel even more and proposed a new formulation called the Wedge-\n                                                                                          Kernel [37]. The drawbacks of these two methods are that they add\n     \u2022 The components \ud835\udc65neu \u2208 \ue244neu gather all neutral variables that\n                                                                                          some extra hyperparameters for every decreed dimension (respectively\n       are not impacted by the meta variables but needed. For ex-                         one extra hyperparameter for the Arc-Kernel and two hyperparam-\n       ample, in the MLP problem, \ue244neu gathers the possible learning                      eters for the Wedge-Kernel) and that they need a normalization\n       rates, momentum, activation functions and batch sizes. Namely,                     according to the bounds. More recently, Pelamatti et al. [60] developed\n       from Table 5, \ue244neu = [10\u22125 , 10\u22122 ] \u00d7 [0, 1] \u00d7 {ReLu, Sigmoid, Tanh} \u00d7             a hierarchical kernel for Bayesian optimization. However, our work\n       {8, 16, \u2026 , 256}.                                                                  is also more general thanks to the framework introduced earlier [38]\n     \u2022 The components \ud835\udc65met gather the meta (also known as dimen-                          that considers variable-wise formulation and is more flexible as we are\n       sional) variables that determine the inclusion or exclusion of                     allowing sub-problems to be intersecting.\n       other variables. For example, in the MLP problem, \ue244met represents                      In the following, we describe our new method to build a correlation\n                                                                                          kernel for hierarchical variables. In particular, we introduce a new alge-\n       the possible numbers of layers in the MLP. Namely, from Table 5,\n                                                                                          braic kernel called Alg-Kernel that behaves like the Arc-Kernel\n       \ue244met = {1, 2, 3}.\n                                                                                          whilst correcting most of its drawbacks. In particular, our kernel does\n     \u2022 The components \ud835\udc65inc (\ud835\udc65met ), contain the decreed variables whose\n                                                                                          not add any hyperparameters, and the normalization is handled in a\n       inclusion (decreed-included) or exclusion (decreed-excluded) is                    natural way.\n       determined by the values of the meta components \ud835\udc65met . For exam-\n       ple, in the MLP problem, \ue244dec represents the number of neurons                     4.2.2. A new hierarchical correlation kernel\n       in the decreed layers. Namely, from Table 5, \ue244inc (\ud835\udc65met = 3) =                         For modeling purposes, we assume that the decreed space is quan-\n                                                                                                                  qnt\n       [50, 55]3 , \ue244inc (\ud835\udc65met = 2) = [50, 55]2 and \ue244inc (\ud835\udc65met = 1) = [50, 55].            titative, i.e., \ue244dec = \ue244dec . Let \ud835\udc62 \u2208 \ue244 be an input point partitioned as\n\n                                                                                      7\n\fP. Saves et al.                                                                                                                                 Advances in Engineering Software 188 (2024) 103571\n\n                                                                                                       alg                                 \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , the new algebraic distance is\n\ud835\udc62 = (\ud835\udc62neu , \ud835\udc62met , \ud835\udc62inc (\ud835\udc62met )) and, similarly, \ud835\udc63 \u2208 \ue244 is another input such                       \ud835\udc58met,dec . For every \ud835\udc56 \u2208 \ud835\udc3cdec , if \ud835\udc56 \u2208 \ud835\udc3c\ud835\udc62,\ud835\udc63\nthat \ud835\udc63 = (\ud835\udc63neu , \ud835\udc63met , \ud835\udc63inc (\ud835\udc63met )). The new kernel \ud835\udc58 that we propose for                        given by\nhierarchical variables is given by\n                                                                                                                                                    \u239b                                           \u239e\n\ud835\udc58(\ud835\udc62, \ud835\udc63) = \ud835\udc58neu (\ud835\udc62neu , \ud835\udc63neu ) \u00d7 \ud835\udc58met (\ud835\udc62met , \ud835\udc63met )                                                    alg                                          \u239c    2|[\ud835\udc62inc (\ud835\udc62met )]\ud835\udc56 \u2212 [\ud835\udc63inc (\ud835\udc63met )]\ud835\udc56 |  \u239f\n                                                                                                   \ud835\udc51         ([\ud835\udc62inc (\ud835\udc62met )]\ud835\udc56 , [\ud835\udc63inc (\ud835\udc63met )]\ud835\udc56 ) = \u239c \u221a                    \u221a                    \u239f \ud835\udf03\ud835\udc56 ,\n                    \u00d7 \ud835\udc58met,dec ([\ud835\udc62met , \ud835\udc62inc (\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc (\ud835\udc63met )]),           (2)                                                        \u239c [\ud835\udc62 (\ud835\udc62 )] + 1 [\ud835\udc63 (\ud835\udc63 )] + 1 \u239f\n                                                                                                                                                                    2                         2\n                                                                                                                                                    \u239d   inc met \ud835\udc56               inc met \ud835\udc56       \u23a0\nwhere \ud835\udc58neu , \ud835\udc58met and \ud835\udc58met,dec are as follows:                                                                                                                                                      (6)\n       \u2022 \ud835\udc58neu represents the neutral kernel that encompasses both categor-                         where \ud835\udf03\ud835\udc56 \u2208 R+ is a continuous hyperparameter. Otherwise, if \ud835\udc56 \u2208 \ud835\udc3cdec\n         ical and quantitative neutral variables, i.e., \ud835\udc58neu can be decom-                                  \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f , there should be a non-zero residual distance between the\n                                                                                                   but \ud835\udc56 \u2209 \ud835\udc3c\ud835\udc62,\ud835\udc63\n         posed into two parts \ud835\udc58neu (\ud835\udc62neu , \ud835\udc63neu ) = \ud835\udc58cat (\ud835\udc62cat    cat   qnt qnt qnt\n                                                           neu , \ud835\udc63neu )\ud835\udc58 (\ud835\udc62neu , \ud835\udc63neu ).           two different subspaces \ue244inc (\ud835\udc62met ) and \ue244inc (\ud835\udc63met ) to ensure the kernel\n         The categorical kernel, denoted \ud835\udc58cat , could be any Symmetric                             SPD property. To have a residual not depending on the decreed values,\n         Positive Definite (SPD) [34] mixed kernel (see Section 3). For                            our model considers that there is a unit distance\n         the quantitative (integer or continuous) variables, a distance-\n         based kernel is used. The chosen quantitative kernel (Exponential,                        \ud835\udc51 alg ([\ud835\udc62inc (\ud835\udc62met )]\ud835\udc56 , [\ud835\udc63inc (\ud835\udc63met )]\ud835\udc56 ) = 1.0 \ud835\udf03\ud835\udc56 , \u2200\ud835\udc56 \u2208 \ud835\udc3cdec \u29f5 \ud835\udc3c\ud835\udc62,\ud835\udc63\n                                                                                                                                                                      \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n                                                                                                                                                                            .\n         Mat\u00e9rn, . . . ), always depends on a given distance \ud835\udc51. For example,                                                                 alg\n         the \ud835\udc5b-dimensional exponential kernel gives                                                The induced meta kernel \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) to preserve the SPD property\n                                                                                                   of \ud835\udc58alg is defined as:\n                                 \u220f\n                                 \ud835\udc5b\n                                                qnt   qnt                                                                \u220f\n         \ud835\udc58qnt (\ud835\udc62qnt , \ud835\udc63qnt ) =         exp(\u2212\ud835\udc51(\ud835\udc62\ud835\udc56 , \ud835\udc63\ud835\udc56 )).                                (3)        alg\n                                                                                                   \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) =   \ud835\udc58qnt (1.0 \ud835\udf03\ud835\udc56 ).                              (7)\n                                 \ud835\udc56=1\n                                                                                                                               \ud835\udc56\u2208\ud835\udc3cmet\n\n       \u2022 \ud835\udc58met is the meta variables related kernel. It is also separated into                      Not only our kernel of Eq. (2) uses less hyperparameters than the Arc-\n         two parts: \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) = \ud835\udc58cat (\ud835\udc62cat    cat   qnt qnt   qnt\n                                                 met , \ud835\udc63met )\ud835\udc58 (\ud835\udc62met , \ud835\udc63met ) where the            Kernel (as we cut off its extra parameters) but it is also a more flexible\n         quantitative kernel is ordered and not continuous because meta                            kernel as it allows different kernels for meta and decreed variables.\n         variables take value in a finite set.                                                     Moreover, another advantage of our kernel is that it is numerically\n       \u2022 \ud835\udc58met,dec is an SPD kernel that models the correlations between the                        more stable thanks to the new non-stationary [61] algebraic distance\n         meta levels (all the possible subspaces) and the decreed variables.                       defined in Eq. (7) [62]. Our proposed distance also does not need any\n         In what comes next, we detailed this kernel.                                              rescaling by the bounds to have values between 0 and 1.\n                                                                                                       In what comes next, we will refer to the implementation of the\n4.2.3. Towards an algebraic meta-decreed kernel                                                    kernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and\n    Meta-decreed kernels like the imputation kernel or the                                         SMT Alg-Kernel. We note also that the implementation of SMT\nArc-Kernel were first proposed in [21,47] and the distance-based                                   Arc-Kernel differs slightly from the original Arc-Kernel as we\nkernels such as Arc-Kernel or Wedge-Kernel [37] were proven                                        fixed some hyperparameters to 1 in order to avoid adding extra hy-\nto be SPD. Nevertheless, to guarantee this SPD property, the same                                  perparameters and use the formulation of Eq. (2) and rescaling of the\nhyperparameters are used to model the correlations between the meta                                data.\nlevels and between the decreed variables [47]. For this reason, the\nArc-Kernel includes additional continuous hyperparameters which\nmakes the training of the GP models more expensive and introduces                                  4.2.4. Illustration on the MLP problem\nmore numerical stability issues. In this context, we are proposing a                                   In this section, we illustrate the hierarchical Arc-Kernel on the\nnew algebraic meta-decreed kernel (denoted as Alg-Kernel) that                                     MLP example. For that sake, we consider two design variables \ud835\udc62 and\nenjoys similar properties as Arc-Kernel but without using additional                               \ud835\udc63 such that \ud835\udc62 = (2.10\u22124 , 0.9, ReLU, 16, 2, 55, 51) and \ud835\udc63 = (5.10\u22123 , 0.8,\ncontinuous hyperparameters nor rescaling. In the SMT 2.0 release, we                               Sigmoid, 64, 3, 50, 54, 53). Since the value of \ud835\udc62met (i.e., the number of\nincluded Alg-Kernel and a simpler version of Arc-Kernel that do                                    hidden layers) differs from one point to another (namely, 2 for \ud835\udc62 and 3\nnot relies on additional hyperparameters.                                                          for \ud835\udc63), the associated variables \ud835\udc62inc (\ud835\udc62met ) have either 2 or 3 variables\n    Our proposed Alg-Kernel kernel is given by                                                     for the number of neurons in each layer (namely 55 and 51 for \ud835\udc62,\n alg                                                                                               and 50, 54 and 53 for the point \ud835\udc63). In our case, 8 hyperparame-\n\ud835\udc58met,dec ([\ud835\udc62met , \ud835\udc62inc (\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc (\ud835\udc63met )])\n                                                                                         (4)       ters ([\ud835\udc451 ]1,2 , \ud835\udf031 , \u2026 , \ud835\udf037 ) will have to be optimized where \ud835\udc58 is given by\n            alg                        alg\n        = \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) \u00d7 \ud835\udc58dec (\ud835\udc62inc (\ud835\udc62met ), \ud835\udc63inc (\ud835\udc63met )).                                 Eq. (2). These 7 hyperparameters can be described using our proposed\n                                                                                                   framework as follows:\nMathematically, we could consider that there is only one meta variable\nwhose levels correspond to every possible included subspace. Let \ud835\udc3csub                                        \u2022 For the neutral components, we have \ud835\udc62neu = (2.10\u22124 , 0.9, ReLU, 16)\ndenotes the components indices of possible subspaces, the subspaces                                            and \ud835\udc63neu = (5.10\u22123 , 0.8, Sigmoid, 64). Therefore, for a categorical\nparameterized by the meta component \ud835\udc62met are defined as \ue244inc (\ud835\udc62met =                                           matrix kernel \ud835\udc451 and a square exponential quantitative kernel,\n\ud835\udc59), \ud835\udc59 \u2208 \ud835\udc3csub . It follows that the fully extended continuous decreed\n                            \u22c3                                                                                  \ud835\udc58neu (\ud835\udc62neu , \ud835\udc63neu ) = \ud835\udc58cat (\ud835\udc62cat    cat   qnt   qnt    qnt\nspace writes as \ue244dec = \ud835\udc59\u2208\ud835\udc3csub \ue244inc (\ud835\udc62met = \ud835\udc59) and \ud835\udc3cdec is the set of the                                                                    neu , \ud835\udc63neu )\ud835\udc58 (\ud835\udc62neu , \ud835\udc63neu )\n                             \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f denotes the set of components related to\nassociated indices. Let \ud835\udc3c\ud835\udc62,\ud835\udc63                                                                                                       = [\ud835\udc451 ]1,2 exp [\u2212\ud835\udf031 (2.10\u22124 \u2212 5.10\u22123 )2 ]\nthe space \ue244inc (\ud835\udc62met , \ud835\udc63met ) containing the variables decreed-included in                                                              exp [\u2212\ud835\udf032 (0.9 \u2212 0.8)2 ] exp [\u2212\ud835\udf033 (16 \u2212 64)2 ].\nboth \ue244inc (\ud835\udc62met ) and \ue244inc (\ud835\udc63met ).\n    Since the decreed variables are quantitative, one has                                                      The values [\ud835\udc451 ]1,2 , \ud835\udf031 , \ud835\udf032 and \ud835\udf033 need to be optimized. Here,\n alg\n                                                                                                               [\ud835\udc451 ]1,2 is the correlation between \"ReLU\" and \"Sigmoid\".\n\ud835\udc58dec (\ud835\udc62inc (\ud835\udc62met ), \ud835\udc63inc (\ud835\udc63met )) = \ud835\udc58qnt (\ud835\udc62inc (\ud835\udc62met ), \ud835\udc63inc (\ud835\udc63met ))\n                                      \u220f                                                                      \u2022 For the meta components, we have \ud835\udc62met = 2 and \ud835\udc63met = 3.\n                                  =          \ud835\udc58qnt ([\ud835\udc62inc (\ud835\udc62met )]\ud835\udc56 , [\ud835\udc63inc (\ud835\udc63met )]\ud835\udc56 )   (5)                   Therefore, for a square exponential quantitative kernel,\n                                       \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\n                                    \ud835\udc56\u2208\ud835\udc3c\ud835\udc62,\ud835\udc63                                                                                                                      qnt    qnt\n                                                                                                               \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) = \ud835\udc58cat (\ud835\udc62cat    cat   qnt\n                                                                                                                                            met , \ud835\udc63met )\ud835\udc58 (\ud835\udc62met , \ud835\udc63met )\nThe construction of the quantitative kernel \ud835\udc58qnt depends on a given\n                                      alg                                                                                          = exp [\u2212\ud835\udf034 (3 \u2212 2)2 ].\ndistance denoted \ud835\udc51 alg . The kernel \ud835\udc58met is an induced meta kernel that\ndepends on the same distance \ud835\udc51     alg  to preserve the SPD property of                                        The value \ud835\udf034 needs to be optimized.\n\n                                                                                               8\n\fP. Saves et al.                                                                                                           Advances in Engineering Software 188 (2024) 103571\n\n\n     \u2022 For the meta-decreed kernel, we have [\ud835\udc62met , \ud835\udc62inc (\ud835\udc62met )] = [2, (55, 51)]       Table 6\n                                                                                        Results on the neural network model.\n       and [\ud835\udc63met , \ud835\udc63inc (\ud835\udc63met )] = [3, (50, 54, 53)] which gives\n                                                                                         Hierarchical method        Prediction        Likelihood         # of\n         alg                                                                                                        error (RMSE)                         hyperparam.\n       \ud835\udc58met,dec ([\ud835\udc62met , \ud835\udc62inc (\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc (\ud835\udc63met )])\n                  alg          alg                                                       SMT Alg-kernel             3.7610            176.11             10\n           = \ud835\udc58met (2, 3) \ud835\udc58dec ((55, 51), (50, 54, 53)).                                  SMT Arc-kernel             4.9208            162.01             10\n                                       (                )                                Imp-Kernel                 4.5455            170.64             10\n       The distance \ud835\udc51 alg (51, 54) = \u221a 2\u00d7|51\u221254| \u221a        \ud835\udf036 = 2.178.10\u22123 \ud835\udf036 . In\n                                                  512 +1 542 +1\n       general, for surrogate models, and in particular in SMT 2.0, the\n       input data are normalized. With a unit normalization\n                                                  (         from )\n                                                                 [50, 55]               our modeling method is Bayesian optimization to perform quickly the\n                                                       2\u00d70.6\n       to [0, 1], we would have \ud835\udc51 alg (0.2, 0.8) =\n                                                 \u221a       \u221a         \ud835\udf036          =        hyperparameter optimization of a neural network [63].\n                                                  0.22 +1 0.62 +1\n       0.919 \ud835\udf036 . Similarly, we have, between 55 and 50, \ud835\udc51 alg (0, 1)    =\n       1.414 \ud835\udf035 . Hence, for a square exponential quantitative kernel, one\n                                                                                        5. Bayesian optimization within SMT 2.0\n       gets\n         alg\n       \ud835\udc58met,dec ([\ud835\udc62met , \ud835\udc62inc (\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc (\ud835\udc63met )])\n                                                                                            Efficient global optimization (EGO) is a sequential Bayesian op-\n               = exp [\u2212\ud835\udf037 ] \u00d7 exp [\u22121.414 \ud835\udf035 ] \u00d7 exp [\u22120.919 \ud835\udf036 ],                      timization algorithm designed to find the optimum of a black-box\n                                                                 alg                    function that may be expensive to evaluate [52]. EGO starts by fitting\n       where the meta induced component is \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) = exp [\u2212\ud835\udf037 ]\n                                                                                        a Kriging model to an initial DoE, and then uses an acquisition function\n       because the decreed value 53 in \ud835\udc63 has nothing to be compared\n                                                                                        to select the next point to evaluate. The most used acquisition function\n       with in \ud835\udc62 as in Eq. (7). The values \ud835\udf035 , \ud835\udf036 and \ud835\udf037 need to be opti-\n                                                                                        is the expected improvement. Once a new point has been evaluated, the\n       mized which complete the description of the hyperparameters.\n                                                                                        Kriging model is updated. Successive updates increase the model accu-\n       We note that for the MLP problem, Alg-Kernel models use\n                                                                                        racy over iterations. This enrichment process repeats until a stopping\n       10 hyperparameters whereas the Arc-Kernel would require\n                                                                                        criterion is met.\n       12 hyperparameters without the meta kernel (\ud835\udf034 ) but with 3\n                                                                                            Because SMT 2.0 implements Kriging models that handle mixed\n       extra decreed hyperparameters and the Wedge-Kernel would\n                                                                                        and hierarchical variables, we can use EGO to solve problems in-\n       require 15 hyperparameters. For deep learning applications, a\n                                                                                        volving such design variables. Other Bayesian optimization algorithms\n       more complex perceptron with up to 10 hidden layers would\n                                                                                        often adopt approaches based on solving subproblems with contin-\n       require 17 hyperparameters with SMT 2.0 models against 26\n                                                                                        uous or non-hierarchical Kriging. This subproblem approach is less\n       for Arc-Kernel and 36 for Wedge-Kernel. The next section\n                                                                                        efficient and scales poorly, but it can only solve simple problems.\n       illustrates the interest of our method to build a surrogate model\n                                                                                        Several Bayesian optimization software packages can handle mixed or\n       for this neural network engineering problem.\n                                                                                        hierarchical variables with such a subproblem approach. The pack-\n                                                                                        ages include BoTorch [25], SMAC [65], Trieste [66], HEBO [67],\n4.3. A neural network test-case using SMT 2.0                                           OpenBox [68], and Dragonfly [69].\n\n\n    In this section, we apply our models to the hyperparameters opti-\n                                                                                        5.1. A mixed optimization problem\nmization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0\nan example illustrates this MLP problem. For the sake of showing the\nKriging surrogate abilities, we implemented a dummy function with no                      Fig. 6 compares the four EGO methods implemented in SMT 2.0:\nsignificance to replace the real black-box that would require training                  SMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that\na whole Neural Network (NN) with big data. This function requires a                     illustrates Bayesian optimization is a toy test case [64] detailed in Ap-\nnumber of variables that depends on the value of the meta variable,                     pendix A. This test case has two variables, one continuous and one\ni.e the number of hidden layers. To simplify, we have chosen only                       categorical with 10 levels. To assess the performance of our algorithm,\n1, 2 or 3 hidden layers and therefore, we have 3 decreed variables                      we performed 20 runs with different initial DoE sampled by LHS.\nbut deep neural networks could also be investigated as our model can                    Every DoE consists of 5 points and we chose a budget of 55 infill\ntackle a few dozen variables. A test case (test_hierarchical_variables_NN )             points. Fig. 6(a) plots the convergence curves for the four methods. In\nshows that our model SMT Alg-Kernel interpolates the data prop-                         particular, the median is the solid line, and the first and third quantiles\nerly, checks that the data dimension is correct and also asserts that                   are plotted in dotted lines. To visualize better the data dispersion,\nthe inactive decreed variables have no influence over the prediction.                   the boxplots of the 20 best solutions after 20 evaluations are plotted\nIn Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical               in Fig. 6(b). As expected, the more a method is complex, the better\nand mixed variables based on the implementation of SMT 2.0 for                          the optimization. Both SMT HH and SMT EHH converged in around 18\ntest_hierarchical_variables_NN.                                                         evaluations whereas SMT CR and SMT GD take around 26 iterations as\n    To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel                       shown on Fig. 6(a). Also, the more complex the model, the closer the\nand SMT Arc-Kernel) with the state-of-the-art imputation method                         optimum is to the real value as shown on Fig. 6(b).\npreviously used on industrial application (Imp-Kernel) [24], we                             In Fig. 7 we illustrate how to use EGO with mixed variables based\ndraw a 99 point LHS (33 points by meta level) as a training set and the                 on the implementation of SMT 2.0. The illustrated problem is a mixed\nvalidation set is a LHS of 3 \u00d7 1000 = 3000 points. For the Imp-Kernel,                  variant of the Branin function [70].\nthe decreed-excluded values are replaced by 52 because the mean value                       Note that a dedicated notebook is available to reproduce the results\n52.5 is not an integer (by default, SMT rounds to the floor value).                     presented in this paper and the mixed integer notebook also includes\nFor the three methods, the precision (computed with a root-mean-                        an extra mechanical application with composite materials [41].11\nsquare error RMSE criterion), the likelihood and the computational\ntime are shown in Table 6. For this problem, we can see that SMT Alg-\nkernel gives better performance than the imputation method or SMT\nArc-kernel. Also, as all methods use the same number of hyperpa-                          11\n                                                                                             https://colab.research.google.com/github/SMTorg/smt/blob/master/\nrameters, they have similar time performances. A direct application of                  tutorial/SMT_MixedInteger_application.ipynb\n\n\n                                                                                    9\n\fP. Saves et al.                                                                           Advances in Engineering Software 188 (2024) 103571\n\n\n\n\n                  Fig. 5. Example of usage of Hierarchical and Mixed Kriging surrogate.\n\n\n\n\n                                                   10\n\fP. Saves et al.                                                                             Advances in Engineering Software 188 (2024) 103571\n\n\n\n\n                          Fig. 6. Optimization results for the Toy function [64].\n\n\n\n\n                  Fig. 7. Example of usage of mixed surrogates for Bayesian optimization.\n\n\n\n\n                                                    11\n\fP. Saves et al.                                                                                                                   Advances in Engineering Software 188 (2024) 103571\n\n\n\n\n                                                          Fig. 8. Optimization results for the hierarchical Goldstein function.\n\n\n5.2. A hierarchical optimization problem                                                        6. Other relevant contributions in SMT 2.0\n\n\n   The hierarchical test case considered in this paper to illustrate                                The new release SMT 2.0 introduces several improvements be-\nBayesian optimization is a modified Goldstein function [60] detailed                            sides Kriging for hierarchical and mixed variables. This section details\nin Appendix B. The resulting optimization problem involves 11 vari-                             the most important new contributions. Recall from Section 2.2 that\nables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical.                         five sub-modules are present in the code: Sampling, Problems,\nThese variables consist in 6 neutral variables, 1 dimensional (or meta)                         Surrogate Models, Applications and Notebooks.\nvariable and 4 decreed variables. Depending on the meta variable\nvalues, 4 different sub-problems can be identified. The optimization\n                                                                                                6.1. Contributions to Sampling\nproblem is given by:\n                  qnt      qnt\nmin \ud835\udc53 (\ud835\udc65cat           cat\n        neu , \ud835\udc65neu , \ud835\udc65\ud835\udc5a , \ud835\udc65dec )                                                                Pseudo-random sampling. The Latin Hypercube Sampling (LHS) is a\nw.r.t. \ud835\udc65cat                                                                                     stochastic sampling technique to generate quasi-random sampling dis-\n        neu = \ud835\udc642 \u2208 {0, 1}\n          qnt                                                                                   tributions. It is among the most popular sampling method in computer\n         \ud835\udc65neu = (\ud835\udc651 , \ud835\udc652 , \ud835\udc655 , \ud835\udc673 , \ud835\udc674 ) \u2208 {0, 100}3 \u00d7 {0, 1, 2}2                   (8)\n                                                                                                experiments thanks to its simplicity and projection properties with\n         \ud835\udc65cat\n          \ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3}                                                                 high-dimensional problems. The LHS method uses the pyDOE package\n          qnt\n         \ud835\udc65dec = (\ud835\udc653 , \ud835\udc654 , \ud835\udc671 , \ud835\udc672 ) \u2208 {0, 100}2 \u00d7 {0, 1, 2}2                                   (Design Of Experiments for Python). Five criteria for the construction\n                                                                                                of LHS are implemented in SMT. The first four criteria (center,\nCompared to the model choice of Pelamatti et al. [60], we chose to                              maximin, centermaximin, correlation) are the same as in\nmodel \ud835\udc655 and \ud835\udc642 as neutral variables even if \ud835\udc53 does not depend on                               pyDOE.12 The last criterion ese, is implemented by the authors of\n\ud835\udc655 when \ud835\udc642 = 0. Other modeling choices are kept; for example, \ud835\udc642 is                             SMT [48]. In SMT 2.0 a new LHS method was developed for the\na so-called \u2018\u2018binary variable\u2019\u2019 and not a categorical one [71]. Similarly,                      Nested design of experiments (NestedLHS) [73] to use with multi-\nwe also keep the formulation of \ud835\udc641 as a categorical variable but a better                       fidelity surrogates. A new mathematical method (expand_lhs) [40]\nmodel would be to model it as a \u2018\u2018cyclic variable\u2019\u2019 [72]. The resulting                         was developed in SMT 2.0 to increase the size of a design of exper-\nproblem is described in Appendix B. To assess the performance of our                            iments while maintaining the ese property. Moreover, we proposed\nalgorithm, we performed 20 runs with different initial DoE sampled by                           a sampling method for mixed variables, and the aforementioned LHS\nLHS. Every DoE consists of \ud835\udc5b + 1 = 12 points and we chose a budget                              method was applied to hierarchical variables in Fig. 8.\nof 5\ud835\udc5b = 55 infill points. To compare our method with a baseline, we\nalso tested the random search method thanks to the expand_lhs\nnew method [40] described in Section 6.1 and we also optimized the                              6.2. Contributions to Surrogate models\nGoldstein function using EGO with a classic Kriging model based on\nimputation method (Imp-Kernel). This method replaces the decreed-                               New kernels and their derivatives for Kriging. Kriging surrogates are\nexcluded variables by their mean values: 50 or 1 respectively for (\ud835\udc653 , \ud835\udc654 )                    based on hyperparameters and on a correlation kernel. Four correla-\nand (\ud835\udc671 , \ud835\udc672 ). Fig. 8(a) plots the convergence curves for the four methods.                    tion kernels are now implemented in SMT 2.0 [74]. In SMT, these\nIn particular, the median is the solid line and the first and third                             correlation functions are absolute exponential (abs_exp), Gaussian\nquantiles are plotted in dotted lines. To visualize better the correspond-                      (squar_exp), Matern 5/2 (matern52) and Matern 3/2 (matern32).\ning data dispersion, the boxplots of the 20 best solutions are plotted                          In addition, the implementation of gradient and Hessian for each kernel\nin Fig. 8(b). The results in Fig. 8 show that the hierarchical Kriging                          makes it possible to calculate both the first and second derivatives of\nmodels of SMT 2.0 lead to better results than the imputation method                             the GP likelihood with respect to the hyperparameters [5].\nor the random search both in terms of final objective value and variance\nover the 20 runs and in term of convergence rate. More precisely, SMT                           Variance derivatives for Kriging. To perform uncertainty quantification\nArc-Kernel and SMT Alg-Kernel Kriging model gave the best                                       for system analysis purposes, it could be interesting to know more\nEGO results and managed to converge correctly as shown in Fig. 8(b).                            about the variance derivatives of a model [75\u201377]. For that purpose\nMore precisely, the 20 sampled DoEs led to similar performance and                              and also to pursue the original publication about derivatives [5], SMT\nfrom one DoE, the method SMT Alg-Kernel managed to find the true                                2.0 extends the derivative support to Kriging variances and kernels.\nminimum. However, this result has not been reproduced in other runs\nand is therefore not statistically significant. The variance between the\n                                                                                                 12\nruns is of similar magnitude regardless of the considered methods.                                    https://pythonhosted.org/pyDOE/index.html\n\n\n                                                                                           12\n\fP. Saves et al.                                                                                                   Advances in Engineering Software 188 (2024) 103571\n\n\nNoisy Kriging. In engineering and in big data contexts with real exper-          where the paths can be sampled by generating independent standard\niments, surrogate models for noisy data are of significant interest. In          Normal distributed samples. The different methods are documented in\nparticular, there is a growing need for techniques like noisy Kriging            the tutorial Gaussian Process Trajectory Sampling [84].\nand noisy Multi-Fidelity Kriging (MFK) for data fusion [78]. For that\n                                                                                 Parallel Bayesian optimization. Due to the recent progress made in\npurpose, SMT 2.0 has been designed to accommodate Kriging and MFK\n                                                                                 hardware configurations, it has been of high interest to perform parallel\nto noisy data including the option to incorporate heteroscedastic noise\n                                                                                 optimizations. A parallel criterion called qEI [85] was developed to\n(using the use_het_noise option) and to account for different noise\n                                                                                 perform Efficient Global Optimization (EGO): the goal is to be able\nlevels for each data source [40].\n                                                                                 to run batch optimization. At each iteration of the algorithm, multiple\nKriging with partial least squares. Beside MGP, for high-dimensional             new sampling points are extracted from the known ones. These new\nproblems, the toolbox implements Kriging with partial least squares              sampling points are then evaluated using a parallel computing environ-\n(KPLS) [57] and its extension KPLSK [44]. The PLS information is                 ment. Five criteria are implemented in SMT 2.0: Kriging Believer (KB),\ncomputed by projecting the data into a smaller space spanned by the              Kriging Believer Upper Bound (KBUB), Kriging Believer Lower Bound\nprincipal components. By integrating this PLS information into the               (KBLB), Kriging Believer Random Bound (KBRand) and Constant Liar\nKriging correlation matrix, the number of inputs can be scaled down,             (CLmin) [86].\nthereby reducing the number of hyperparameters required. The result-\ning number of hyperparameters \ud835\udc51\ud835\udc52 is indeed much smaller than the                 7. Conclusion\noriginal problem dimension \ud835\udc51. Recently, in SMT 2.0, we extended the\nKPLS method for multi-fidelity Kriging (MFKPLS and MFKPLSK) [73,79,                 SMT 2.0 introduces significant upgrades to the Surrogate Modeling\n80]. We also proposed an automatic criterion to choose automatically             Toolbox. This new release adds support for hierarchical and mixed\nthe reduced dimension \ud835\udc51\ud835\udc52 based on Wold\u2019s R criterion [81]. This                  variables and improves the surrogate models with a particular focus\ncriterion has been applied to aircraft optimization with EGO where the           on Kriging (Gaussian process) models. SMT 2.0 is distributed through\nnumber \ud835\udc51\ud835\udc52 (\ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code) for the model is automatically selected           an open-source license and is freely available online.14 We provide\nat every iteration [39]. Special efforts have been made to accommodate           documentation that caters to both users and potential developers.15\nKPLS for multi-fidelity and mixed integer data [79,80].                          SMT 2.0 enables users and developers collaborating on the same\nMarginal Gaussian process. SMT 2.0 implements Marginal Gaussian                  project to have a common surrogate modeling tool that facilitates the\nProcess (MGP) surrogate models for high dimensional problems [82].               exchange of methods and reproducibility of results.\nMGP are Gaussian processes taking into account hyperparameters un-                  SMT has been widely used in aerospace and mechanical modeling\ncertainty defined as a density probability function. Especially we sup-          applications. Moreover, the toolbox is general and can be useful for\npose that the function to model \ud835\udc53 \u2236 \ud835\udefa \u21a6 R, where \ud835\udefa \u2282 R\ud835\udc51 and \ud835\udc51 is                 anyone who needs to use or develop surrogate modeling techniques,\nthe number of design variables, lies in a linear embedding \ue22d such as             regardless of the targeted applications. SMT is currently the only open-\n\ue22d = {\ud835\udc62 = \ud835\udc34\ud835\udc65, \ud835\udc65 \u2208 \ud835\udefa}, \ud835\udc34 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc52 and \ud835\udc53 (\ud835\udc65) = \ud835\udc53\ue22d (\ud835\udc34\ud835\udc65) with \ud835\udc53 (\ud835\udc65) = \ud835\udc53\ue22d \u2236             source toolbox that can build hierarchical and mixed surrogate models.\n\ue22d \u21a6 R and \ud835\udc51\ud835\udc52 \u226a \ud835\udc51. Then, we must use a kernel \ud835\udc58(\ud835\udc65, \ud835\udc65\u2032 ) = \ud835\udc58\ue22d (\ud835\udc34\ud835\udc65, \ud835\udc34\ud835\udc65\u2032 )\nwhose each component of the transfer matrix \ud835\udc34 is an hyperparameter.              Declaration of competing interest\nThus we have \ud835\udc51\ud835\udc52 \u00d7 \ud835\udc51 hyperparameters to find. Note that \ud835\udc51\ud835\udc52 is defined\nas \ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code [49].                                                          The authors declare that they have no known competing finan-\n                                                                                 cial interests or personal relationships that could have appeared to\nGradient-enhanced neural network. The new release SMT 2.0 imple-\n                                                                                 influence the work reported in this paper.\nments Gradient-Enhanced Neural Network (GENN) models [45].\nGradient-Enhanced Neural Networks (GENN) are fully connected multi-\nlayer perceptrons whose training process was modified to account for             Data availability\ngradient information. Specifically, the model is trained to minimize not\nonly the prediction error of the response but also the prediction error              Data will be made available on request. Results can be reproduced\nof the partial derivatives: the chief benefit of gradient enhancement is         freely online at https://colab.research.google.com/github/SMTorg/smt/\nbetter accuracy with fewer training points. Note that GENN applies to            blob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\nregression (single-output or multi-output), but not classification since\nthere is no gradient in that case. The implementation is fully vectorized        Acknowledgments\nand uses ADAM optimization, mini-batch, and L2-norm regularization.\nFor example, GENN can be used to learn airfoil geometries from a                     We want to thank all those who contribute to this release. Namely,\ndatabase. This usage is documented in SMT 2.0.13                                 M. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde\n                                                                                 Arenzana, S. Dubreuil, A. F. L\u00f3pez-Lopera, M. Meliani, M. Menz, N.\n6.3. Contributions to Applications                                               Mo\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is\n                                                                                 part of the activities of ONERA - ISAE - ENAC joint research group. We\nKriging trajectory and sampling. Sampling a GP with high resolution              also acknowledge the partners institutions: ONERA, NASA Glenn, ISAE-\nis usually expensive due to the large dimension of the associated                SUPAERO, Institut Cl\u00e9ment Ader (ICA), the University of Michigan,\ncovariance matrix. Several methods are proposed to draw samples                  Polytechnique Montr\u00e9al and the University of California San Diego.\nof a GP on a given set of points. To sample a conditioned GP, the                    The research presented in this paper has been performed in the\nclassic method consists in using a Cholesky decomposition (or eigende-           framework of the AGILE 4.0 project (Towards cyber-physical collabo-\ncomposition) of the conditioned covariance matrix of the process but             rative aircraft development), funded by the European Union Horizon\nsome numerical computational errors can lead to non SPD matrix. A                2020 research and innovation framework programme under grant\nmore recent approach based on Karhunen\u2013Lo\u00e8ve decomposition of the                agreement n\u25e6 815122 and in the COLOSSUS project (Collaborative\ncovariance kernel with the Nystr\u00f6m method has been proposed in [83]              System of Systems Exploration of Aviation Products, Services and\n\n\n 13                                                                               14\n    https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_                   https://github.com/SMTorg/SMT\n                                                                                  15\nparameters/learning_airfoil_parameters.html                                            https://smt.readthedocs.io/en/latest/\n\n\n                                                                            13\n\fP. Saves et al.                                                                                                                 Advances in Engineering Software 188 (2024) 103571\n\n\nBusiness Models) funded by the European Union Horizon Europe re-                     Then, the functions \ud835\udc530 , \ud835\udc531 and \ud835\udc532 are defined as mixed variants of\nsearch and innovation framework programme under grant agreement                      \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as such\nn\u25e6 101097120.\n                                                                                     \ud835\udc530 (\ud835\udc651 , \ud835\udc652 , \ud835\udc671 , \ud835\udc672 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 ) =\n    We also are grateful to E. Hall\u00e9-Hannan from Polytechnique Mon-                          (\ntr\u00e9al for the hierarchical variables framework.                                      1\ud835\udc672 =0 1\ud835\udc671 =0 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 20, 20, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n\n                                                                                         + 1\ud835\udc671 =1 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 50, 20, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\nAppendix A. Toy test function                                                                                                                    )\n                                                                                        + 1\ud835\udc671 =2 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 80, 20, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n                                                                                           (\n    This Appendix gives the detail of the toy function of Section 5.1.16             1\ud835\udc672 =1 1\ud835\udc671 =0 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 20, 50, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n                                                                                                                                                                           (B.3)\nFirst, we recall the optimization problem:                                               + 1\ud835\udc671 =1 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 50, 50, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\nmin \ud835\udc53 (\ud835\udc65cat , \ud835\udc65qnt )                                                                                                                             )\n                                                                                        + 1\ud835\udc671 =2 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 80, 50, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n              cat\nw.r.t. \ud835\udc65            = \ud835\udc501 \u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}               (A.1)              (\n                                                                                     1\ud835\udc672 =2 1\ud835\udc671 =0 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 20, 80, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n            \ud835\udc65qnt = \ud835\udc651 \u2208 [0, 1]\n                                                                                         + 1\ud835\udc671 =1 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 50, 80, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\nThe toy function \ud835\udc53 is defined as\n                                                                                                                                                     )\n\ud835\udc53 (\ud835\udc65, \ud835\udc501 ) =1\ud835\udc501 =0 cos(3.6\ud835\udf0b(\ud835\udc65 \u2212 2)) + \ud835\udc65 \u2212 1                                              + 1\ud835\udc671 =2 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 80, 80, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n\n                                                     \ud835\udc65                               \ud835\udc531 (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , \ud835\udc672 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 ) =\n             +1\ud835\udc501 =1 2 cos(1.1\ud835\udf0b exp(\ud835\udc65)) \u2212              +2\n                                                     2\n                                         1                                           1\ud835\udc672 =0 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , 20, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n             +1\ud835\udc501 =2 cos(2\ud835\udf0b\ud835\udc65) +            \ud835\udc65\n                                         2\n                                                                                         + 1\ud835\udc672 =1 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , 50, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n                                                      \ud835\udc65\u22121\n             +1\ud835\udc501 =3 \ud835\udc65(cos(3.4\ud835\udf0b(\ud835\udc65 \u2212 1)) \u2212                 )\n                                                       2                                 + 1\ud835\udc672 =2 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , 80, \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n                       \ud835\udc652\n             +1\ud835\udc501 =4 \u2212                                                               \ud835\udc532 (\ud835\udc651 , \ud835\udc652 , \ud835\udc654 , \ud835\udc671 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 ) =\n                       2                                                (A.2)\n                                                            \ud835\udc65                        1\ud835\udc671 =0 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 20, \ud835\udc654 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n             +1\ud835\udc501 =5 2 cos(0.25\ud835\udf0b exp(\u2212\ud835\udc654 ))2 \u2212                +1\n                                                            2\n                                   \ud835\udc65                                                     + 1\ud835\udc671 =1 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , 50, \ud835\udc652 , \ud835\udc654 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n             +1\ud835\udc501 =6 \ud835\udc65 cos(3.4\ud835\udf0b\ud835\udc65) \u2212  +1\n                                   2\n                                                                                         + 1\ud835\udc671 =2 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , 80, \ud835\udc654 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )\n                                     \ud835\udc65\n             +1\ud835\udc501 =7 \u2212 \ud835\udc65(cos(3.5\ud835\udf0b\ud835\udc65) + ) + 2                                          To finish with, the function \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont is given by\n                                     2\n                       \ud835\udc655                                                            \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , \ud835\udc654 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 ) = 53.3108 + 0.184901\ud835\udc651\n             +1\ud835\udc501 =8 \u2212    +1\n                       2\n                                  \u221a                                                  \u2212 5.02914\ud835\udc651 3 .10\u22126 + 7.72522\ud835\udc651 \ud835\udc673 .10\u22128 \u2212 0.0870775\ud835\udc652 \u2212 0.106959\ud835\udc653\n             +1\ud835\udc501 =9 \u2212 cos(2.5\ud835\udf0b\ud835\udc65)2 \ud835\udc65 \u2212 0.5 ln(\ud835\udc65 + 0.5) \u2212 1.3\n                                                                                     + 7.98772\ud835\udc653 \ud835\udc674 .10\u22126 + 0.00242482\ud835\udc654 + 1.32851\ud835\udc654 3 .10\u22126 \u2212 0.00146393\ud835\udc651 \ud835\udc652\n                                                                                     \u2212 0.00301588\ud835\udc651 \ud835\udc653 \u2212 0.00272291\ud835\udc651 \ud835\udc654 + 0.0017004\ud835\udc652 \ud835\udc653 + 0.0038428\ud835\udc652 \ud835\udc654\nAppendix B. Hierarchical Goldstein test function\n                                                                                     \u2212 0.000198969\ud835\udc653 \ud835\udc654 + 1.86025\ud835\udc651 \ud835\udc652 \ud835\udc653 .10\u22125 \u2212 1.88719\ud835\udc651 \ud835\udc652 \ud835\udc654 .10\u22126\n                                                                                                                                         (      (       ) )\n                                                                                                                                                  2\ud835\udf0b\n    This Appendix gives the detail of the hierarchical Goldstein problem             + 2.50923\ud835\udc651 \ud835\udc653 \ud835\udc654 .10\u22125 \u2212 5.62199\ud835\udc652 \ud835\udc653 \ud835\udc654 .10\u22125 + \ud835\udc642 5 cos      \ud835\udc65 \u22122 .\n                                                                                                                                                  100 5\nof Section 5.2.17 First, we recall the optimization problem:\n                                                                                                                                                                           (B.4)\n                      qnt         qnt\nmin \ud835\udc53 (\ud835\udc65cat           cat\n        neu , \ud835\udc65neu , \ud835\udc65\ud835\udc5a , \ud835\udc65dec )\n                                                                                     Appendix C. Supplementary data\nw.r.t. \ud835\udc65cat\n        neu = \ud835\udc642 \u2208 {0, 1}\n              qnt\n            \ud835\udc65neu = (\ud835\udc651 , \ud835\udc652 , \ud835\udc655 , \ud835\udc673 , \ud835\udc674 ) \u2208 [0, 100]3 \u00d7 {0, 1, 2}2   (B.1)            More at https://colab.research.google.com/github/SMTorg/smt/blob/\n                                                                                     master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.\n            \ud835\udc65cat\n             \ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3}                                                       Supplementary material related to this article can be found online\n              qnt\n            \ud835\udc65dec = (\ud835\udc653 , \ud835\udc654 , \ud835\udc671 , \ud835\udc672 ) \u2208 [0, 100]2 \u00d7 {0, 1, 2}2                     at https://doi.org/10.1016/j.advengsoft.2023.103571.\n\nThe hierarchical and mixed function \ud835\udc53 is defined as a hierarchical\nfunction that depends on \ud835\udc530 , \ud835\udc531 , \ud835\udc532 and \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as describes in the               References\nfollowing.\n                                                                                       [1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach\n\ud835\udc53 (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , \ud835\udc654 , \ud835\udc671 , \ud835\udc672 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc641 , \ud835\udc642 ) =                                for the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\u201373.\n                                                                                       [2] Kennedy M, O\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc\n       1\ud835\udc641 =0 \ud835\udc530 (\ud835\udc651 , \ud835\udc652 , \ud835\udc671 , \ud835\udc672 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )                                  Ser B Stat Methodol 2001;63:425\u201364.\n                                                                                       [3] Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets.\n+ 1\ud835\udc641 =1 \ud835\udc531 (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , \ud835\udc672 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )                    (B.2)              Aerosp Sci Technol 2018;75:74\u201387.\n                                                                                       [4] Martins JRRA, Ning A. Engineering design optimization. Cambridge University\n+ 1\ud835\udc641 =2 \ud835\udc532 (\ud835\udc651 , \ud835\udc652 , \ud835\udc654 , \ud835\udc671 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 )                                       Press; 2021.\n                                                                                       [5] Bouhlel MA, Hwang JT, Bartoli N, Lafage R, Morlier J, Martins JRA.\n+ 1\ud835\udc641 =3 \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont (\ud835\udc651 , \ud835\udc652 , \ud835\udc653 , \ud835\udc654 , \ud835\udc673 , \ud835\udc674 , \ud835\udc655 , \ud835\udc642 ).                                A Python surrogate modeling framework with derivatives. Adv Eng Softw\n                                                                                           2019;135:102662.\n                                                                                       [6] Bouhlel MA, Martins J. Gradient-enhanced kriging for high-dimensional\n                                                                                           problems. Eng Comput 2019;35:157\u201373.\n  16\n        https://github.com/jbussemaker/SBArchOpt                                       [7] Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikit-learn:\n  17\n        https://github.com/jbussemaker/SBArchOpt                                           Machine learning in Python. J Mach Learn Res 2011;12:2825\u201330.\n\n\n                                                                                14\n\fP. Saves et al.                                                                                                                      Advances in Engineering Software 188 (2024) 103571\n\n\n [8] Lataniotis C, Marelli S, Sudret B. Uqlab 2.0 and uqcloud: open-source vs.                   [38] Audet C, Hall\u00e9-Hannan E, Le Digabel S. A general mathematical framework\n     cloud-based uncertainty quantification. In: SIAM conference on uncertainty                       for constrained mixed-variable blackbox optimization problems with meta and\n     quantification. 2022.                                                                            categorical variables. Oper Res Forum 2023;4:1\u201337.\n [9] Faraci A, Beaurepaire P, Gayton N. Review on Python toolboxes for Kriging                   [39] Saves P, Nguyen Van E, Bartoli N, Diouane Y, Lefebvre T, David C, Defoort S,\n     surrogate modelling. In: ESREL. 2022.                                                            Morlier J. Bayesian optimization for mixed variables using an adaptive dimension\n[10] Kr\u00fcgener M, Zapata Usandivaras J, Bauerheim M, Urbano A. Coaxial-injector                        reduction process: applications to aircraft design. In: AIAA scitech 2022. 2022.\n     surrogate modeling based on Reynolds-averaged Navier\u2013Stokes simulations using               [40] Conde Arenzana R, L\u00f3pez-Lopera A, Mouton S, Bartoli N, Lefebvre T. Multi-\n     deep learning. J Propuls Power 2022;38:783\u201398.                                                   fidelity Gaussian process model for CFD and wind tunnel data fusion. In:\n[11] Ming D, Williamson D, Guillas S. Deep Gaussian process emulation using                           ECCOMAS aerobest. 2021.\n     stochastic imputation. Technometrics 2022;1\u201312.                                             [41] Rufato RC, Diouane Y, Henry J, Ahlfeld R, Morlier J. A mixed-categorical\n[12] Eli\u00e1\u0161 J, Vo\u0159echovsky\u0300 M, Sad\u00edlekv V. Periodic version of the minimax distance                    data-driven approach for prediction and optimization of hybrid discontinuous\n     criterion for Monte Carlo integration. Adv Eng Softw 2020;149:102900.                            composites performance. In: AIAA aviation 2022 forum. 2022.\n[13] Drouet V, Balesdent M, Brevault L, Dubreuil S, Morio J. Multi-fidelity algo-                [42] Gorissen D, Crombecq K, Couckuyt I, Dhaene T, Demeester P. A surrogate\n     rithm for the sensitivity analysis of multidisciplinary problems. J Mech Des                     modeling and adaptive sampling toolbox for computer based design. J Mach\n     2023;145:1\u201322.                                                                                   Learn Res 2010;11:2051\u20135.\n                                                                                                 [43] Williams CK, Rasmussen CE. Gaussian processes for machine learning. MA: MIT\n[14] Karban P, P\u00e1nek D, Orosz T, Petr\u00e1\u0161ov\u00e1 I, Dole\u017eel I. FEM based robust design\n                                                                                                      press Cambridge; 2006.\n     optimization with Agros and A\u0304rtap. Comput Math Appl 2021;81:618\u201333.\n                                                                                                 [44] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. Efficient Global Opti-\n[15] Kudela J, Matousek R. Recent advances and applications of surrogate models for\n                                                                                                      mization for high-dimensional constrained problems by using the Kriging models\n     finite element method computations: a review. Soft Comput 2022;26:13709\u201333.\n                                                                                                      combined with the Partial Least Squares method. Eng Optim 2018;50:2038\u201353.\n[16] Chen Y, Dababneh F, Zhang B, Kassaee S, Smith BT, Liu K, et al. Surrogate mod-\n                                                                                                 [45] Bouhlel MA, He S, Martins J. Scalable gradient-enhanced artificial neural\n     eling for capacity planning of charging station equipped with photovoltaic panel\n                                                                                                      networks for airfoil shape design in the subsonic and transonic regimes. Struct\n     and hydropneumatic energy storage. J Energy Res Technol 2020;142:050907.\n                                                                                                      Multidiscip Optim 2020;61:1363\u201376.\n[17] Jasa J, Bortolotti P, Zalkind D, Barter G. Effectively using multifidelity\n                                                                                                 [46] Kwan LS, Pitrou A, Seibert S. Numba: A LLVM-based python JIT compiler. In:\n     optimization for wind turbine design. Wind Energy Sci 2022;7:991\u20131006.\n                                                                                                      Proceedings of the second workshop on the LLVM compiler infrastructure in\n[18] Wang W, Tao G, Ke D, Luo J, Cui J. Transpiration cooling of high pres-\n                                                                                                      HPC. 2015.\n     sure turbine vane with optimized porosity distribution. Appl Therm Eng                      [47] Zaefferer M, Horn D. A first analysis of kernels for Kriging-based optimization\n     2023;223:119831.                                                                                 in hierarchical search spaces. 2018, arXiv.\n[19] Savage T, Almeida-Trasvina HF, del R\u00edo-Chanona EA, Smith R, Zhang D.                        [48] Jin R, Chen W, Sudjianto A. An efficient algorithm for constructing optimal\n     An adaptive data-driven modelling and optimization framework for complex                         design of computer experiments. J Statist Plann Inference 2005;2:545\u201354.\n     chemical process design. Comput Aided Chem Eng 2020;48:73\u20138.                                [49] Garnett R, Osborne M, Hennig P. Active learning of linear embeddings for\n[20] Chan A, Pires AF, Polacsek T. Trying to elicit and assign goals to the right actors.             Gaussian processes. In: Uncertainty in artificial intelligence - Proceedings of the\n     In: Conceptual modeling: 41st international conference. 2022.                                    30th conference. 2013.\n[21] Hutter F, Osborne MA. A kernel for hierarchical parameter spaces. 2013, arXiv.              [50] Jones D. A taxonomy of global optimization methods based on response surfaces.\n[22] Bussemaker JH, Ciampa PD, Nagel B. System architecture design space explo-                       J Global Optim 2001;21:345\u201383.\n     ration: An approach to modeling and optimization. In: AIAA aviation 2020 forum.             [51] Lafage R. egobox, a Rust toolbox for efficient global optimization. J Open Source\n     2020.                                                                                            Softw 2022;7:4737.\n[23] Fouda MEA, Adler EJ, Bussemaker J, Martins JRRA, Kurtulus DF, Boggero L,                    [52] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive\n     et al. Automated hybrid propulsion model construction for conceptual aircraft                    black-box functions. J Global Optim 1998;13:455\u201392.\n     design and optimization. In: 33rd congress of the international council of the              [53] Deng X, Lin CD, Liu K, Rowe RK. Additive Gaussian process for computer models\n     aeronautical sciences. 2022.                                                                     with qualitative and quantitative factors. Technometrics 2017;59:283\u201392.\n[24] Bussemaker JH, Bartoli N, Lefebvre T, Ciampa PD, Nagel B. Effectiveness of                  [54] Cuesta-Ramirez J, Le Riche R, Roustant O, Perrin G, Durantin C, Gliere A. A\n     surrogate-based optimization algorithms for system architecture optimization. In:                comparison of mixed-variables Bayesian optimization approaches. Adv Model\n     AIAA aviation 2021 forum. 2021.                                                                  Simul Eng Sci 2021;9:1\u201329.\n[25] Balandat M, Karrer B, Jiang D, Daulton S, Letham B, Wilson A, et al. BoTorch:               [55] Rebonato R, Jaeckel P. The most general methodology to create a valid\n     A framework for efficient Monte-Carlo Bayesian optimization. Adv Neural Inf                      correlation matrix for risk management and option pricing purposes. J Risk\n     Process Syst 2020;33:21524\u201338.                                                                   2001;2:17\u201327.\n[26] Adams B, Bohnhoff W, Dalbey K, Ebeida M, Eddy J, Eldred M, et al. Dakota,                   [56] Rapisarda F, Brigo D, Mercurio F. Parameterizing correlations: a geometric\n     a multilevel parallel object-oriented framework for design optimization, pa-                     interpretation. IMA J Manag Math 2007;18:55\u201373.\n     rameter estimation, uncertainty quantification, and sensitivity analysis: Version           [57] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. An improved\n     6.13 user\u2019s manual. Technical report, Albuquerque, NM (United States: Sandia                     approach for estimating the hyperparameters of the Kriging model for high-\n     National Lab.(SNL-NM); 2020.                                                                     dimensional problems through the Partial Least Squares method. Math Probl\n[27] Roustant O, Ginsbourger D, Deville Y. DiceKriging, DiceOptim: Two R packages                     Eng 2016;2016:6723410.\n     for the analysis of computer experiments by Kriging-based metamodeling and                  [58] Cheng GH, Younis A, Hajikolaei KH, Wang GG. Trust region based mode pursuing\n     optimization. J Stat Softw 2012;51:1\u201355.                                                         sampling method for global optimization of high dimensional design problems.\n[28] Zhang Y, Tao S, Chen W, Apley D. A latent variable approach to Gaus-                             J Mech Des 2015;137:021407.\n                                                                                                 [59] Karlsson R, Bliek L, Verwer S, de Weerdt M. Continuous surrogate-based\n     sian process modeling with qualitative and quantitative factors. Technometrics\n                                                                                                      optimization algorithms are well-suited for expensive discrete problems. In:\n     2020;62:291\u2013302.\n                                                                                                      Artificial intelligence and machine learning. 2021.\n[29] Chang TH, Wild SM. ParMOO: A Python library for parallel multiobjective\n                                                                                                 [60] Pelamatti J, Brevault L, Balesdent M, Talbi E-G, Guerin Y. Bayesian optimization\n     simulation optimization. J Open Source Softw 2023;8:4468.\n                                                                                                      of variable-size design space problems. Opt Eng 2021;22:387\u2013447.\n[30] Garrido-Merch\u00e1n EC, Hern\u00e1ndez-Lobato D. Dealing with categorical and\n                                                                                                 [61] Hebbal A, Brevault L, Balesdent M, Talbi E-G, Melab N. Bayesian optimization\n     integer-valued variables in Bayesian optimization with Gaussian processes.\n                                                                                                      using deep Gaussian processes with applications to aerospace system design. Opt\n     Neurocomputing 2020;380:20\u201335.\n                                                                                                      Eng 2021;22:321\u201361.\n[31] Halstrup M. Black-box optimization of mixed discrete-continuous optimization\n                                                                                                 [62] Wildberger N. A rational approach to trigonometry. Math Horiz 2007;15:16\u201320.\n     problems (Ph.D. thesis), TU Dortmund; 2016.\n                                                                                                 [63] Cho H, Kim Y, Lee E, Choi D, Lee Y, Rhee W. Basic enhancement strategies when\n[32] Roustant O, Padonou E, Deville Y, Cl\u00e9ment A, Perrin G, Giorla J, et al.                          using bayesian optimization for hyperparameter tuning of deep neural networks.\n     Group kernels for gaussian process metamodels with categorical inputs. SIAM                      IEEE Access 2020;8:52588\u2013608.\n     J Uncertain Quant 2020;8:775\u2013806.                                                           [64] Zuniga MM, Sinoquet D. Global optimization for mixed categorical-continuous\n[33] Zhou Q, Qian PZG, Zhou S. A simple approach to emulation for computer models                     variables based on Gaussian process models with a randomized categorical space\n     with qualitative and quantitative factors. Technometrics 2011;53:266\u201373.                         exploration step. INFOR Inf Syst Oper Res 2020;58:310\u201341.\n[34] Saves P, Diouane Y, Bartoli N, Lefebvre T, Morlier J. A mixed-categorical                   [65] Lindauer M, Eggensperger K, Feurer M, AB, Deng D, Benjamins C, et al. SMAC3:\n     correlation kernel for Gaussian process. Neurocomputing 2023;550:126472.                         A versatile Bayesian optimization package for hyperparameter optimization. J\n[35] Pelamatti J, Brevault L, Balesdent M, Talbi E-G, Guerin Y. Efficient                             Mach Learn Res 2022;23:1\u20139.\n     global optimization of constrained mixed variable problems. J Global Optim                  [66] Picheny V, Berkeley J, Moss H, Stojic H, Granta U, Ober S, et al. Trieste:\n     2019;73:583\u2013613.                                                                                 Efficiently exploring the depths of black-box functions with TensorFlow. 2023,\n[36] Horn D, Stork J, ler N-JS, Zaefferer M. Surrogates for hierarchical search spaces:               arXiv.\n     The Wedge-Kernel and an automated analysis. In: Proceedings of the genetic and              [67] Cowen-Rivers AI, Ly W, Wang Z, Tutunov R, Jianye H, Wang J, et al. HEBO:\n     evolutionary computation conference. 2019.                                                       Heteroscedastic evolutionary Bayesian optimisation. 2020, arXiv.\n[37] Hung Y, Joseph VR, Melkote SN. Design and analysis of computer experiments                  [68] Jiang H, Shen Y, Li Y, Zhang W, Zhang C, Cui B. OpenBox: A Python toolkit for\n     with branching and nested factors. Technometrics 2009;51:354\u201365.                                 generalized black-box optimization. 2023, arXiv.\n\n\n                                                                                            15\n\fP. Saves et al.                                                                                                                Advances in Engineering Software 188 (2024) 103571\n\n\n[69] Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et           [78] Platt J, Penny S, Smith T, Chen T, Abarbanel H. A systematic exploration of\n     al. Tuning hyperparameters without grad students: Scalable and robust bayesian              reservoir computing for forecasting complex spatiotemporal dynamics. Neural\n     optimisation with dragonfly. J Mach Learn Res 2020;21:3098\u2013124.                             Netw 2022;153:530\u201352.\n[70] Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient          [79] Charayron R, Lefebvre T, Bartoli N, Morlier J. Multi-fidelity Bayesian optimiza-\n     global optimization algorithm with multiple infill strategy - Applied to a wing             tion strategy applied to overall drone design. In: AIAA scitech 2023 forum.\n     topology optimization problem. In: AIAA scitech 2019 forum. 2019.                           2023.\n[71] M\u00fcller J, Shoemaker CA, Pich\u00e9 R. SO-MI: A surrogate model algorithm for                [80] Charayron R, Lefebvre T, Bartoli N, Morlier J. Towards a multi-fidelity and\n     computationally expensive nonlinear mixed-integer black-box global optimization             multi-objective Bayesian optimization efficient algorithm. Aerosp Sci Technol\n     problems. Comput Oper Res 2013;40:1383\u2013400.                                                 2023;142:108673.\n[72] Tran T, Sinoquet D, Da Veiga S, Mongeau M. Derivative-free mixed binary                [81] Wold H. Soft modelling by latent variables: The non-linear iterative partial least\n     necklace optimization for cyclic-symmetry optimal design problems. Opt Eng                  squares (NIPALS) approach. J Appl Probab 1975;12:117\u201342.\n     2021.                                                                                  [82] Priem R, Diouane Y, Bartoli N, Dubreuil S, Saves P. High-dimensional efficient\n[73] Meliani M, Bartoli N, Lefebvre T, Bouhlel MA, Martins JRRA, Morlier J. Multi-               global optimization using both random and supervised embeddings. In: AIAA\n     fidelity efficient global optimization: Methodology and application to airfoil              aviation 2023 forum. 2023.\n     shape design. In: AIAA aviation 2019 forum. 2019.                                      [83] Betz W, Papaioannou I, Straub D. Numerical methods for the discretization of\n[74] Lee H. Gaussian processes. Springer Berlin Heidelberg; 2011, p. 575\u20137.                      random fields by means of the Karhunen\u2013Lo\u00e8ve expansion. Comput Methods\n[75] L\u00f3pez-Lopera AF, Idier D, Rohmer J, Bachoc F. Multioutput Gaussian processes                Appl Mech Engrg 2014;271:109\u201329.\n     with functional data: A study on coastal flood hazard assessment. Reliab Eng           [84] Menz M, Dubreuil S, Morio J, Gogu C, Bartoli N, Chiron M. Variance based sen-\n     Syst Saf 2022;218:108139.                                                                   sitivity analysis for Monte Carlo and importance sampling reliability assessment\n[76] Berthelin G, Dubreuil S, Sala\u00fcn M, Bartoli N, Gogu C. Disciplinary proper                   with Gaussian processes. Struct Saf 2021;93:102116.\n     orthogonal decomposition and interpolation for the resolution of parameterized         [85] Ginsbourger D, Le Riche R, Carraro L. Kriging is well-suited to parallelize\n     multidisciplinary analysis. Internat J Numer Methods Engrg 2022;123:3594\u2013626.               optimization. Springer Berlin Heidelberg; 2010, p. 131\u201362.\n[77] Cardoso I, Dubreuil S, Bartoli N, Gogu C, Sala\u00fcn M, Lafage R. Disciplinary             [86] Roux E, Tillier Y, Kraria S, Bouchard P-O. An efficient parallel global opti-\n     surrogates for gradient-based optimization of multidisciplinary systems. In:                mization strategy based on Kriging properties suitable for material parameters\n     ECCOMAS Aerobest. 2023.                                                                     identification. Arch Mech Eng 2020;67.\n\n\n\n\n                                                                                       16\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nSaves et al. ", " Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et al. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J Mach Learn Res 2020;21:3098\u2013124. ", " Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient global optimization algorithm with multiple infill strategy - Applied to a wing topology optimization problem. In: AIAA scitech 2019 forum. 2019."], "publish_date": "2023-12-07", "approved": true}}, {"_index": "articles", "_id": "JetpCI0BUzn7PtN-N4ua", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "Advances in Engineering Software", "abstract": "The Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that minimizes the probability of system failure over the admissible space for the design parameters. It is based on the simulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and exploiting the information contained in these samples to identify subregions for the optimal design parameters within the original design space. This paper presents an improved version of SSO, named iSSO to overcome the shortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design space into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A double-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a generalized design optimization approach primarily tailored for the stochastic structural systems but also adaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness and efficiency of the proposed iSSO.", "authors": ["Mohd Aman Khalid", "Sahil Bansal"], "institutions": ["Department of Civil Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India"], "keywords": ["Stochastic subset optimization", "Voronoi tessellation", "Stochastic simulation", "Stochastic optimization", "Optimization under uncertainty"], "full_text": "                                                               Advances in Engineering Software 188 (2024) 103568\n\n\n                                                                     Contents lists available at ScienceDirect\n\n\n                                                        Advances in Engineering Software\n                                                        journal homepage: www.elsevier.com/locate/advengsoft\n\n\n\n\nImproved stochastic subset optimization method for structural\ndesign optimization\nMohd Aman Khalid , Sahil Bansal *\nDepartment of Civil Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India\n\n\n\n\nA R T I C L E I N F O                                      A B S T R A C T\n\nKeywords:                                                  The Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that mini\u00ad\nStochastic subset optimization                             mizes the probability of system failure over the admissible space for the design parameters. It is based on the\nVoronoi tessellation                                       simulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and\nStochastic simulation\n                                                           exploiting the information contained in these samples to identify subregions for the optimal design parameters\nStochastic optimization\nOptimization under uncertainty\n                                                           within the original design space. This paper presents an improved version of SSO, named iSSO to overcome the\n                                                           shortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design\n                                                           space into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A\n                                                           double-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a\n                                                           generalized design optimization approach primarily tailored for the stochastic structural systems but also\n                                                           adaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness\n                                                           and efficiency of the proposed iSSO.\n\n\n\n\n1. Introduction                                                                                  approaches available in the literature, however, but it\u2019s worth noting\n                                                                                                 that no one-size-fits-all optimization approach is ideal for all sorts of\n    Structural optimization may be defined as the rational establishment                         problems [4\u20137]. The choice of optimization method is often determined\nof an economical structural design with the available resources while                            by the specific characteristics of the problem, such as its complexity,\nsatisfying specific performance criteria. In general terms, the economy                          dimensionality, constraints, and the nature of the objective function. As\nmay be characterized by minimum weight, minimum cost, maximum                                    a result, there is always a scope for new approaches to be developed or\nutility, or even minimum probability of failure. Broadly, structural                             the adaptation of existing methods to better suit specific problem clas\u00ad\noptimization can be categorized into deterministic and stochastic opti\u00ad                          ses. A detailed discussion of deterministic optimization approaches can\nmization [1,2]. The classical statement of unconstraint deterministic                            be found in the literature [8,9].\noptimization is mathematically expressed as:                                                         In any practical situation, several parameters, such as loadings,\n                                                                                                 structural parameters, geometric parameters, operation conditions, etc.,\nminimize : g(\u03c6)                                                                        (1)\n    \u03c6\u2208\u03a6                                                                                          are either not known at the design stage or are subjected to random\n                                                                                                 fluctuations that give rise to performance variability and affect the\nwhere, \u03c6 = [\u03c61 \u22ef\u03c6n\u03c6 ]T \u2208 \u03a6\u2282Rn\u03c6 is a set of deterministic adjustable pa\u00ad                          performance of a system [10]. These parameters are characterized as\nrameters that define the structural design, referred to herein as design                         uncertain parameters. Deterministic structural optimization discards the\nparameters, g(\u03c6) : Rn\u03c6 \u2192R is the objective function to be minimized, and                         impact of uncertainty and can result in improper design. Therefore, it is\n\u03a6 denotes the bounded admissible design space. The deterministic                                 desirable to account for the uncertainty in the parameters during opti\u00ad\nconstraints can be considered by the appropriate definition of the ad\u00ad                           mization by using the rational methods of probabilistic structural\nmissible design space \u03a6 for deterministic design parameters \u03c6, as                                analysis [11]. Such structural optimization that accounts for un\u00ad\nmentioned in [3]. In the deterministic structural optimization problem,                          certainties is called stochastic optimization [12]. Although stochastic\nthe uncertainties in parameters are ignored, and fixed values are                                optimization refers to any method that employs randomness within\nassumed for all the parameters. There are numerous optimizations                                 some communities, in this paper, we will only consider settings where\n\n\n\n * Corresponding author.\n   E-mail addresses: mohdamankhalid@gmail.com (M.A. Khalid), sahil@iitd.ac.in (S. Bansal).\n\nhttps://doi.org/10.1016/j.advengsoft.2023.103568\nReceived 5 June 2023; Received in revised form 2 October 2023; Accepted 24 November 2023\nAvailable online 12 December 2023\n0965-9978/\u00a9 2023 Published by Elsevier Ltd.\n\fM.A. Khalid and S. Bansal                                                                                         Advances in Engineering Software 188 (2024) 103568\n\n\nthe objective function is random. Stochastic optimization or optimal              various challenging optimization problems, it has two shortcomings.\ndesign under uncertainty has been widely applied in many practical                First, the effectiveness of SSO is dependent on the correct selection of\nengineering fields, including civil engineering structures [13\u201315],               the geometrical shape of the admissible subsets. Here, it is pertinent to\ncomposite structures [16,17], and vehicles [18,19].                               mention that choosing a geometrical shape that effectively investigates\n   Consider an engineering system that involves deterministic design              the sensitivity of the objective function to each design variable is\nparameters \u03c6, and uncertain variables \u03b8 = [\u03b81 \u22ef\u03b8n\u03b8 ]T \u2208 \u0398\u2282Rn\u03b8 following           essential. The shapes, such as hyper-rectangle and hyper-ellipse are\na joint PDF p(\u03b8|\u03c6), where \u0398 denotes the parameter space of the uncer\u00ad             suggested in the literature for the admissible subsets. However, as shown\ntain variables. The classical statement of stochastic optimization is             later via the illustrative example, these shapes fail to include the optimal\nmathematically expressed as:                                                      solution in cases with complex design spaces or problems with multiple\n                                                                                  optimal solutions. And second, identifying the optimal subset that con\u00ad\nminimize : E\u03b8 [h(\u03c6, \u03b8)]                                                (2)        tains the smallest volume density involves a non-smooth optimization\n    \u03c6\u2208\u03a6\n                                                                                  problem which is quite challenging.\nwhere, h(\u03c6, \u03b8) : Rn\u03b8 +n\u03c6 \u2192R is the structural performance function, and               In this paper, an improved version of SSO is developed to overcome\nE\u03b8[ \u22c5 ] denotes expectation with respect to the PDF for \u03b8. Note that the          the shortcomings of the original SSO. This new version of the algorithm,\nobjective function in the optimization problem in (2) is the expectation          as mentioned earlier, is named iSSO (improved SSO). Voronoi tessella\u00ad\nE\u03b8 [h(\u03c6, \u03b8)] which is a deterministic function. It\u2019s worth mentioning that        tion is implemented to partition the design space into non-overlapping\nstochastic optimization may also involve other stochastic measures such           subregions (a set of Voronoi cells) using the pool of samples distrib\u00ad\nas variance or quantile values. However, these stochastic measures can            uted according to the auxiliary PDF. The admissible set (a set of all\nrarely be evaluated analytically; therefore, several methods have been            admissible subregions) is then defined as a set containing all subsets of\nproposed for solving stochastic optimization problems. These special\u00ad             the set of Voronoi cells. This approach is able to capture the regions with\nized methods include, for example, sample average approximation,                  lower objective function values even if they are disjointed or when the\nstochastic approximation, stochastic subset optimization, and ap\u00ad                 design space is complex. The details of the Voronoi tessellation are\nproaches based on the use of Taylor series expansion [15,20,21],                  presented in Appendix A. A double-sort algorithm is then implemented\nresponse surface, and metamodels [22\u201325]. Specific to structural engi\u00ad            to identify the optimal subset containing the smallest volume density.\nneering, there are two broad categories of problems involving design                  In the next section, the original SSO is reviewed. Section 3 presents\noptimization under uncertainty [26\u201335]: Reliability-Based Design                  the general theoretical and computational framework for the iSSO al\u00ad\nOptimization (RBDO) and Robust Design Optimization (RDO). The                     gorithm. Section 4 considers several optimization problems to illustrate\nobjective of RBDO is to find an optimal solution that minimizes some              the effectiveness and efficiency of the proposed iSSO algorithm.\ndeterministic, objective function under observance of probabilistic\nconstraints instead of conventional deterministic constraints [36,37].            2. Original stochastic subset optimization\nOn the other hand, RDO aims to find an optimal solution that is insen\u00ad\nsitive (or less sensitive) to input variations. It improves the design                In SSO, say at the i + 1th iteration, the design space is represented by\nquality by minimizing performance variation without eliminating un\u00ad               a subset I(i), where I(i) \u2208 I(i \u2212 1)\u22c5\u22c5\u22c5 \u2208 I(0) \u2208 \u03a6. Following the augmented\ncertainty [29,38].                                                                formulation concept initially discussed in [44] for RBDO, the design\n    Taflanidis and Beck [39] introduced a novel algorithm for optimal             parameters \u03c6, are artificially considered uncertain variables with a\nreliability problem, the so-called SSO. SSO involves formulating an               prescribed PDF p(\u03c6|I(i)) over the design space I(i) [45]. For convenience,\naugmented problem where the design parameters are artificially                    p(\u03c6|I(i)) = 1/V(i) is considered, where V(i) is the volume of I(i). In this\nconsidered uncertain and defining an auxiliary PDF that includes the              setting of the augmented stochastic design problem, the auxiliary PDF is\nstructural performance function and the PDF of the uncertain variables.           defined as:\nNext, SSO involves generating a pool of samples distributed according to                                  (      \u20d2 )\n                                                                                   (     \u20d2 ) h(\u03c6, \u03b8)p \u03c6, \u03b8\u20d2I (i)                (   \u20d2 )\nthis auxiliary PDF and identifying a subregion in the original design             \u03c0 \u03c6, \u03b8\u20d2I (i) =                    \u221dh(\u03c6, \u03b8)p \u03c6, \u03b8\u20d2I (i)                   (3)\nspace, which, on average, improves the value of the objective function.                            E\u03c6,\u03b8 [hs (\u03c6, \u03b8)]\nBy repeating this procedure several times, it is possible to determine at\n                                                                                  where, p(\u03c6, \u03b8|I(i)) = p(\u03b8|\u03c6)p(\u03c6|I(i)). Note that if h(\u03c6, \u03b8)\u2264 0, it must be\neach step a smaller subregion in the design space, which in turn im\u00ad\n                                                                                  suitably transformed to ensure that \u03c0(\u03c6, \u03b8|I(i)) \u2265 0. One way to do this is\nproves the value of the objective function. Ultimately, this subregion\n                                                                                  to define hs(\u03c6,\u03b8) = h(\u03c6, \u03b8) \u2212 s, since E\u03b8[hs(\u03c6,\u03b8)] = E\u03b8[h(\u03c6,\u03b8)] \u2212 s, that is,\nwill be sufficiently small to directly identify the optimal solution or\n                                                                                  the two expected values differ only by a constant, and the optimization\nprovide sufficient information to launch another optimization algo\u00ad\n                                                                                  of the expected value of h( \u22c5 ) is equivalent, in terms of the optimal design\nrithm, such as the sample average approximation or stochastic approx\u00ad\n                                                                                  choice, to optimization for the expected value for hs( \u22c5 ). In the above\nimation. The implementation of the SSO method closely resembles the\n                                                                                  equation, the denominator is a normalizing constant given by:\nSubset Simulation (SS) algorithm [40] for reliability analysis. Since SSO\n                                                                                                   \u222b \u222b\nis based on simulation, it can deal with linear or nonlinear problems and,                                     (      \u20d2 )\n                                                                                  E\u03c6,\u03b8 [h(\u03c6, \u03b8)] =     h(\u03c6, \u03b8)p \u03c6, \u03b8\u20d2I (i) d\u03b8d\u03c6.                            (4)\nat least theoretically, an unbounded number of design parameters. The                                  \u03a6 \u0398\nnumerical effort for solving a given optimization problem is indepen\u00ad\n                                                                                      Although this expected value is not explicitly needed, it can be\ndent of the number of uncertain variables, and it grows linearly with the\n                                                                                  determined using any state-of-the-art stochastic simulation method. The\nnumber of design parameters.\n                                                                                  objective function E\u03b8 [hs (\u03c6, \u03b8)] in this context of the auxiliary PDF is\n    Since the introduction of SSO, several extensions of SSO have been\n                                                                                  expressed as:\nproposed. An extension of SSO termed Non-Parametric SSO, which\n                                                                                                 ( \u20d2 )\nadopts kernel density estimation to approximate the objective function,                         \u03c0 \u03c6\u20d2I (i)\nis presented in [41]. In [42], efficient integration of the Moving Least          E\u03b8 [h(\u03c6, \u03b8)] = ( \u20d2 (i) )E\u03c6,\u03b8 [h(\u03c6, \u03b8)],                               (5)\n                                                                                                p \u03c6\u20d2I\nSquares approximation within SSO is introduced to reduce the compu\u00ad\ntational effort in SSO. In [3], an augmented formulation is presented for         where, the marginal \u03c0(\u03c6|I(i)) is given by:\nthe RDO of structures using SSO. SSO or its variants have also been                           \u222b\napplied to solve structural optimization problems. SSO has been used for           ( \u20d2 )\n                                                                                  \u03c0 \u03c6\u20d2I (i) =   \u03c0(\u03c6, \u03b8)d\u03b8.                                                      (6)\nreliability optimization and sensitivity analysis in system design in [39].                    I (i)\n\nA framework for RDO of Tuned Mass Dampers (TMD) by SSO is dis\u00ad                       In (5), since E\u03c6,\u03b8 [h(\u03c6, \u03b8)] is a normalizing constant, minimization of\ncussed in [43]. Even though SSO has proved to be efficient for meeting\n\n                                                                              2\n\fM.A. Khalid and S. Bansal                                                                                                       Advances in Engineering Software 188 (2024) 103568\n\n\nE\u03b8 [h(\u03c6, \u03b8)] is equivalent to minimization of J(\u03c6), which is equal to:                                                            /\n                               ( \u20d2 )                                                      I (i+1) = argminH(I) = arg min NI          VI\n ( \u20d2 )        E\u03b8 [hs (\u03c6, \u03b8)]  \u03c0 \u03c6\u20d2I (i)                                                                                                                                     (10)\n                                                                       (7)\n                                                                                                      I\u2208A\u03c1\nJ \u03c6\u20d2I (i) =                                                                                                                               .\n                                                                                                                        (i+1)\n                             = ( \u20d2 (i) ).                                                                            I\u2208A\u03c1\n             E\u03c6,\u03b8 [hs (\u03c6, \u03b8)] p \u03c6\u20d2I                                                               {                / }\n                                                                                          A(i+1)\n                                                                                           \u03c1     = I\u2282I (i) : \u03c1 = NI N (i)\n     The estimation of the marginal \u03c0(\u03c6|I(i)) in (7) is necessary to mini\u00ad\n                                                                                              The effectiveness of SSO is dependent on the correct selection of the\nmize J(\u03c6|I(i)). Analytical approximations of \u03c0(\u03c6|I(i)) based on kernel\n                                                                                          geometrical shape and size of the admissible subsets. Choosing a\ndensity approaches or the maximum entropy method might be arduous\n                                                                                          geometrical shape that effectively investigates the sensitivity of the\nin case of complex problems, such as when design parameters n\u03c6 are\n                                                                                          objective function to each design variable is essential. The optimization\nlarge, or the sensitivity for some design parameters is complex [44]. In\n                                                                                          in (10) determines the subset with the smallest average value of J(\u03c6|I(i))\nthe SSO framework, such approximation of \u03c0(\u03c6|I(i)) is avoided. In SSO,\nsamples distributed as \u03c0(\u03c6|I(i)) are obtained, and the information in                     (or equivalently E\u03b8 [hs (\u03c6, \u03b8)]) within the admissible set A(i+1)\n                                                                                                                                                       \u03c1    . I(i + 1) is a\n                                                                                                                          (i)\nthese samples is exploited to identify a smaller subset of the design space               subset of the design space I with a high likelihood of containing the\nwith a high likelihood of containing the optimal design parameters.                       optimal design parameters. The above steps are repeated until the\nSamples distributed as \u03c0(\u03c6, \u03b8|I(i)) are obtained using any appropriate                    stopping criterion is met. This way, SSO adaptively converges to a\nstochastic sampling algorithm, such as Markov Chain Monte Carlo                           relatively small subregion within the original design space. The imple\u00ad\n(MCMC) sampling [46]. The \u03c6 component of these samples then cor\u00ad                          mentation of SSO is demonstrated in Fig. 1. The reader may refer to the\nresponds to samples from the marginal distribution \u03c0(\u03c6|I(i)).                             original publication for a detailed explanation of SSO [39].\n     The sensitivity of objective function E\u03b8 [hs (\u03c6, \u03b8)] to \u03c6 is determined by               H(I(i) ) expresses the average relative sensitivity of E\u03b8[h(\u03c6,\u03b8)] to \u03c6. A\nevaluating the average value (or equivalently volume density) of J(\u03c6|                     low value of H(I(i) ) indicates that E\u03b8[h(\u03c6,\u03b8)] is more sensitive to \u03c6, and\nI(i)) over any subset I in I(i), which is denoted by H(I) and defined as:                 vice versa. A high value of H(I(i) ), close to 1 corresponds to a sample\n           \u222b                   \u222b (          )           \u222b                                 density in design space I(i) that approximates a uniform distribution and\n               (       )     1 \u03c0 \u03c6|I (i)                   (       )\n                                                                                          suggests that the identified subset I(i) has a low likelihood of containing\n         1                                        V (i)\nH(I) =        J \u03c6|I (i) d\u03c6 =       (        ) d\u03c6 = I      \u03c0 \u03c6|I (i) d\u03c6      (8)\n         VI I                VI I p \u03c6|I (i)        VI I\n                                                                                          \u03c6* [39]. Therefore, the SSO is stopped when H(I(i) ) exceeds a threshold\n                                                                                          value. A threshold value of 0.75\u20130.80 has been found to give satisfactory\nwhere, VI is the volume of subset I. Based on the samples distributed\n                                                                                          results [39].\naccording to \u03c0(\u03c6|I(i)) belonging to I(i), an estimate of H(I) is provided by:\n\nH(I) =\n          NI /VI\n                        ,                                                      (9)        3. Proposed approach\n         NI (i) /VI (i)\n                                                                                              In the proposed approach, the Voronoi tessellation is implemented to\nwhere, NI(i) is the number of samples distributed as \u03c0(\u03c6|I(i)) belonging to               partition the design space into non-overlapping subregions (a set of\nI(i), and NI denotes the number of samples from \u03c0(\u03c6|I(i)) belonging to the                Voronoi cells) using the pool of samples distributed according to this\nI (NI < NI(i\u2212 1) since I\u2282I(k \u2212 1)). Say NI = p0 NI(i\u2212 1) . A smaller value of \u03c1 re\u00ad       auxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a\nsults in a faster decrease in the size of the identified subsets but with                 space into convex polygons, called Voronoi cells, such that each cell\npoorer accuracy. The use of \u03c1 equal to 0.1 - 0.2 is suggested in the                      contains exactly one sample, called a cell-generating sample. Every\nliterature [39].                                                                          sample in a given polygon is closer to its generating sample compared to\n      A deterministic optimization, based on the estimate H(I) of H(I), is                any other. In the proposed approach, the admissible set (a set of all\nnext performed to identify the subset I \u2208 A\u03c1 , where A\u03c1\n                                                  (i+1)\n                                                               is a set of\n                                                                  (i+1)\n                                                                                          admissible subspaces) is defined as a set containing all subsets of the set\nadmissible subsets in I(i), that contains the smallest volume density NI/                 of Voronoi cells. An alternative approach to identify the optimal subset\nVI, that is,                                                                              without performing any non-smooth deterministic optimization is also\n                                                                                          presented. The general theoretical and computational framework for the\n                                                                                          iSSO algorithm is presented in the following subsections, and the\n\n\n\n\n                                                           Fig. 1. Illustration of the original SSO algorithm.\n\n                                                                                      3\n\fM.A. Khalid and S. Bansal                                                                                             Advances in Engineering Software 188 (2024) 103568\n\n\nalgorithm is demonstrated in Fig. 2.\n                                                                                      3.2. Identification of an optimal subset\n3.1. Partitioning of design space\n                                                                                         A deterministic optimization needs to be performed to identify a\n                                                                                      subset I that contains the smallest volume density NI/VI. In the case of\n     In the proposed approach, at the i + 1th iteration, say N(i) is the\nnumber of samples distributed as \u03c0(\u03c6|I(i)) belonging to the design space              unique samples, since \u03b7(\u22c5) = 1, the solution to the minimization problem\n                                                                                                               (i)\n\n\nI(i). Let nv = N(i) /(1 + \u03b3), \u03b3 \u2265 0 be the number of unique samples. If               in (10) is a set of \u03c1N(i) Voronoi cells with the largest volume. For the case\nsampling techniques such as accept rejection, importance sampling, etc.,              with repeated samples, the optimization can be performed using\nare used, then \u03b3 = 0, and each sample in the design space will be unique.             methods appropriate for non-smooth optimization problems, such as\nHowever, if MCMC sampling techniques are used, the resulting samples                  sub-gradient methods, bundle methods, gradient sampling methods, etc.\nwill be correlated, that is \u03b3 > 0, and we will have repeated samples.                     In this study, we propose an alternative approach to identify the\nAssume that the design space I(i) is divided into vk , k = 1\u22c5\u22c5\u22c5nv, Voronoi\n                                                      (i)                             optimal subset without performing any non-smooth deterministic opti\u00ad\n                                                                                      mization. A double-sort algorithm is proposed, which involves sorting\ncells using nv unique samples, and say the Voronoi cell vk contains \u03b7k\n                                                              (i)           (i)\n                                                                                      the Voronoi cells in ascending order of the sample counts and then in\nrepeated samples, then, an estimate of \u03c0(\u03c6|I(i)) is provided by:                      groups of cells with the same sample count in descending order of cell\n (      )        \u03b7(i)                                                                 volume. Finally, the top cells containing \u03c1N(i)samples are selected as an\n\u03c0 \u03c6|I (i) =       k\n                            \u2265 0,   \u2200 \u03c6 \u2208 v(i)\n                                          k ,                            (11)         approximate optimal solution from the sorted list.\n              N (i) Vk(i)\n                                                                                          One may argue that the optimal subset can be obtained by first\n                                                                  \u222b                   sorting the Voronoi cells in ascending order of the cell density, defined\nwhere, Vk is the volume of the kth Voronoi cell. Obviously, I(i) \u03c0(\u03c6|I(i) )\n             (i)\n                                                                                      as \u03b7k /Vk , and then by selecting the top cells containing \u03c1N(i) samples\n                                                                                          (i)       (i)\nd\u03c6 = 1.\n                                                                                      from the sorted list. However, this argument is erroneous because the\n    Similar to the original SSO, the sensitivity of the objective function J                                  \u2211      (i) \u2211                  \u2211\n(\u03c6|I(i)) to \u03c6 is determined by evaluating the average value of J(\u03c6|I(i))              objective is to minimize Ss=1 \u03b7(s) / Ss=1 V(s) and not Ss=1 (\u03b7(s) /V(s) ). The\n                                                                                                                                 (i)                (i)   (i)\n\nover any subspace I of the design space I(i). Subset I is any subset of               effectiveness of the proposed double-sort algorithm is demonstrated in\nnvVoronoi cells (these cells may be disjointed). Since the design space is            Section 4 with the help of examples.\npartitioned into nv subspaces or Voronoi cells, the number of admissible\nsubsets (proper subsets) is given by 2n\u03bd \u2212 1. Based on the estimate                   3.3. Simulation of conditional samples\n\u03c0 (\u03c6|I(i) ) provided in (11), an estimate of H(I) is provided as:\n                 \u222b                     \u2211 (i)                                                At the i + 1th iteration, \u03c1N(i) samples distributed as \u03c0(\u03c6|I(i + 1)) are\nH(I) =\n           V (i)    (       )\n                   \u03c0 \u03c6|I (i) d\u03c6 =\n                                  V (i) I \u03b7i\n                                             =\n                                               V (i) NI\n                                                                        (12)          available from the previous iteration. Using these samples as seeds,\n           VI I                   VI N (i)     VI N (i)                               additional (1 \u2212 \u03c1)N(i + 1) are simulated. The proposed method to\n                                                                                      simulate additional samples involves two steps: (a) randomly selecting a\nwhere, VI is the volume of the subset I and NI is the number of samples\n                                                                                      Voronoi cell within the subset I(i + 1) based on the estimate \u03c0 (\u03c6|I(i) ) and\nbelonging to it. Let I = {v(1) , v(2) \u22efv(S) }, where S is the number of Vor\u00ad\n                                    (i)   (i)   (i)\n                                                                                      (b) applying the Metropolis-Hastings algorithm within the selected\nonoi cells defining the subset I. Note that the parentheses are used in the           Voronoi cell.\nsubscript to differentiate between the Voronoi cell number defined in                       A Voronoi cell is selected according to the following weights in the\nthe previous section from the Voronoi cell index describing the subset I.             first step:\nAn estimate of H(I) is then provided as:                                                             /\n            [ (i)                     ]                                                         \u03b7(i)\n                                                                                                 k    Vk(i)\n                     (i)         (i)\n       V (i) \u03b7(1) + \u03b7(2) + \u22ef + \u03b7(S)\n                                                                                        (i)\n                                                                                      wk = \u2211 / .                                                                (14)\nH(I) = (i) (i)        (i)         (i)\n                                        .                              (13)                      \u03b7(i)\n                                                                                                   k   Vk(i)\n       N     V(1) + V(2)  + \u22ef + V(S)                                                            k\n\n\n\n\n                                                       Fig. 2. Illustration of the proposed iSSO algorithm.\n\n                                                                                  4\n\fM.A. Khalid and S. Bansal                                                                                       Advances in Engineering Software 188 (2024) 103568\n\n\n   To simulate a new sample within a selected Voronoi cell, the sample           3.5. Implementation issues\nthat generated the selected Voronoi cell or the last simulated sample in\nthe selected Voronoi cell is used as the seed sample, and the Metropolis-             An important issue for the effective implementation of the iSSO is the\nHastings algorithm is implemented. A candidate sample [\u03c6c,\u03b8c] is                 creation of the Voronoi cells at the current iteration bounded within the\nsimulated using the proposal q(\u03c6c,\u03b8c|\u03c6,\u03b8) and is accepted with the               Voronoi cell created at the previous iterations. Although it is possible to\nprobability min(1, a0), where, a0 is given as:                                   create such bounded Voronoi cells, due to the geometrical complexities,\n                                                                                 it is usually unfeasible for the higher dimensional problems (n\u03c6>2). An\n       h(\u03c6c , \u03b8c )p(\u03c6c , \u03b8c )q(\u03c6, \u03b8|\u03c6c , \u03b8c )\na0 =                                         .                        (15)       alternative approach is proposed in the present study for creating the\n         h(\u03c6, \u03b8)p(\u03c6, \u03b8)q(\u03c6c , \u03b8c |\u03c6, \u03b8)\n                                                                                 Voronoi cells at any iteration of the iSSO. The proposed approach in\u00ad\n    In the present study, the proposed PDF is equal to the uniform PDF           volves creating Voronoi cells using the samples generated at the current\nfor design parameters and the initial PDF for uncertain variables, i.e., q       and all previous iterations and then by considering Voronoi cells cor\u00ad\n(\u03c6, \u03b8|\u03c6c,\u03b8c) = p(\u03c6, \u03b8). Therefore, on simplifying (15), a0 is given as:          responding to the samples from the current iteration. This is shown in\n                                                                                 Fig. 3, where Fig. 3(a) shows the N samples at the first iteration and the\n       h(\u03c6c , \u03b8c )\na0 =              .                                                   (16)       corresponding Voronoi cells. Fig. 3(b) shows the \u03c1N selected Voronoi\n        h(\u03c6, \u03b8)\n                                                                                 cells leading to the smallest volume density and the additional (1 \u2212 \u03c1)N\n                                                                                 samples being generated using these \u03c1N samples as seeds. Fig. 3(c)\n3.4. Stopping criteria                                                           shows that the Voronoi cells are generated using all N + (1 \u2212 \u03c1)N\n                                                                                 samples that are generated in the two iterations. The Voronoi cells\n    A new stopping criterion is proposed in this study. The convergence          corresponding to the N samples for consideration at the second iteration\nof the expected value of the performance measure h(\u03c6, \u03b8) with respect to         are also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of\nthe PDF for \u03c6 and \u03b8 in consecutive iterations is used as the stopping            Fig. 3(c) where it can be observed that the area covered by the N Voronoi\ncriterion. Mathematically the proposed stopping criterion is represented         cells considered in the second iteration is not the same as the area\nby:                                                                              covered by the \u03c1N Voronoi cells selected in the first iteration. On the\n\u20d2                                     \u20d2\n\u20d2E\u03c6,\u03b8 [h(\u03c6, \u03b8)]i \u2212 E\u03c6,\u03b8 [h(\u03c6, \u03b8)]i\u2212 1 \u20d2 \u2264 \u03b5                         (17)         contrary, the area covered by the Voronoi cells in the second iteration is\n                                                                                 more than the area covered by the Voronoi cells corresponding to the\nwhere, \u03b5 is a user-specified tolerance limit. Other stopping criteria, as        seed samples from the first iteration. This is because a new sample\nindicated in [39,47], can also be chosen.                                        within the Voronoi cell between an existing sample and the existing\n                                                                                 Voronoi cell edge results in the relocation of the Voronoi cell edge in a\n\n\n\n\n                                                  Fig. 3. Implementation of Voronoi tessellation in iSSO.\n\n                                                                             5\n\fM.A. Khalid and S. Bansal                                                                                             Advances in Engineering Software 188 (2024) 103568\n\n\ndirection away from the new sample. The increase at each iteration in\u00ad             performed. In the third example, the mean minimization of 120 bars\ntroduces a bias in the estimate of \u03c0(\u03c6|I(i)) in (11). However, this does not       truss problems is explored to demonstrate the applicability of the pro\u00ad\naffect the performance of the proposed approach as the objective is not            posed approach to a high-dimensional stochastic design problem.\nto simulate the samples distributed as \u03c0(\u03c6|I(i)) but to identify the subsets       Finally, the fourth example investigates the reliability-based optimiza\u00ad\nfor an optimal solution. In addition, the increase is not substantial, as          tion of a base isolation system for a 10-story building.\nseen later in the illustrative examples in Section 4.                                  In this study, after implementing iSSO, the optimal design solution is\n                                                                                   identified as follows. Let \u03b8j, j = 1\u22c5\u22c5\u22c5n be a set of independent, identically\n                                                                                   distributed realizations of \u03b8, and let h(\u03c6, \u03b8j) be the structural perfor\u00ad\n3.6. Special case: deterministic optimization\n                                                                                   mance function realization for \u03b8j. The expected structural performance\n                                                                                   function is approximated by the average of the realizations as:\n    In the iSSO framework, a deterministic optimization problems can\nalso be handled with the vector of uncertain variables \u03b8 set equal to a                             1\u2211 n\n                                                                                                           (     )\nnull vector (n\u03b8 = 0). Since the determination of the subset at each iSSO           E\u03b8 [h(\u03c6, \u03b8)] \u2248         h \u03c6, \u03b8j .                                               (18)\n                                                                                                    n j=1\niteration is solely dependent on the samples distributed as \u03c0(\u03c6), no\nmodification to the iSSO algorithm is required to solve a deterministic                E\u03b8 [h(\u03c6, \u03b8)] is evaluated for all unique \u03c6 samples obtained at the last\noptimization problem, and the entire formulation remains valid.                    iteration of the iSSO, and the \u03c6 sample resulting in the smallest value of\n                                                                                   E\u03b8 [h(\u03c6, \u03b8)] is taken as the optimal solution. Alternatively, as the right-\n4. Illustrative examples                                                           hand side of (18) is deterministic, any deterministic optimization\n                                                                                   method can also be used to solve the optimization problem with the\n    In this section, typical optimization problems are considered to               approximate expectation.\ndemonstrate the effectiveness and efficiency of the proposed approach.                 In the following examples, both iSSO and SSO are implemented with\nFirst, deterministic optimization problems are considered. These prob\u00ad             N = 1000n\u03c6, \u03c1 = 0.20 and the stopping criteria as stated in (17). Here, a\nlems include several local and global minima. Next, stochastic optimi\u00ad             value of \u03b5 = 10\u2212 3 is adopted.\nzation problems are illustrated. The second example presents an RDO\nproblem of the TMD. In this example, the variance minimization of the\nprotected structure\u2019s displacement (TMD attached to the structure) is\n\n\n\n\n                                                          Fig. 4. Results for the Griewank function.\n\n                                                                               6\n\fM.A. Khalid and S. Bansal                                                                                                Advances in Engineering Software 188 (2024) 103568\n\n\n\n\n                                                           Fig. 5. Results for the Cross-in-Tray function.\n\n\n4.1. Multimodal deterministic optimization problems                                                    \u20d2                         \u221a\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u20d2)\u20d2\n                                                                                                                           (\u20d2\n                                                                                                       \u20d2                    \u20d2     \u03c621 + \u03c622 \u20d2\u20d2 \u20d2\u20d2\n                                                                                                       \u20d2                    \u20d2\n   In this section, three two-dimensional benchmark deterministic                          minh(\u03c6) = \u2212 \u20d2sin(\u03c61 )cos(\u03c62 )exp \u20d21 \u2212\n                                                                                                       \u20d2                    \u20d2         \u03c0\n                                                                                                                                                  \u20d2 \u20d2,\n                                                                                                                                                  \u20d2 \u20d2               (21))\noptimization problems are considered. Results are also compared with\nthe SSO. The test functions are:                                                           s.t.\u03c6 = [ \u2212 10, 10]\n\na) Griewank function:\n                                           ( )\n                 \u2211\n                 d\n                   \u03c62               \u220f\n                                    d\n                                            \u03c6                                              The results for the Griewank function are presented in Fig. 4. Fig. 4(a,\n    minh(\u03c6) =               i\n                                \u2212       cos \u221ai\u0305 + 1,                                   b) shows that the function has multiple closely spaced local minima with\n                  i=1\n                      4000          i=1       i                           (19)\n                                                                                       a single global minimum. Fig. 4(c, d) shows the SSO optimization using\n    s.t.\u03c6 = [ \u2212 10, 10]                                                                hyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is\n                                                                                       seen that these shapes fail to capture the region containing the optimal\n                                                                                       design due to the presence of multiple local minima. Next, the iSSO is\nb) Cross-in-Tray function:                                                             implemented, where the Voronoi cells selected at the first and last\n                      (\u20d2\n                        \u20d2\n                                          (\u20d2\n                                           \u20d2      \u221a\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u20d2\u20d2)\u20d2\u20d2  )0.1         iteration are shown in Fig. 4(e, f). It is observed that at the first iteration,\n                        \u20d2                  \u20d2       \u03c621 + \u03c622 \u20d2 \u20d2\n   minh(\u03c6) = \u2212 0.0001 \u20d2sin(\u03c61 )sin(\u03c62 )exp \u20d2100 \u2212                   \u20d2 \u20d2+1      ,       the selected Voronoi cells effectively capture both the local and global\n                        \u20d2                  \u20d2           \u03c0            \u20d2 \u20d2\n                                                                                       minima and in the subsequent iterations, the selected cells are more\n    s.t.\u03c6 = [ \u2212 10, 10]                                                                concentrated near the global minimum. The region selected at the last\n                                                                          (20)         iteration captures the optimal global solution.\n                                                                                           The Cross-in-Tray function has a relatively complex design space\n                                                                                       compared to the Griewank function. Fig. 5(a, b) shows multiple local\nc) Holder Table function:                                                              and global minima. Minimization by using SSO is demonstrated in Fig. 5\n                                                                                       (c, d). It is found that both the hyper-rectangle and hyper-ellipse are\n                                                                                       trapped around any one of the global minima. At the same time, the iSSO\n                                                                                       is able to capture the regions that include all of the global minima, as\n\n\n                                                                                   7\n\fM.A. Khalid and S. Bansal                                                                                                Advances in Engineering Software 188 (2024) 103568\n\n\n\n\n                                                        Fig. 6. Results for the Holder Table function.\n\n\nseen in Fig. 5(e, f).                                                              three functions. Fig. 7 shows the value of H(I(1) ) for the 50 indepen\u00ad\n    The Holder Table function has multiple local and global minima; the            dent simulation runs, which is estimated by implementing the proposed\nglobal minima are placed at the boundary of the design space, as shown             double sort algorithm and by using the Genetic algorithm. It can be\nin Fig. 6(a, b). Once again, it is seen that both the hyper-rectangle and          noted that for each run, the H(I(1) ) values obtained using the proposed\nhyper-ellipse are trapped around any of one of the global minima, and              double sort algorithm and Genetic algorithm are well matched, thereby\non the other hand, the iSSO is able to capture the regions that include all        confirming the adequacy of the proposed double sort algorithm.\nof the global minima, as seen in Fig. 6(e, f).                                        At any iteration of iSSO, new samples are simulated using the seed\n    The results from the three examples demonstrate that the proposed              samples. In the proposed approach, the volume of the Voronoi cells\niSSO is able to capture the regions containing the optimal solution                corresponding to the seed and new samples is greater than the volume of\neffectively.                                                                       the Voronoi cells corresponding only to the seed samples. Fig. 8 shows\n    Next, the statistics of the results of 50 independent runs, both for SSO                                     \u2212 V\n                                                                                   this change in volume V V(seeds)     due to the creation of Voronoi cells\n                                                                                                           (seeds+new)     (seeds)\n\nand iSSO are presented in Table 1. It also includes the results obtained by\n                                                                                   at any generation of iSSO using the procedure mentioned in Section 3.4.\nusing state-of-the-art approaches, such as the Genetic algorithm, particle\n                                                                                   The increase is observed to be small which further reduces with an in\u00ad\nswarm optimization, and the gradient based optimization approach\n                                                                                   crease in the iteration number. It is also observed that the increase in\n(interior-point algorithm). The proposed iSSO outperforms all other\n                                                                                   volume decreases with an increase in sample size at each iteration and\napproaches as more successes in determining the optimal solution are\n                                                                                   increases with an increase in the dimension of the problem.\nobserved in all three optimization problems. It is also seen that both SSO\nand iSSO result in a similar value of volume reduction for the same\nstopping criterion; however, with SSO, the number of iterations required           4.2. Robust design optimization of the tuned mass damper\nto achieve this volume reduction are relatively higher. The proposed\napproach outperformed the state-of-the-art approaches, as indicated by                 This example considers a stochastic design problem involving a\nthe number of successes. These examples demonstrate that the main                  Tuned Mass Damper (TMD) attached to a Single Degree of Freedom\nadvantage of implementing Voronoi tessellation is an effective explo\u00ad              (SDOF) system. The problem is taken from [48] and is shown in Fig. 9.\nration of the design space.                                                            In this problem, the system is excited by a white noise signal with a\n    Next, the performance of the proposed \"double sort algorithm\" for              mean zero and unit variance. The performance measure is the variance\nselecting the optimal subset is studied by using the above-mentioned               of the displacement of the system \u03c3 2xs . The mass mS, stiffness kS, and\n\n                                                                               8\n\fM.A. Khalid and S. Bansal                                                                                               Advances in Engineering Software 188 (2024) 103568\n\n\nTable 1\nStatistics of optimization results for multimodal deterministic optimization problems.\n  Example                                SSO                                                  iSSO                GA*                     PSO*                GBA*\n\n                                         Hyper-Rectangle            Hyper-Ellipse\n\n  Griewank                  NF           30                         32                        5                   38                      35                  47\n                            NS           20                         18                        45                  12                      15                  3\n                            BV           0                          0                         0                   0                       0                   0\n                            WV           0.1028                     0.1161                    0.0270              0.0296                  0.0232              0.0296\n                            AV           0.0190                     0.0292                    0.0110              0.0094                  0.0057              0.0173\n                            c.o.v        1.1663                     1.0168                    0.5137              0.9603                  0.8763              0.05971\n                            FE           22,702                     15,223                    12,426              3385                    1432                33\n                            Gen          7                          5                         4                                              N/A\n                            VR           94.025                     84.79                     93.56\n  Cross-In-Tray             NF           50                         50                        1                   50                      50                  50\n                            NS           0                          0                         49                  0                       0                   0\n                            BV           \u2212 2.0576                   \u2212 2.0626                  \u2212 2.0624            \u2212 2.0626                \u2212 2.0626            \u2212 2.0626\n                            WV           \u2212 2.0472                   \u2212 2.0481                  \u2212 2.0260            \u2212 2.0626                \u2212 2.0626            \u2212 1.3853\n                            AV           \u2212 2.0527                   \u2212 2.0621                  \u2212 2.0522            \u2212 2.0626                \u2212 2.0626            \u2212 1.7360\n                            c.o.v        0.0133                     0.001                     0.0042              0                       0                   0.0977\n                            FE           27,563                     21,595                    9982                3178                    933                 32\n                            Gen          9                          7                         3                                              N/A\n                            VR           99.89                      98.95                     88.71\n  Holder-Table              NF           50                         50                        3                   50                      50                  50\n                            NS           0                          0                         47                  0                       0                   0\n                            BV           \u2212 19.2085                  \u2212 17.5025                 \u2212 19.2085           \u2212 19.2085               \u2212 19.2085           \u2212 19.2085\n                            WV           \u2212 18.8916                  \u2212 1.1419                  \u2212 17.3030           \u2212 9.5047                \u2212 15.1402           \u2212 1.1831\n                            AV           \u2212 19.0916                  \u2212 8.432                   \u2212 18.8798           \u2212 19.0144               \u2212 18.9745           \u2212 6.5493\n                            c.o.v        0.0025                     0.3898                    0.0182              0.0722                  0.0443              0.8358\n                            FE           40,700                     24,684                    18,142              3413                    988                 30\n                            Gen          13                         8                         6                   N/A\n                            VR           99.59                      99.42                     94.37\n\nGA = genetic algorithm, PSO = particle swarm optimization, GBA = gradient-based optimization approach, NF = no. of. failure, NS = no. of. success, BV = best value,\nWV = worst value, AV = average value, c.o.v = coefficient of variation, FE = no. of. function evaluations, Gen = generations, VR = volume reduction percentage, * =\nefficiently applicable only for deterministic problems.\n\n\n\n\n                                           Fig. 7. Comparison of double sort algorithm and Genetic algorithm results.\n\n\n\n\n                                                     Fig. 8. Percentage change in volume at each iSSO iteration.\n\n\ndamping cS of the system are taken as uncertain parameters, following                   parameters mT,\u03c9T,and\u03c9S are, in order, the mass of the TMD, the natural\n                                                                                                                  \u221a\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nindependent Gaussian distribution. The mean value of these variables is                 frequency of the TMD ( kT /mT ), and the natural frequency of the\ntaken to be 105 kg, 107 N/m, and 4 \u00d7 104 Ns/m respectively. To account                             \u221a\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n                                                                                        structure ( ks /ms ). The optimization problem is written as:\nfor uncertainty, the c.o.v value for each variable taken is 0.05. The\n                                                                                                                            [(              )]2\nfrequency ratio \u03b2 = \u03c9T /\u03c9S and damping \u03beT of the TMD are considered                     minimize :   E\u03b8 [h(\u03b8, \u03c6, \u03c6)] = E\u03b8    \u03c3 2xs (\u03b8, \u03c6) \u2212 \u03c6     ,                  (22)\ndesign parameters. The TMD has a mass ratio, mT/ms, of 0.10. The                         \u03c6\u2208\u03a6,\u03c6\u2208\u03a6\n\n\n\n\n                                                                                    9\n\fM.A. Khalid and S. Bansal                                                                                                    Advances in Engineering Software 188 (2024) 103568\n\n\n                                                                                             iSSO are shown. SAA is applied with a sample size of 103, as mentioned\n                                                                                             in [43]. The results demonstrate that iSSO is effective in locating the\n                                                                                             optimal solution. SSO implemented with hyper-ellipse gives an optimal\n                                                                                             solution but has a higher computational cost.\n\n\n                                                                                             4.3. 120-bars truss structure\n\n                                                                                                 The third example involves minimizing the mean of the compliance\n                                                                                             of a 120-bar linear elastic truss structure shown in Fig. 10 under the\n                                                                                             weight constraint W \u2264 15, 000kg. Because of structural symmetry,\n                                                                                             design parameters corresponding to the cross-sectional areas of elements\n                                                                                             are divided into seven groups, each with a minimum area of 10\u2212 4 m2.\n                                                                                             The Young\u2019s modulus for the bar groups are assumed as uncorrelated\n                                                                                             normal random variables with mean values equal to 210 GPa and the c.\n                                                                                             o.v equal to 0.10 respectively. The density of the material is 7971.89 kg/\n                                                                                             m3. The dome is subjected to concentrated vertical loads acting down\u00ad\n                                                                                             ward at the top node, normally distributed with a mean equal to 60 kN\n                                                                                             and c.o.v equal to 0.20. In addition, the mass of bars is concentrated at\n                                                                                             the nodes. The problem is taken from [48].\n                 Fig. 9. TMD attached to a SDOF system [48].                                     Table 3 presents the best of 10 independent run results obtained with\n                                                                                             SSO and iSSO. Once again, the SSO and iSSO solutions agree well,\n                                                                                             thereby demonstrating the effectiveness of the proposed approach. At\nwhere,\n                                                                                             the same time, the number of function evaluations is substantially less in\n0.01 \u2264 \u03b2 \u2264 1.5, 0.01 \u2264 \u03beT \u2264 1.0, 0 \u2264 \u03c6 \u2264 1000.                                (23)           the case of iSSO, indicating the efficiency of the proposed approach.\n   Table 2 presents the optimal design parameter values as well as the\nobjective function value that solve the optimization problem in (22).                        4.4. Reliability-based design of a base isolated structure\nResults obtained using SSO, Sample Average Approximation (SAA), and\n                                                                                                This example, adapted from [49], involves the reliability-based\n\nTable 2\nVariance minimization of TMD-structure.\n  Method                    Admissible Subset shape           Design parameters                         E\u03b8 [h(\u03b8, \u03c6, \u03c6)]            FE                     NS              NF\n                                                              \u03b2                 \u03beT                      (\u00d7 10\u2212 16 mm4)\n\n  SSO [48]                  Hyper-Rectangle                   0.551             0.623                   41.324                     7433                   0               50\n                            Hyper-Ellipse                     0.749             0.221                   1.7586                     8245                   34              16\n  SAA                       N/A                               0.749             0.221                   1.7587                     3 \u00d7 106                4               46\n  iSSO                      Voronoi tessellation              0.749             0.221                   1.7586                     6198                   50              0\n\n\n\n\n                                                               Fig. 10. 120-bar dome truss structure [48].\n\n\nTable 3\nResults for the 120 bars truss structure.\n  Method             Design parameters                                                                                                 \u03c6\u2217 )\n                                                                                                                                   \u03bcg (\u0303           \u03c32g (\u0303\n                                                                                                                                                        \u03c6\u2217 )         FE\n                     A1             A2                A3              A4             A5              A6             A7             (Nm)            (Nm)2\n                     (cm2)          (cm2)             (cm2)           (cm2)          (cm2)           (cm2)          (cm2)\n\n  SSO [48]           47.2                67.5         42.7            9.4            30.1            55.6           13.1           242.3           5294.2            54,700\n  SAA                47.3                68.3         40.7            10.5           30.3            49.5           14.8           243.5           5407.4            3 \u00d7 106\n  iSSO               48.1                66.4         42.6            8.5            31.4            56.5           12.5           242.8           5317.2            14,937\n\n\n                                                                                      10\n\fM.A. Khalid and S. Bansal                                                                                                                Advances in Engineering Software 188 (2024) 103568\n\n\n\n\n                                   Fig. 11. (left) 10-story base isolated shear model, and (right) force-deformation of bilinear isolator [49].\n\n                                                                                                                                  /\nTable 4                                                                                                           2\u03b6g\n                                                                                                       2\n                                                                                                S0 = \u03c3 \u03c9      (           )m  2\n                                                                                                                                  s3 ,                                               (27)\nBase isolation structure system optimization results (best of 10 independent\n                                                                                                           \u03c0\u03c9g 4\u03b62g + 1\nruns).\n  Method      Design parameters (\u03c6*)                                  Failure\n                                                                                                where, \u03c9g, \u03b6gand \u03c3\u03c9 are the resonant frequency, damping, and RMS of the\n              Kpr (MN/     Fy                    Kp (MN/   cd (MNs/   probability\n              m)           (MN)                  m)        m)         PF(\u03c6*)                    acceleration input of the filter, respectively. These are also considered\n                                                                                                uncertain variables with mean values of [2\u03c0rad/s, 0.5, 0.2g] and a c.o.v\n  SSO         425.33            1.20             15.52     6.54       0.0340\n    [39]\n                                                                                                equal to 0.20. The non-stationarity of the excitation is modeled by\n  SAA         414.68            1.16             16.15     6.26       0.0324                    multiplying the filter output with the envelope function as:\n  iSSO        418.34            1.11             15.88     7.08       0.0366\n                                                                                                e(t) = \u03bb3 t\u03bb1 exp( \u2212 \u03bb2 t),                                                          (28)\n\noptimization of a base-isolation system attached to a 10-story building                         with parameters \u03bb1 = 1.25, \u03bb2 = 0.2 and \u03bb3 = 0.353 chosen to simulate\nas shown in Fig. 11. This optimization problem includes maximizing the                          strong earthquake excitation for a duration of 40 s with a sampling time\nreliability of the base-isolated structure which is performed by the                            of 0.02 s. The base-isolation system considered is a lead\u2013rubber bilinear\nminimization of its failure probability and mathematically expressed as:                        isolator with an additional viscous damper. The base has a 247-ton mass.\n                                     \u222b                                                          The design parameters \u03c6 for the base isolation structure system are the\nminimize : P(F|\u03c6) = E\u03b8 [IF (\u03c6, \u03b8)] = IF (\u03c6, \u03b8)p(\u03c6, \u03b8)d\u03b8,           (24)                         stiffness before yielding Kprand after yielding Kp, the yield force is Fy,\n                                                                                                and the damping coefficient cd. The reader may refer to [39,50] for\n    \u03c6\u2208\u03a6\n                                                   \u0398\n                                                                                                additional details regarding the base isolation structure system adopted\nwhere, IF(\u03c6,\u03b8) is the function that indicates failure, and it equals 1 when                     in this study.\nthe system fails, i.e., when unacceptable performance occurs. Notably,                              Failure is indicated when any of the normalized base displacements\nin this problem h(\u03c6, \u03b8) = IF(\u03c6,\u03b8).                                                              or inter-story drifts exceeds unity. The normalization constants are 0.5 m\n    The 10-story building is considered as a shear structure with un\u00ad                           and 0.033 m respectively. The design interval for each variable is\ncertain inter-story stiffness and damping. Each story has a total mass of                       specified as Kpr = [50, 600] MN/m, Fy = [1, 8] MN, Kp = [5, 60] MN/m,\n207 ton. The inter-story stiffness ki of all stories are parameterized by ki                    and cd = [0.1, 10]MNs/m. In this example, iSSO and SSO are imple\u00ad\n=\u0302 k i \u03b8i , i = 1, \u2026, 10 where the most probable values of the inter-story                      mented with six number of iterations.\n                                                                                                    Table 4 shows the optimization results for the best 10 independent\n               k i ] = [687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6,\nstiffness are [\u0302\n                                                                                                simulation runs. The comparison of the results obtained using SSO, SAA\n225.6, 184.5, 104.5] MN/m. The entity \u03b8i is a set of non-dimensional\n                                                                                                (with a sample size of 103), and iSSO shows that the optimal design\nuncertain variables that are considered to be correlated Gaussian vari\u00ad\n                                                                                                obtained using the proposed approach iSSO is in good agreement. The\nables with a unit mean value \u0302  \u03b8 i = 1, \u2200i and a covariance matrix defined\n                                                                                                failure probability of the structure is reduced from 0.95 (without the\nas:\n                                                                                                base isolation system) to 0.0326 after installing the optimally designed\n [\nE (\u03b8i \u2212 \u0302\n             (\n        \u03b8 i ) \u03b8j \u2212 \u0302\n                      )]           [          / ]\n                   \u03b8 j = (0.2)2 exp \u2212 (j \u2212 i)2 22 .                                 (25)        base isolation system.\n\n    The damping ratios are considered independent Gaussian variables                            5. Conclusion\nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The Kanai-\nTajimi model is used to simulate the ground excitation modelled as a                                This study attempts to provide an optimization approach called\nfiltered white noise process, with the power spectral density function                          \"iSSO\", which is an improved version of SSO, primarily for stochastic\ngiven as:                                                                                       optimization problems while it retains utility for deterministic optimi\u00ad\n                                                                                                zation problems as well. Two novel ideas are introduced in this study:\n                \u03c94g + 4\u03b62g \u03c92g \u03c92\nS(\u03c9) = S0 (             )2                   ,                                      (26)        first, a better characterization of the design space is offered by parti\u00ad\n             \u03c92g \u2212 \u03c92        + 4\u03b62g \u03c92g \u03c92                                                      tioning the design space into non-overlapping subregions using Voronoi\n\n\n                                                                                           11\n\fM.A. Khalid and S. Bansal                                                                                                      Advances in Engineering Software 188 (2024) 103568\n\n\ntessellation which improves the effectiveness and efficiency of the pro\u00ad                     Voronoi cells. At present the methods available in the literation for\nposed iSSO considerably in comparison to SSO. Second, a novel \"double                        creating the Voronoi tessellation are computationally demanding when\nsort\" approach is proposed, eliminating the need for optimization to                         considering problems of very high dimension. Future work will focus on\nidentify the subregions for the optimal design at each iSSO iteration.                       developing a method for creating the Voronoi tessellation in higher di\u00ad\nSeveral mathematical and engineering design examples, including TMD,                         mensions, particularly those greater than ten.\n120 bars truss structure, and base-isolated structure, are included in this\nstudy to demonstrate the efficacy of the proposed iSSO. The results show                     CRediT authorship contribution statement\nthat the proposed iSSO effectively identifies the reduced design space for\ncomplex design problems with multiple global and local minima. This is                          Mohd Aman Khalid: Investigation, Methodology, Formal analysis,\nattributable to the Voronoi tessellation, which eliminates the require\u00ad                      Software, Visualization, Writing \u2013 original draft. Sahil Bansal:\nment of the presumed admissible design space form to resemble the                            Conceptualization, Methodology, Supervision.\ncontour of the original design. Voronoi tessellation enabled better\ndesign space exploration, allowing multiple global minima scattered                          Declaration of Competing Interest\nthroughout the design pace to be effectively identified. Due to the dis\u00ad\ncretization of the design space via Voronoi tessellation, computation                            The authors declare that they have no known competing financial\ndemand is significantly reduced as the number of function evaluations                        interests or personal relationships that could have appeared to influence\nfor all examples is lower vis-a-vis the original SSO. Moreover, the novel                    the work reported in this paper.\nidea of the double sort approach achieves the requisite precision in\nidentifying the subregions for optimal solutions and makes iSSO                              Data availability\nimplementation simple and effective.\n    The applicability of the approach is dependent on the creation of the                       No data was used for the research described in the article.\n\n\n\nAppendix-A: Voronoi Tessellation\n\n   Voronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or\nDirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with\neach polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each\npolygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more\npoints in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space.\n\n\n\n\n                                                             Fig. 12. Voronoi diagram in 2-dimensional space.\n\n\n    There are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the\nVoronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm [51], which generates a\nDelaunay triangulation in any number of dimensions, can be applied while creating a Voronoi diagram. The Delaunay triangulation is a triangulation\nof the point in which no point falls within the circumcircle of any triangle. The polygon generated by the intersection of the half-planes defined by the\nedges of the Delaunay triangles enclosing the point is therefore obtained as the Voronoi cell of a point.\n    It can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to\na set of points. Voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and\nmicrostructure study [52]. The MATLAB command \"Voronoin\" from the \"Parallel Computing Toolbox\" [54] has been used in this study to create the\nVoronoi cells.\n\n\n\nReferences                                                                                   [4] Meng Z, Li G, Wang X, Sait SM, R\u0131za A. A comparative study of metaheuristic\n                                                                                                 algorithms for reliability \u2011 based design optimization problems. Arch Comput\n                                                                                                 Methods Eng 2021;28:1853\u201369. https://doi.org/10.1007/s11831-020-09443-z.\n [1] Marti K. Stochastic optimization methods. Berlin: Springer; 2008.\n                                                                                             [5] Abualigah L, Elaziz MA, Khasawneh AM, Alshinwan M. Meta-heuristic\n [2] Tsompanakis Y, Lagaros ND, Papadrakakis M. Structural design optimization\n                                                                                                 optimization algorithms for solving real-world mechanical engineering design\n     considering uncertainties. CRC Press; 2008.\n                                                                                                 problems : a comprehensive survey, applications, comparative analysis, and\n [3] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design\n                                                                                                 results. Neural Comput Appl 2022;34:4081\u2013110. https://doi.org/10.1007/s00521-\n     optimization of structures using stochastic simulation method. Res Eng Des 2023;\n                                                                                                 021-06747-4.\n     34:179\u2013200. https://doi.org/10.1007/s00163-022-00405-z.\n                                                                                             [6] Katebi J, Shoaei M, Nguyen S, Trung T, Khorami M. Developed comparative\n                                                                                                 analysis of metaheuristic optimization algorithms for optimal active control of\n\n\n                                                                                        12\n\fM.A. Khalid and S. Bansal                                                                                                              Advances in Engineering Software 188 (2024) 103568\n\n     structures. Eng Comput 2020;36:1539\u201358. https://doi.org/10.1007/s00366-019-                   [31] Yildiz AR. Comparison of evolutionary-based optimization algorithms for\n     00780-7.                                                                                           structural design optimization. Eng Appl Artif Intell 2013;26:327\u201333. https://doi.\n [7] Alorf A. Engineering applications of artificial intelligence a survey of recently                  org/10.1016/j.engappai.2012.05.014.\n     developed metaheuristics and their comparative analysis. Eng Appl Artif Intell                [32] Beck AT, Gomes WJDS. A comparison of deterministic, reliability-based and risk-\n     2023;117:105622. https://doi.org/10.1016/j.engappai.2022.105622.                                   based structural optimization under uncertainty. Probab Eng Mech 2012;28:18\u201329.\n [8] Kirsch U. Structural optimization: fundamentals and applications. Springer-Verlag;                 https://doi.org/10.1016/j.probengmech.2011.08.007.\n     2012.                                                                                         [33] Acar, E., Bayrak, G., Jung, Y., Lee, I., Ramu, P., Ravichandran, S.S.: Modeling,\n [9] Floudas CA, Pardalos PA. Encyclopedia of optimization. Springer; 2008.                             analysis, and optimization under uncertainties: a review, (2021). 10.1007/s001\n[10] Kiureghian AD, Ditlevsen O. Aleatory or epistemic? Does it matter? Struct Saf                      58-021-03026-7.\n     2009;31:105\u201312. https://doi.org/10.1016/j.strusafe.2008.06.020.                               [34] Georghiou A, Kuhn D, Wiesemann W. The decision rule approach to optimization\n[11] Schue\u0308ller GI, Jensen HA. Computational methods in optimization considering                        under uncertainty: methodology and applications. Comput Manag Sci 2019.\n     uncertainties - an overview. Comput Methods Appl Mech Eng 2008;198:2\u201313.                           https://doi.org/10.1007/s10287-018-0338-5.\n     https://doi.org/10.1016/j.cma.2008.05.004.                                                    [35] Braydi O, Lafon P, Younes R. Study of uncertainties and objective function\n[12] Schneider J, Kirkpatrick S. Stochastic optimization. Springer; 2007.                               modeling effects on probabilistic optimization results. ASCE ASME J Risk\n[13] Do B, Ohsaki M. A random search for discrete robust design optimization of linear-                 Uncertain Eng Syst Part B Mech Eng 2019. https://doi.org/10.1115/1.4044152.\n     elastic steel frames under interval parametric uncertainty. Comput Struct 2021;               [36] Liu WS, Cheung SH. Reliability based design optimization with approximate failure\n     249:106506. https://doi.org/10.1016/j.compstruc.2021.106506.                                       probability function in partitioned design space. Reliab Eng Syst Saf 2017;167:\n[14] Asadpoure A, Tootkaboni M, Guest JK. Robust topology optimization of structures                    602\u201311. https://doi.org/10.1016/j.ress.2017.07.007.\n     with uncertainties in stiffness - application to truss structures. Comput Struct 2011.        [37] Chiralaksanakul A, Mahadevan S. First-order approximation methods in reliability-\n     https://doi.org/10.1016/j.compstruc.2010.11.004.                                                   based design optimization. J Mech Des Trans ASME 2005. https://doi.org/\n[15] Doltsinis I, Kang Z. Robust design of structures using optimization methods.                       10.1115/1.1899691.\n     Comput Methods Appl Mech Eng 2004;193:2221\u201337. https://doi.org/10.1016/j.                     [38] Doltsinis I, Kang Z, Cheng G. Robust design of non-linear structures using\n     cma.2003.12.055.                                                                                   optimization methods. Comput Methods Appl Mech Eng 2005;194:1779\u201395.\n[16] Carneiro G, das N, Anto\u0301nio CC. Dimensional reduction applied to the reliability-                  https://doi.org/10.1016/j.cma.2004.02.027.\n     based robust design optimization of composite structures. Compos Struct 2021;                 [39] Taflanidis AA, Beck JL. Stochastic Subset Optimization for optimal reliability\n     255. https://doi.org/10.1016/j.compstruct.2020.112937.                                             problems. Probab Eng Mech 2008. https://doi.org/10.1016/j.\n[17] An H, Youn BD, Kim HS. Reliability-based design optimization of laminated                          probengmech.2007.12.011.\n     composite structures under delamination and material property uncertainties. Int J            [40] Au SK, Beck JL. Estimation of small failure probabilities in high dimensions by\n     Mech Sci 2021. https://doi.org/10.1016/j.ijmecsci.2021.106561.                                     subset simulation. Probab Eng Mech 2001;16:263\u201377. https://doi.org/10.1016/\n[18] Li Z, Duan LB, Cheng AG, Yao ZP, Chen T, Yao W. Lightweight and crashworthiness                    S0266-8920(01)00019-4.\n     design of an electric vehicle using a six-sigma robust design optimization method.            [41] Jia GF, Taflanidis AA. Non-parametric stochastic subset optimization for optimal-\n     Eng Optim 2019. https://doi.org/10.1080/0305215X.2018.1521396.                                     reliability design problems. Comput Struct 2013;126:86\u201399. https://doi.org/\n[19] Gholinezhad H, Torabi SH. Reliability-based multidisciplinary design optimization                  10.1016/j.compstruc.2012.12.009.\n     of an underwater vehicle including cost analysis. J Mar Sci Technol 2021. https://            [42] Taflanidis AA. Stochastic subset optimization incorporating moving least squares\n     doi.org/10.1007/s00773-021-00804-2.                                                                response surface methodologies for stochastic sampling. Adv Eng Softw 2012;44:\n[20] Lee KH, Park GJ. Robust optimization considering tolerances of design variables.                   3\u201314. https://doi.org/10.1016/j.advengsoft.2011.07.009.\n     Comput Struct 2001;79:77\u201386. https://doi.org/10.1016/S0045-7949(00)00117-6.                   [43] Khalid MA, Bansal S. Framework for robust design optimization of tuned mass\n[21] Anderson TV, Mattson CA. Propagating skewness and kurtosis through engineering                     dampers by stochastic subset optimization. Int J Struct Stab Dyn 2023;23. https://\n     models for low-cost, meaningful, nondeterministic design. J Mech Des Trans                         doi.org/10.1142/S0219455423501559.\n     ASME. 2012. https://doi.org/10.1115/1.4007389.                                                [44] Au SK. Reliability-based design sensitivity by efficient simulation. Comput Struct\n[22] Zhou Q, Wang Y, Choi SK, Jiang P, Shao X, Hu J, Shu L. A robust optimization                       2005;83:1048\u201361.\n     approach based on multi-fidelity metamodel. Struct Multidiscip Optim 2018.                    [45] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system\n     https://doi.org/10.1007/s00158-017-1783-4.                                                         design using stochastic simulation. Comput Methods Appl Mech Eng 2008. https://\n[23] Wang GG, Shan S. Review of metamodeling techniques in support of engineering                       doi.org/10.1016/j.cma.2008.03.029.\n     design optimization. J Mech Des Trans ASME. 2007;129:370\u201380. https://doi.org/                 [46] Robert CP, Casella G. Monte carlo statistical methods. New York, NY: Springer;\n     10.1115/1.2429697.                                                                                 2004.\n[24] Chatterjee T, Chakraborty S, Chowdhury R. A critical review of surrogate assisted             [47] Li HS. Subset simulation for unconstrained global optimization. Appl Math Model\n     robust design optimization. Arch Comput Methods Eng 2019;26:245\u201374. https://                       2011;35:5108\u201320. https://doi.org/10.1016/j.apm.2011.04.023.\n     doi.org/10.1007/s11831-017-9240-5.                                                            [48] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design\n[25] Chatterjee T, Friswell MI, Adhikari S, Chowdhury R. A global two-layer meta-                       optimization of structures using stochastic simulation method. Res Eng Des 2022.\n     model for response statistics in robust design optimization. Eng Optim 2021.                       https://doi.org/10.1007/s00163-022-00405-z.\n     https://doi.org/10.1080/0305215X.2020.1861262.                                                [49] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system\n[26] Guo X, Zhao X, Zhang W, Yan J, Sun G. Multi-scale robust design and optimization                   design using stochastic simulation. Comput Methods Appl Mech Eng 2008;198:\n     considering load uncertainties. Comput Methods Appl Mech Eng 2015;283:                             88\u2013101. https://doi.org/10.1016/j.cma.2008.03.029.\n     994\u20131009. https://doi.org/10.1016/j.cma.2014.10.014.                                          [50] Kandemir EC, Mortazavi A. Optimization of seismic base isolation system using a\n[27] Jerez DJ, Jensen HA, Beer M. Reliability-based design optimization of structural                   fuzzy reinforced swarm intelligence. Adv Eng Softw 2022;174:103323. https://doi.\n     systems under stochastic excitation: an overview. Mech Syst Signal Process 2022.                   org/10.1016/j.advengsoft.2022.103323.\n     https://doi.org/10.1016/j.ymssp.2021.108397.                                                  [51] Rebay S. Efficient unstructured mesh generation by means of delaunay\n[28] Li W, Gao L, Xiao M. Multidisciplinary robust design optimization under parameter                  triangulation and Bowyer-Watson algorithm. J Comput Phys 1993;106:125\u201338.\n     and model uncertainties. Eng Optim 2020;52:426\u201345. https://doi.org/10.1080/                   [52] Wade N, Graham-Brady L. Estimating microstructural feature distributions from\n     0305215X.2019.1590564.                                                                             image data using a Bayesian framework. J Microsc 2023:1\u201316. https://doi.org/\n[29] Beyer HG, Sendhoff B. Robust optimization - a comprehensive survey. Comput                         10.1111/jmi.13184.\n     Methods Appl Mech Eng 2007;196:3190\u2013218. https://doi.org/10.1016/j.                           [53] Duan X, Li L, Ge Y, Liu B. Exact Voronoi diagram for topographic spatial analysis.\n     cma.2007.03.003.                                                                                   GIScience Remote Sens 2023;60. https://doi.org/10.1080/\n[30] Motta R, de S, Afonso SMB. An efficient procedure for structural reliability-based                 15481603.2023.2171703.\n     robust design optimization. Struct Multidiscip Optim 2016;54:511\u201330. https://doi.             [54] MATLAB and parallel computing toolbox release. Natick, Massachusetts, United\n     org/10.1007/s00158-016-1418-1.                                                                     States: The Mathworks, Inc.; 2021.\n\n\n\n\n                                                                                              13\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nReferences\n[7]Alorf A. Engineering applications of artificial intelligence a survey of recently \ndeveloped metaheuristics and their comparative analysis. Eng Appl Artif Intell \n2023;117:105622. https://doi.org/10.1016/j.engappai.2022.105622 . \n[8]Kirsch U. Structural optimization: fundamentals and applications. Springer-Verlag; \n2012 . \n[9]Floudas CA, Pardalos PA. Encyclopedia of optimization. Springer; 2008 . \n\nAlorf A. Engineering applications of artificial intelligence a survey of recently \ndeveloped metaheuristics and their comparative analysis. \nKirsch U. Structural optimization: fundamentals and applications. \nFloudas CA, Pardalos PA. Encyclopedia of optimization."], "publish_date": "2024-12-12", "approved": true}}, {"_index": "articles", "_id": "MetwCI0BUzn7PtN-84sU", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model", "abstract": "The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97\u201398%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model\u2019s results are examined in terms of recall, accuracy, and precision", "authors": ["Shivangi Sachan", "Khushbu Doulani", "Mainak Adhikari"], "institutions": ["Department of CSE", "IIIT Lucknow", "Lucknow, UP, India", "Vardhaman College of Engineering", "Hyderabad, India"], "keywords": ["Dataset", "KNN", "Gaussian Naive Bayes", "LSTM", "SVM", "Bidirectional LSTM", "GRU", "Word-Embeddings", "CNN"], "full_text": "           Semantic Analysis and Classification of Emails through\n          Informative Selection of Features and Ensemble AI Model\n                  Shivangi Sachan\u2217                                               Khushbu Doulani                                      Mainak Adhikari\n                  Department of CSE                                   Vardhaman College of Engineering                               Department of CSE\n                     IIIT Lucknow                                             Hyderabad, India                                          IIIT Lucknow\n                  Lucknow, UP, India                                      khushidoulani@gmail.com                                          UP, India\n                  mcs21025@iiitl.ac.in                                                                                              mainak.ism@gmail.com\n\nABSTRACT                                                                                              1   INTRODUCTION\nThe emergence of novel types of communication, such as email, has                                     Over the past few years, a clear surge of both the amount of spam-\nbeen brought on by the development of the internet, which radically                                   mers as well as spam emails. This is likely due to a fact that the\nconcentrated the way in that individuals communicate socially and                                     investment necessary for engaging in the spamming industry is\nwith one another. It is now establishing itself as a crucial aspect of                                relatively low. As a result of this, we currently have a system that\nthe communication network which has been adopted by a variety                                         identifies every email as suspicious, which has caused major expen-\nof commercial enterprises such as retail outlets. So in this research                                 ditures in the investment of defense systems [12]. Emails are used\npaper, we have built a unique spam-detection methodology based                                        for online crimes like fraud, hacking, phishing, E-mail bombing, bul-\non email-body sentiment analysis. The proposed hybrid model is                                        lying, and spamming. [16]. Algorithms that are based on machine\nput into practice and preprocessing the data, extracting the proper-                                  learning (ML) are now the most effective and often used approach to\nties, and categorizing data are all steps in the process. To examine                                  the recognition of spam. Phishing, which is defined as a fraudulent\nthe emotive and sequential aspects of texts, we use word embed-                                       attempt to acquire private information by masquerading as a trust-\nding and a bi-directional LSTM network. this model frequently                                         worthy party in electronic communication, has rapidly advanced\nshortens the training period, then utilizes the Convolution Layer to                                  past use of simple techniques and the tactic of casting a wide net;\nextract text features at a higher level for the BiLSTM network. Our                                   instead, spear phishing uses a variety of sophisticated techniques\nmodel performs better than previous versions, with an accuracy                                        to target a single high-value individual. Other researchers used NB,\nrate of 97\u201398%. In addition, we show that our model beats not just                                    Decision Trees, and SVM to compare the performance of supervised\nsome well-known machine learning classifiers but also cutting-edge                                    ML algorithms for spam identification [6]. Spam emails clog up re-\nmethods for identifying spam communications, demonstrating its                                        cipients\u2019 inboxes with unsolicited communications, which frustrate\nsuperiority on its own. Suggested Ensemble model\u2019s results are                                        them and push them into the attacker\u2019s planned traps [7]. As a re-\nexamined in terms of recall, accuracy, and precision                                                  sult, spam messages unquestionably pose a risk to both email users\n                                                                                                      and the Internet community. In addition, Users may occasionally\nCCS CONCEPTS                                                                                          read the entire text of an unsolicited message that is delivered to\n\u2022 Computer systems organization \u2192 Embedded systems; Re-                                               the target users\u2019 inboxes without realizing that the message is junk\ndundancy; Robotics; \u2022 Networks \u2192 Network reliability.                                                 and then choosing to avoid it. Building a framework for email spam\n                                                                                                      detection is the aim of this project. In this approach, we combine the\nKEYWORDS                                                                                              Word-Embedding Network with the CNN layer, Bi-LSTM, and GRU\n                                                                                                      (BiLSTM+GRU). CNN layers are used to speed up training time\nDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional\n                                                                                                      before the Bi-LSTM network, and more advanced textual character-\nLSTM, GRU, Word-Embeddings, CNN\n                                                                                                      istics are extracted with the use of this network in comparison to\nACM Reference Format:                                                                                 the straight LSTM network, in less time. Gated recurrent neural net-\nShivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. Semantic                                 works (GRUs) are then added because they train more quickly and\nAnalysis and Classification of Emails through Informative Selection of\n                                                                                                      perform better for language modeling. To evaluate and investigate\nFeatures and Ensemble AI Model. In 2023 Fifteenth International Conference\n                                                                                                      various machine learning algorithms for predicting email spam,\non Contemporary Computing (IC3-2023) (IC3 2023), August 03\u201305, 2023, Noida,\nIndia. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.                              and develop a hybrid classification algorithm to filter email spam\n3607979                                                                                               before employing an ensemble classification algorithm to forecast\n                                                                                                      it. To put an innovative technique into practice and compare it to\n\u2217 Both authors contributed equally to this research.\n                                                                                                      the current method in terms of various metrics. Ensemble learn-\nPermission to make digital or hard copies of all or part of this work for personal or\n                                                                                                      ing, a successful machine learning paradigm, combines a group of\nclassroom use is granted without fee provided that copies are not made or distributed                 learners rather than a single learner to forecast unknown target\nfor profit or commercial advantage and that copies bear this notice and the full citation             attributes. Bagging, boosting, voting, and stacking are the four main\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or                types of ensemble learning techniques. To increase performance,\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission         an integrated method and the combining of two or three algorithms\nand/or a fee. Request permissions from permissions@acm.org.                                           are also suggested. Extraction of text-based features takes a long\nIC3 2023, August 03\u201305, 2023, Noida, India\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.                     time. Furthermore, it can be challenging to extract all of the crucial\nACM ISBN 979-8-4007-0022-4/23/08. . . $15.00                                                          information from a short text. Over the span associated with this\nhttps://doi.org/10.1145/3607947.3607979\n\n\n\n\n                                                                                                181\n\fIC3 2023, August 03\u201305, 2023, Noida, India                                                                                           Sachan et al.\n\n\nresearch, we utilize Bidirectional Large Short-Term Memories (Bi-          RF (random forest) ensemble techniques were performed to forecast\nLSTM) in conjunction with Convolutional Neural Networks (CNN)              class labels, and the outputs were input into a classifying unit to\nto come up with an innovative method to the detection of spam.             increase accuracy. A method for content-based phishing detection\nBagging and boosting approaches were widely preferred in this              was presented by the authors in [2], to classify phishing emails,\nstudy. Contribution and paper organization is as follows: section 1.1      they employed RF. They categorize spam and phishing emails. They\ndescribes literature study, section 1.2 describe motivation for this       enhanced phishing email classifiers with more accurate predictions\nresearch work, section 2 sketches procedure of details implemen-           by extracting features. They showed some effective Machine learn-\ntation, Section 3 present experimental setup, dataset description          ing spam filtering techniques. When the PCA method is used, it will\nand evaluation metrics, and section 4 summarizing outcomes of the          lower the number of features in the dataset. The collected features\nexperiment.                                                                go through the PCA algorithm to reduce the number of features.\n                                                                           The PCA method is used to make a straightforward representation\n                                                                           of the information which illustrates the amount of variability there\n1.1     Related Work                                                       is in the data. The authors of [20] presented the Fuzzy C-means\nEmail is indeed the second most frequently utilized Internet appli-        method for classifying spam email. To stop spam, they implemented\ncation as well as the third most common method of cyberbullying,           a membership threshold value. A methodology to identify unla-\nclaims one study. Cybercriminals exploit it in a number of ways,           beled data was put forth by the authors of [1] and applied motive\nincluding as sending obscene or abusive messages, adding viruses           analysis to the Enron data collection. They divided the data into\nto emails, snatching the private information of victims, and ex-           categories that were favorable, negative, and neutral. They grouped\nposing it to a broad audience. Spam letters made up 53.95% of all          the data using k-means clustering, an unsupervised ML technique\nemail traffic in March 2020. We examine three main types of un-            and then classified it using the supervised ML techniques SVM and\nlawful emails in our study. First are fake emails, which are sent          NB. Hina, Maryam, and colleagues (2021) implemented Sefaced:\nto manipulate recipients to submit sensitive information. The sec-         Deep learning-based semantic analysis and categorization of e-mail\nond as being cyberbullying\u2019s use of harassing emails to threaten           data using a forensic technique. For multiclass email classification,\nindividuals. Suspicious emails that describe illegal activities belong     SeFACED employs a Gated Recurrent Neural Network (GRU) based\nto the third category. Many researchers have earlier contributed           on Long Short-Term Memory (LSTM). Different random weight ini-\nmassively to this subject. The researcher claims there is some proof       tializations affect LSTMs [9]. Zhang, Yan, et al.(2019) Experiments\nthat suspicious emails were sent before to the events of 9/11. [14].       on three-way game-theoretic rough set (GTRS) email spam filter-\nWhen it comes to data labeling, there are also convinced rule-based        ing show that it is feasible to significantly boost coverage without\napproaches and technologies ( like VADER) that are used, even              decreasing accuracy [23]. According to Xia et al. [22], SMS spam\nthough their efficiency of the are together is adversely affected. A       has been identified using machine learning model such as naive\nhidden layer, which itself is essential for vectorization, is the top      bayes , vector-space modeling, support vector machines (SVM),\nlayer of the model. We use oversampling methods for this minority          long selective memory machines (LSTM), and convolutional neural\nclass because of the absence of data. Sampling techniques can help         networks including every instance of a method for categorizing\nwith multicollinearity, but they have an impact on simulation re-          data. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic\nsults. Oversampling causes data to be randomly repeated, which             gradient descent (sgd) algorithms for e-mail filtering with R and\naffects test data because dividing data may result in duplicates. Un-      orange software spam [3]. Orange software was used to create the\ndersampling may result in the loss of some strong information. In          classifications, which included Adaboost and SGD. The majority of\norder to advance email research, it is crucial to provide datasets on      researchers focused on text-based email spam classification meth-\ncriminal activity. P. Garg et al. (2021) [5], which revealed that spam     ods because image-based spam can be filtered in the early stages\nin an email was detected in 70 percent of business emails, spam was        of pre-processing. There are widely used word bag (BoW) model,\nestablished as an obstacle for email administrators. Recognizing           which believes that documents are merely unordered collections\nspam and getting rid of it were the primary concerns, as spam can          of words, is the foundation for these techniques. Kumaresan [11]\nbe offensive, may lead to other internet sites being tricked, which        explains SVM with a cuckoo search algorithm was used to extract\ncan offer harmful data, and can feature those who are not particu-         textual features for spam detection. Renuka and Visalakshi made\nlar with their content using NLP. To select the best-trained model,        use of svm [17] spam email identification, followed by selecting\neach mail transmission protocol requires precise and effective email       features using Latent Semantic Indexing (LSI). Here we have used\nclassification, a machine learning comparison is done. Our study           labeled dataset to train the hybrid classifier. We used TF-IDF for\nhas suggested that innovative deep learning outperforms learning           feature extraction [20] and Textual features for spam detection\nalgorithms like SVM and RF. Current studies on the classification          were extracted using SVM and a cuckoo search algorithm. [4] for\nof emails use a variety of machine learning (ML) techniques, with          filtering out the spam email. Combining the integrated strategy to\na few of them focusing on the study of the sentiments consisted of         the pure SVM and NB methods, overall accuracy is really improved.\nwithin email databases. The lack of datasets is a significant obstacle     Moreover, accurate detection for spam email has been proposed\nto email classification. There are few publicly accessible E-mail          using the Negative Selection Algorithm (NSA) and Particle Swarm\ndatasets, thus researchers must use these datasets to test their hy-       Optimization\u2019s (PSO) algorithm. PSO is used in this instance to\npotheses or gather data on their own. Authors[15] describe supplied        improve the effectiveness of the classifier.\ntwo-phased outlier detection models to enhance the IIOT network\u2019s\ndependability. Artificial Neural Network, SVM, Gaussian NB, and\n\n\n\n\n                                                                     182\n\fSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model                                               IC3 2023, August 03\u201305, 2023, Noida, India\n\n\n1.2        Motivation and Novelty                                                                                      language processing (NLP). The major preprocessing steps are de-\nEmail is most common form of communication between people                                                              picted below.\nin this digital age. Many users have been victims of spam emails,\nand their personal information has been compromised. The email                                                         2.1    NLP Tokenization\nClassification technique is employed to identify and filter junk                                                       Tokenization of documents into words follows predefined rules.\nmail, junk, and virus-infected emails prior to reach a user\u2019s inbox.                                                   The tokenization step is carried out in Python with spacy library.\nExisting email classification methods result in irrelevant emails\nand/or the loss of valuable information. Keeping these constraints                                                     2.2    Stop Words Removal\nin mind, the following contributions are made in this paper:\n                                                                                                                       Stop words appear infrequently or frequently in the document, but\n       \u2022 Text-based feature extraction is a lengthy process. Further-                                                  they are less significant in terms of importance. As a result, these\n         more, extracting every important feature from text is difficult.                                              are removed to improve data processing.\n         In this paper, we show how to employ GRU with Convo-\n         lutional Neural Networks and Bidirectional-LSTM to find                                                       2.3    Text Normalization\n         spam.                                                                                                         A word\u2019s lexicon form or order may differ. Thus, they must all be\n       \u2022 Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-                                                        changed to their root word to be correctly analyzed. Lemmatization\n         ral Networks to examine the relationships, sentimental con-                                                   and stemming are the two methods that can be used for normal-\n         tent, and sequential way of email contents.                                                                   ization. When a word\u2019s final few characters are removed to create\n       \u2022 Applied CNN before the Bi-LSTM network, training time can                                                     a shorter form, even if that form has no meaning, the procedure\n         be sped up. This network can also extract more advanced                                                       is known as stemming. lemmatization [21] is a mixture of corpus-\n         textual features faster than the Bi-LSTM network alone when                                                   based an rule-based methods, and it retains the context of a term\n         combined with the GRU network.                                                                                while changing it back to its root.\n       \u2022 We use Enorn Corpora datasets and compute precision, re-\n         call, and f-score to assess how well the suggested technique                                                  2.4    Feature Extraction\n         performs. Our model outperforms several well-known ma-\n         chine learning techniques as well as more contemporary                                                        feature extraction which transforms the initial text into its features\n         methods for spam message detection.                                                                           so that it may be used for modeling after being cleaned up and\n                                                                                                                       normalized. Before predicting them, we use a specific way to give\n                                                                                                                       weights to specific terms in our document. While it is simple for a\n2      PROPOSED SYSTEM ARCHITECTURE AND                                                                                computer to process numbers, we choose to represent individual\n       MODEL                                                                                                           words numerically. In such cases, we choose word embeddings. IDF\nE-mail is a valuable tool for communicating with other users. Email                                                    is the count of documents containing the term divided by the total\nallows the sender to efficiently forward millions of advertisements                                                    number of documents, and occurrence is the amount of instances a\nat no cost. Unfortunately, this scheme is now being used in a variety                                                  word appears in a document. We derive characteristics based on\nof organizations. As a result, a massive amount of redundant emails                                                    equations. 1,2,3,4,5, and 6. We use equations to derive properties.\nis known as spam or junk mail, many people are confused about the                                                                                               \u0012 \u0013\n                                                                                                                                                                  1\nemails in their E- Mailboxes. Each learning sequence is given for-                                                                             \ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51 \ud835\udc53 = \ud835\udc61 \ud835\udc53 \u2217                           (1)\n                                                                                                                                                                 \ud835\udc51\ud835\udc53\nward as well as backward to two different LSTM networks that are\nattached to the same outputs layer in order for bidirectional Lstms\nto function. This indicates that the Bi-LSTM has detailed sequential                                                                        \ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51 \ud835\udc53 = \ud835\udc61 \ud835\udc53 \u2217 Inverse(\ud835\udc51 \ud835\udc53 )                         (2)\ninformation about all points before and following each point in a\nspecific sequence. In other words, we concatenate the outputs from                                                                     \ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51 \ud835\udc53 (\ud835\udc61, \ud835\udc51, \ud835\udc37) = \ud835\udc47 \ud835\udc53 (\ud835\udc61, \ud835\udc51).\ud835\udc3c\ud835\udc51 \ud835\udc53 (\ud835\udc61, \ud835\udc37)                 (3)\nboth the forward and the backward LSTM at each time step rather\n                                                                                                                                                                      \ud835\udc41\nthan just encoding the sequence in the forward direction. Each                                                                              \ud835\udc47 \ud835\udc3c\ud835\udc51 \ud835\udc53 (\ud835\udc61, \ud835\udc51) = log                                    (4)\n                                                                                                                                                                   |\ud835\udc51\ud835\udf16\ud835\udc37\ud835\udc61\ud835\udf16\ud835\udc37 |\nword\u2019s encoded form now comprehends the words that come before\nand after it. This is a problem for the Internet community. The di-                                                    A word2vec neural network-based approach is the method that is\nagram depicts various stages that aid in the prediction of email spam:                                                 utilized for this goal as the tool. The following equation, referred\n                                                                                                                       to as 5, shows how word2vec handles word context through the\n                                                                                                                       use of probability-accurate measurements. Here letter D stands for\n                                                                                                                       the paired-wise display of a set of words, while the letters w and c0\n                                                                                                                       or c1 represent paired word context that originated from a larger\n                                                                                                                       collection of set D.\n                                                                                                                                                                           1\n                                                                                                                               \ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64, \ud835\udc50 11:\ud835\udc58 ) =                                            (5)\n                                                                                                                                                          1 + \ud835\udc52 \u2212 (\ud835\udc64\u00b7\ud835\udc50 1 1+\ud835\udc64\u00b7\ud835\udc50 1 2+...+\ud835\udc64\u00b7\ud835\udc50 1\ud835\udc58 )\nBecause real-world data is messy and contains unnecessary infor-                                                                                                          1\nmation and duplication, data preprocessing is critical in natural                                                                       \ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64, \ud835\udc50 1:\ud835\udc58 ) =                                    (6)\n                                                                                                                                                                   1 + \ud835\udc52 \u2212 (\ud835\udc64\u00b7\ud835\udc500)\n\n\n\n\n                                                                                                                 183\n\fIC3 2023, August 03\u201305, 2023, Noida, India                                                                                                 Sachan et al.\n\n\n2.5     Word-Embeddings                                                         long-term dependencies[10]. LSTM was actually created to address\nWord-Embedding helps to improve on the typical \"bag-of-words\"                   the problem of long-term reliance. LSTM has the unique ability to\nworldview, which requires a massive sparse feature vector to score              recall. The cell state is the LSTM model\u2019s central concept. With\nevery word individually to represent this same entire vocabulary.               only a small amount of linear interaction, the cell state follows the\nThis perception is sparse because the vocabulary is large, and each             sequence essentially unmodified from beginning to end. gate of\nword or document is defined by a massive vector. Using a word                   an LSTM is also significant. Under the command of these gates,\nmap-based dictionary, word embedding needs to be converted terms                information is safely inserted to or eliminated from the cell stated.\n(words) into real value feature vectors. There are two basic issues             The following equations are used by the LSTM model to update\nwith standard feature engineering techniques for deep learning.                 each cell:                  \u0010                       \u0011\nData is represented using sparse vectors, and the second is that                                    \ud835\udc53\ud835\udc61 = \ud835\udf0e \ud835\udc4a \ud835\udc53 \u00b7 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f \ud835\udc53                (9)\nsome of the meanings of words are not taken into consideration.                 In this case, Xt denotes input, and ht is the hidden state at the t\nSimilar phrases will have values in embedding vectors that are                  time step. The following is the revised cell state Ct:\nalmost real-valued. The Input length in our proposed study is set\n                                                                                                     \ud835\udc56 t = \ud835\udf0e (\ud835\udc4a\ud835\udc56 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f\ud835\udc56 )               (10)\nto 700 for our suggested model. If the texts seemed to be integer\nencoded with value systems between 10 and 20, the vocabulary                                      \ud835\udc36\ud835\udc47 = tanh (\ud835\udc4a\ud835\udc50 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f\ud835\udc50\ud835\udc61 )               (11)\ndistance would be 11. Our data is encoded as integers, and the input\n                                                                                                      \ud835\udc36\ud835\udc61 = \ud835\udc53\ud835\udc61 \u2217 \ud835\udc36\ud835\udc61 \u22121 + \ud835\udc56\ud835\udc61 \u2217 \ud835\udc36\ud835\udc47                    (12)\nand output dimensions are both set to 50,000. The embedding layer\noutcome will be used in successive layers and for BiLSTM and GRU                Here, we may compute the output and hidden state at t time steps\nlayers.                                                                         using the point-wise multiplication operator *.\n                                                                                                   \ud835\udc5c\ud835\udc61 = \ud835\udf0e (\ud835\udc4a\ud835\udc5c \u00b7 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f\ud835\udc5c )                (13)\n2.6     Machine Learning Model\n                                                                                                         \u210e\ud835\udc61 = \ud835\udc5c\ud835\udc61 \u2217 tanh (\ud835\udc36\ud835\udc61 )                      (14)\nWithin the scope of the research, we are using the subsequent ma-\n                                                                                Due to the reality it only considers all prior contexts from the\nchine learning techniques, to examine and compare the overall\n                                                                                present one, LSTM does have a few drawbacks. As a result of this,\nefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-\n                                                                                it may accept data from preceding time steps through LSTM as well\nchine, Gaussian NB, Logistic Regression, K - nearest neighbors, and\n                                                                                as RNN. Therefore, in order to avoid this issue, further improve-\nRandom Forest (RF).\n                                                                                ments are carried out with the help of a bidirectional recurrent\n                                                                                neural network(Bi-RNN). BiRNN [13] can handle two pieces of in-\n2.7     Convolution Network\n                                                                                formation from both the front and the back. Bi-LSTM is created\nThe popular RNN model generally performs well but takes too                     by combining the Bi-RNN and LSTM. As a result, operating LSTM\nlong to train the model incorporating the textual sequential data.              has advantages such as cell state storage so that BiRNN have way\nWhen a layer is added after the RNN layer, the model\u2019s learning                 to acknowledge from the context before and after. As a conse-\nduration is considerably decreased. Higher-level feature extraction             quence of this, it provides the Bi-LSTM with the advantages of an\nis another benefit. [19] additionally possible using the convolutional          LSTM with feedback for the next layer. Remembering long-term\nlayer. In essence, the convolution layer looks for combinations of              dependencies is a significant new benefit of Bi-LSTM. The output,\nthe various words or paragraphs in the document that involve the                which is a feature vector, will be based on the call state. Finally,\nfilters. We use features with 128 dimensions and a size 10 for each.            we forecast the probability of email content as Normal, Fraudu-\nFor this task, the Relu activation function is utilized. After that, the        lent, Harassment, and Suspicious Emails using as an input to the\none-dimensional largest pooling layers with a pooling size of 4 are             softmax activation function, which is a weighted sum of the dense\nput on the data in order to obtain higher-level features.                       layer\u2019s outputs. To regulate the information flow, GRU employs\n                                                                                the point-wise multiplying function and logistic sigmoid activation.\n2.8     BiLSTM Network with GRU                                                 The GRU has hidden states of storage memory and does not have\nRecurrent Neural Network (RNN) technique of text sentiment anal-                distinct memory cells or units for state control. The W, U, and b\nysis is particularly well-liked and frequently applied. Recurrent               vectors, which stand for weights, gates, and biases, respectively, are\nneural networks (RNN) surpass conventional neural networks. be-                 crucial variables that must be calculated during the creation of the\ncause it can remember the information from earlier time steps                   GRU model. For training reasons, the pre-trained word embedding\nthanks to its memory. A state vector is combined with an RNN\u2019s                  known as the Glove vector is used. They made it clear that GRU\ndata to create a new state vector. The resulting state vector uses the          is the superior model when there is a large amount of training\npresent to recollect past knowledge. The RNN is straightforward                 data for textual groups and word embedding is available. BiLSTM,\nand is based on the following equations:                                        CNN, and GRU is required so as to compensate for the deletion\n                      \u210e\ud835\udc61 = tanh (\ud835\udc4a\u210e\u210e \u210e\ud835\udc61 \u22121 + \ud835\udc4a\ud835\udf0b\u210e \ud835\udc65\ud835\udc61 )               (7)         of the document\u2019s long-term and short-term connections. In our\n                                                                                case, the embedding dimension, maximum sequence length, and\n                                  \ud835\udc66\ud835\udc61 = \ud835\udc4a\u210e \ud835\udc66 \u210e\ud835\udc61                      (8)         lexicon size were used to start the LSTM embedding layer in three\nThe vanilla RNN[18]is not very good at remembering previous                     separate LSTM models. The input vector was modified to make it\nsequences. In addition to that, RNN struggles with diminishing                  appropriate for such a Conv1D layer, prior situations\u2019 sequences are\ngradient descent. A kind of RNN is a long short-term recall network             returned by LSTM layer. The \"return sequences\" of the LSTM layer\n(LSTM), solves a vanishing gradient descent problem and learns                  must be set to False when the subsequent state is free of the gated\n\n\n\n\n                                                                          184\n\fSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model                                            IC3 2023, August 03\u201305, 2023, Noida, India\n\n\narchitecture. Quantity of learning parameters must be taken into                                                       including accuracy, precision, and recall, are used to examine the\nconsideration. A 350-unit LSTM layer was set - up, and different                                                       outcomes.\nLSTM unit combinations were tested. More importantly, because\nit has more parts, the model made with BiLSTM will take longer                                                         3.3     Evaluation Metrics and Results\nto train. Bidirectional LSTM is the name of a particular kind of                                                       Classifier performance is assessed Using metrics such as accuracy,\nrecurrent neural network that is primarily used for the processing                                                     precision, and recall. Four terms make up a confusion matrix that\nof natural languages. (BiLSTM). It is able to use data from both                                                       is used to calculate these metrics.\nsides, and, in contrast to regular LSTM, it enables input flow in\n                                                                                                                             \u2022 True positives (TP) are positive values that have been accu-\nboth directions. It is an effective instrument for demonstrating the\n                                                                                                                               rately assigned the positive label.\nlogical relationships between words and phrases, and this involves\n                                                                                                                             \u2022 The negative values that are accurately identified as negative\nboth the forward and backward directions of the sequence. In con-\n                                                                                                                               are known as True Negatives (TN).\nclusion, BiLSTM works by adding one extra layer of LSTM, causing\n                                                                                                                             \u2022 True Negative values are those that can be accurately identi-\nthe information flow to travel in the other direction. It only denotes\n                                                                                                                               fied as being negative (TN).\nthat the input sequence runs in reverse at the next LSTM layer. Mul-\n                                                                                                                             \u2022 Positive readings that have been mistakenly labeled as nega-\ntiple operations, including averaging, summation, multiplication,\n                                                                                                                               tive are known as False Negatives (FN).\nand concatenation, are then applied to the results of the two LSTM\nlayers. The gated design of Bi-LSTM and GRU networks solves                                                            Assess the efficacy of the suggested model is listed below:\nthe disappearing gradient and exploding problems. A good way to\n                                                                                                                       3.3.1 Accuracy. Accuracy reveals how frequently the ML model\nhandle more long sequences is to use Bi-LSMT and GRU together.\n                                                                                                                       was overall correct.\nGRU works well with datasets that don\u2019t have text. In two to three\nrounds, the complicated CNN+BiLSTM+GRU model learns the long                                                                                            \ud835\udc47\ud835\udc43 +\ud835\udc47\ud835\udc41\n                                                                                                                                       Accuracy =                               (15)\nsequence of email text well. We have used word embedding, cnn,                                                                                    \ud835\udc47\ud835\udc43 + \ud835\udc47 \ud835\udc41 + \ud835\udc39\ud835\udc43 + \ud835\udc39\ud835\udc41\nbidirectional lstm and gru networks as our three building blocks                                                       3.3.2 Precision. The accuracy of the model gauges how effectively\nto separate email messages based on their sentiment and text\u2019s                                                         it can predict a specific category.\nsequential features. Also, we succinctly demonstrate below why\n                                                                                                                                                               \ud835\udc47\ud835\udc43\nthese blocks help identify email spam:                                                                                                         Precision =                                     (16)\n                                                                                                                                                             \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc43\n     \u2022 First, We have used the Sequence - to - sequence Lstm as the\n       current block in the networks since it can retrieve both the                                                    3.3.3 Recall. Recall tells us how often the model was able to rec-\n       previous and next sequences from the current. More so than                                                      ognize a specific category.\n       a straightforward LSTM network, it can also recognize and                                                                                              \ud835\udc47\ud835\udc43\n                                                                                                                                                 Recall =                                      (17)\n       extract text sentiment and sequential properties.                                                                                                    \ud835\udc47\ud835\udc43 + \ud835\udc39\ud835\udc41\n     \u2022 Second, we extract the more complex and advanced charac-\n       teristics for Bi-LSTM network using Convolutional Network\n       block, which is the network\u2019s second block after the Bi-LSTM                                                     Model                  Accuracy      Precision    Recall\n       block. Bi-LSTM takes a long time to extract text-based fea-                                                      Gaussian NB            91.3          90.1         91.8\n       tures, hence one of the reasons for using this block is to                                                       Random Forest          88.41         90           88\n       reduce the network\u2019s overall training time.                                                                      KNN                    86.6          89           87\n                                                                                                                        SVM                    92.4          91           92\n3 EXPERIMENTAL EVALUATION                                                                                               LSTM                   95.2          95           95.7\n                                                                                                                        Proposed Ensemble 97.32              95.6         95.3\n3.1 Experimental Setup\n                                                                                                                        (CNN,BiLSTM+GRU)\nWe divided the information into training and testing groups of                                                                 Table 1: Differet Model\u2019s Score on Test Data\n80/20. We divided the remaining 20% of the 80 percent training\ndata into test data for the model. Construct, compute, and evaluate\nthe efficacy of the suggested method using the Pythonic packages\nKeras, as TensorFlow and Scikit learn.                                                                                    Accuracy, Precision, and Recall metrics are computed. In the\n                                                                                                                       given Table 1 where six different classifiers are Gaussian NB, Ran-\n3.2        Dataset Description                                                                                         dom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid\nEmail spam detection is the foundation of this research project. The                                                   Model (CNN+BiLSTM+GRU) have been used in this work. In the\ndataset includes normal emails from the Enron corpora, deceptive                                                       CNN, Bi-LSTM, and GRU architectures which enable sequence pre-\nemails from phished email corpora, harassment emails chosen from                                                       diction, CNN strands for feature extraction on data input which are\nhate speech, and the offensive dataset. Only the content of the email                                                  combined with LSTM. It requires less time training and a higher\nbody is used for analysis; all header information, including sender,                                                   expandable model. Any bottlenecks are created by predictions and\ntopic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word                                                      the increasing number of distinct units of information. This model\nEmbedding are used to extract characteristics from the email mes-                                                      is useful for dealing with issue-related classifications that consist\nsage and classify them. This dataset[8] is publicly available. The                                                     of two or more than two classes. So suggested Ensemble model, out\npresented model is implemented using Python, and several metrics,                                                      of these six classifiers, produces more accurate findings.\n\n\n\n\n                                                                                                                 185\n\fIC3 2023, August 03\u201305, 2023, Noida, India                                                                            Sachan et al.\n\n\n\n\n                                                                          Figure 3: LSTM Model Training and Validation Loss\n                   Figure 1: Performance Analysis\n\n\n\n\n3.4     Comparative Analysis\nA model\u2019s ability to fit new data is measured by the validation\nloss, whereas its ability to fit training data is determined by the\ntraining loss. The two main variables that decide whether in which\nlearning is efficient or not are validation loss and training loss.\nLSTM and Suggested Ensemble hybrid Models have equivalent loss\nand accuracy. In this context, we are contrasting the LSTM with the\nproposed model (CNN, Bilstm, and GRU) in terms of their respective\nvalidation accuracies and losses. The model\u2019s accuracy was at its\nhighest after 14 epochs of operation when it achieved an accuracy\nof roughly 97-98% while minimizing model loss.\n                                                                        Figure 4: Ensemble Model (CNN,BiLSTM+GRU) Training\n                                                                        and Validation Accuracy\n\n\n\n\n                                                                        Figure 5: Ensemble Model (CNN,BiLSTM+GRU)Training\n Figure 2: LSTM Model Training and Validation Accuracy                  and Validation Loss\n\n\n\n\n                                                                  186\n\fSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model                                                   IC3 2023, August 03\u201305, 2023, Noida, India\n\n\n   In this Proposed ensemble hybrid model\u2019s train accuracy is 98.7%                                                         Egyptian Informatics Journal 15, 3 (2014), 169\u2013174.\nValidation accuracy is 97.32% and LSTM has train accuracy of 97.41%                                                    [15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,\n                                                                                                                            and Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT using\nand validation accuracy is 95.2%. So based on figures 3 and 5 indicate                                                      ensemble classifier. arXiv preprint arXiv:2102.01515 (2021).\nthe validation loss for LSTM and the proposed ensemble hybrid                                                          [16] Justinas Rastenis, Simona Ramanauskaite\u0307, Justinas Janulevi\u010dius, Antanas \u010cenys,\n                                                                                                                            Asta Slotkiene\u0307, and K\u0119stutis Pakrijauskas. 2020. E-mail-based phishing attack\nmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show the                                                       taxonomy. Applied Sciences 10, 7 (2020), 2363.\nvalidation accuracy to be 95.2% and 97.3%, respectively. LSTM and                                                      [17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVM\nthe proposed hybrid model used ensemble artificial intelligence,                                                            model for email spam classification. (2014).\n                                                                                                                       [18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequence\nwith the proposed hybrid model outperforming the LSTM. We                                                                   modeling for intelligent typing assistant with Bangla and English keyboard. In\ndecide on dense architecture as the final model for identifying the                                                         2018 International Conference on Innovation in Engineering and Technology (ICIET).\ntext messages as spam or nonspam based on loss, accuracy, and the                                                           IEEE, 1\u20136.\n                                                                                                                       [19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Ha\u015fim Sak. 2015. Convolu-\naforementioned charts. The loss and accuracy over epochs are more                                                           tional, long short-term memory, fully connected deep neural networks. In 2015\nstable than LSTM, and the Proposed classifier has a straightforward                                                         IEEE international conference on acoustics, speech and signal processing (ICASSP).\n                                                                                                                            Ieee, 4580\u20134584.\nstructure.                                                                                                             [20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spam\n                                                                                                                            messages and mails using fuzzy C means algorithm. In 2019 4th International\n4      CONCLUSION                                                                                                           Conference on Internet of Things: Smart Innovation and Usages (IoT-SIU). IEEE,\n                                                                                                                            1\u20135.\nThe model is composed of four networks Word-Embeddings, CNN,                                                           [21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-\n                                                                                                                            zation and part-of-speech prediction. In Proceedings of the Joint Conference of\nBi-LSTM, and GRU. We may train the model more quickly by using                                                              the 47th Annual Meeting of the ACL and the 4th International Joint Conference on\nthe convolutional layer first, followed by the word-embedding layer,                                                        Natural Language Processing of the AFNLP. 486\u2013494.\nand then the BiLSTM network. The Bidirectional LSTM network                                                            [22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boosting\n                                                                                                                            throughput on rule-based filtering systems. IEEE Access 8 (2020), 82653\u201382661.\nalso has higher-level properties that we can extract. We have used                                                     [23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filtering\na bidirectional LSTM(BiLSTM)and GRU network to memorize a                                                                   with game-theoretic rough sets. In 2019 International conference on computing,\nsentence\u2019s contextual meaning and sequential structure, which im-                                                           networking and communications (ICNC). IEEE, 552\u2013556.\nproves the model\u2019s performance accuracy to roughly 97.32 percent.                                                      Received 15 April 2023\n\nREFERENCES\n [1] Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-\n     beled email data. In 2019 International Conference on Computational Intelligence\n     and Knowledge Economy (ICCIKE). IEEE, 328\u2013333.\n [2] Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-\n     rithm to filter spam using machine learning techniques. Pacific Science Review A:\n     Natural Science and Engineering 18, 2 (2016), 145\u2013149.\n [3] Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic\n     gradient descent (sgd) algorithms with R and orange software for filtering e-mail\n     spam. In 2019 11th Computer Science and Electronic Engineering (CEEC). IEEE,\n     41\u201346.\n [4] Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. A\n     support vector machine based naive Bayes algorithm for spam filtering. In 2016\n     IEEE 35th International Performance Computing and Communications Conference\n     (IPCCC). IEEE, 1\u20138.\n [5] Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam Filtering\n     Techniques based on Natural Language Processing Framework. In 2021 11th Inter-\n     national Conference on Cloud Computing, Data Science & Engineering (Confluence).\n     IEEE, 30\u201335.\n [6] Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-\n     lyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316\u2013342.\n [7] Radicati Group et al. 2015. Email Statistics Report 2015\u20132019. Radicati Group.\n     Accessed August 13 (2015), 2019.\n [8] Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensic\n     analysis and classification of e-mail data using deep learning. IEEE Access 9\n     (2021), 98398\u201398411.\n [9] Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali\n     Khan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis and\n     classification of e-mail data using deep learning. IEEE Access 9 (2021), 98398\u2013\n     98411.\n[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and Yuan\n     Zhang. 2017. Short-term residential load forecasting based on LSTM recurrent\n     neural network. IEEE transactions on smart grid 10, 1 (2017), 841\u2013851.\n[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoo\n     search and support vector machine. International Journal of Bio-Inspired Compu-\n     tation 9, 3 (2017), 142\u2013156.\n[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification of\n     spam emails using deep learning. In 2021 1st Babylon International Conference on\n     Information Technology and Science (BICITS). IEEE, 63\u201368.\n[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural\n     network language model. In 2012 IEEE Spoken Language Technology Workshop\n     (SLT). IEEE, 234\u2013239.\n[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.\n     2014. Detection of fraudulent emails by employing advanced feature abundance.\n\n\n\n\n                                                                                                                 187\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nRayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-\nbeled email data. In 2019 International Conference on Computational Intelligence\nand Knowledge Economy (ICCIKE) . IEEE, 328\u2013333.", "Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-\nrithm to filter spam using machine learning techniques. Pacific Science Review A:\nNatural Science and Engineering 18, 2 (2016), 145\u2013149.", "Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic\ngradient descent (sgd) algorithms with R and orange software for filtering e-mail\nspam. In 2019 11th Computer Science and Electronic Engineering (CEEC) . IEEE,\n41\u201346."], "publish_date": "2023-08-03", "approved": true}}, {"_index": "articles", "_id": "JOtlCI0BUzn7PtN-c4vj", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "AI Model for Computer games based on Case Based Reasoning and AI Planning", "abstract": "Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory.", "authors": ["Vlado Menkovski"], "institutions": ["Athens Information Technology"], "keywords": ["Game AI", "Case Based Reasoning", "AI Planning", "Game Trees"], "full_text": "                                                            Interactive and Adaptable Media                                                   295\n\n\n\n\n        AI Model for Computer games based on Case Based\n                     Reasoning and AI Planning\n                         Vlado Menkovski                                                          Dimitrios Metafas\n                 Athens Information Technology                                                Athens Information Technology\n                      0.8km Markopoulou Ave.                                                    0.8km Markopoulou Ave.\n                       Peania, 19002, Greece                                                     Peania, 19002, Greece\n                           vmen@ait.edu.gr                                                          dmeta@ait.edu.gr\n\n\nAbstract                                                                           With the advancement in computer hardware increasingly\n                                                                              more computing power is left for executing AI algorithms in\n      Making efficient AI models for games with imperfect                     games. In the past AI in games was mainly a cheating set of\ninformation can be a particular challenge. Considering the large              instructions that simulated the increasing difficulty in the game\nnumber of possible moves and the incorporated uncertainties                   environment so that the player had the illusion of real counterpart.\nbuilding game trees for these games becomes very difficult due to             Improvement in available memory and processing power allows\nthe exponential growth of the number of nodes at each level. This             implementation of more intelligent algorithms for building the\neffort is focused on presenting a method of combined Case Based               game environment as well as direct interaction with the human\nReasoning (CBR) with AI Planning which drastically reduces the                players.\nsize of game trees. Instead of looking at all possible combinations\n                                                                                    In this particular research the emphasis is put on the\nwe can focus only on the moves that lead us to specific strategies\n                                                                              interaction between the AI agent and a computer player in the\nin effect discarding meaningless moves. These strategies are\n                                                                              realm of the game rules. It is particularly focused on turn based\nselected by finding similarities to cases in the CBR database. The\n                                                                              games that have the elements of uncertainty like dice or concealed\nstrategies are formed by a set of desired goals. The AI planning is\n                                                                              information. At the beginning a description of Game AI\nresponsible for creating a plan to reach these goals. The plan is\n                                                                              algorithms are given; such as Game Trees and Minimax. The\nbasically a set of moves that brings the player to this goal. By\n                                                                              following section describes an approach of using AI Planning to\nfollowing these steps and not regarding the vast number of other\n                                                                              improve building Game Trees in games with imperfect\npossible moves the model develops Game Trees which grows\n                                                                              information where Game Trees tend to be very large with high\nslower so they can be built with more feature moves restricted by\n                                                                              growth ratio. Section 4 discusses another approach that provides a\nthe same amount of memory.\n                                                                              significant reduction to the number of considered moves in order\nCategories and Subject Descriptors                                            to find the favorable strategy of the AI player. This approach uses\n                                                                              AI Planning techniques and Case Base Reasoning (CBR) to plan\n                                                                              for different scenarios in predetermined strategies which would be\nI.2.1 [Applications and Expert Systems]: Games                                analogous to human player experience in the particular game. The\n                                                                              CBR database illustrates a set of past experiences for the AI\nGeneral Terms                                                                 problem and the AI Planning illustrates the procedure to deal with\nAlgorithms, Performance.\n                                                                              the given situation in the game. In the next two sections\nKeywords                                                                      implementations and evaluations of both approaches are given.\nGame AI, Case Based Reasoning, AI Planning, Game Trees                        The AI Planning approach is implemented with the Tic-tac-toe\n                                                                              game and the combined AI Planning and CBR approach is\n1. Introduction                                                               implemented with a model for the Monopoly game. The last part\n     The goal of this effort is to explore a model for design and             contains conclusions and future work ideas.\nimplementation of an AI agent for turn based games. This model\nprovides for building more capable computer opponents that rely\n                                                                              2. Game Trees and Minimax\non strategies that closely resemble human approach in solving                      Game Trees are common model for evaluating how different\nproblems opposed to classical computational centric heuristics in             combinations of moves from the player and his opponents will\ngame AI. In this manner the computational resources can be                    affect the future position of the player and eventually the end\nfocused on more sensible strategies for the game play.                        result of the game. An algorithm that decides on the next move by\n                                                                              evaluating the results from the built Game Tree is minimax [1].\n                                                                              Minimax assumes that the player at hand will always choose the\n Permission to make digital or hard copies of all or part of this work for    best possible move for him, in other words the player will try to\n personal or classroom use is granted without fee provided that copies are    select the move that maximizes the result of the evaluation\n not made or distributed for profit or commercial advantage and that          function over the game state. So basically the player at hand needs\n copies bear this notice and the full citation on the first page. To copy     to choose the best move overall while taking into account that the\n otherwise, or republish, to post on servers or to redistribute to lists,     next player(s) will try to do the same thing. Minimax tries to\n requires prior specific permission and/or a fee.                             maximize the minimum gain. Minimax can be applied to multiple\n DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.\n Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00\n\n\n\n                                3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f296                                                              DIMEA 2008\n\n\n\nlevels of nodes on the game tree, where the leaves bring the final        problem is the enormous number of possibilities that the player\nknown (or considered) game state.                                         can choose from in making his move. In addition some of the\nThe minimax theorem states:                                               moves are accompanied with probabilities based on the random\n                                                                          elements in the games. The number of possible moves\nFor every two-person, zero-sum game there is a mixed strategy             exponentially grows with each move so the depth of the search\nfor each player, such that the expected payoff for both is the same       has to be very limited to accommodate for the memory\nvalue V when the players use these strategies. Furthermore, V is          limitations.\nthe best payoff each can expect to receive from a play of the                  The basic idea behind using HTN for building game trees is\ngame; that is, these mixed strategies are the optimal strategies for      that the HTN provides the means of expressing high level goals\nthe two players.                                                          and describing strategies how to reach those goals. These goals\nThis theorem was established by John von Neumann, who is                  may be decomposed in goals at lower level called sub-goals. This\nquoted as saying \"As far as I can see, there could be no theory of        approach closely resembles the way a human player usually\ngames \u2026 without that theorem \u2026 I thought there was nothing                addresses a complex problem. It is also good for domains where\nworth publishing until the Minimax Theorem was proved\" [2].               classical search for solution is not feasible due to the vastness of\n                                                                          the problem domain or uncertainties.\n     A simple example of minimax can be observed by building a\ngame tree of the tic-tac-toe game. The tic-tac-toe game is a simple       3.2.1 Hierarchical Task Networks\ngame which can end by the first player wining, the second player\nwining or a tie. There are nine positions for each of the players in           The Hierarchical Task Network, or HTN, is an approach to\nwhich at each turn the player puts X or O sign. If the player has         automated planning in which the dependency among actions can\nthree adjacent signs in a row, column or the two diagonals he or          be given in the form of networks [9] [Figure 1].\nshe wins. This game has limited number of position and it is well              A simple task network (or just a task network for short) is an\nsuited for building the whole game tree. The leaves of this tree          acyclic digraph \u0002 \u0003 \u0004\u0005\u0006 \u0007\b in which U is the node set, E is the\nwill be final positions in the game. A heuristics evaluation              edge set, and each node       \u0005 contains a task . The edges of \u0002\nfunction will also need to be written to evaluate the value of each       define a partial ordering of U. If the partial ordering is total, then\nnode along the way.                                                       we say that \u0002 is totally ordered, in which case \u0002 can be written as\n                                                                          a sequence of tasks \u0002 \u0003 \u000e \u0006 \u000f \u0006 \u0010 \u0006 \u0011 \u0012.\n3. AI Planning for building Game Trees\n3.1.1 AI Planning                                                                                Buy milk\n      AI Planning also referred as Automated Planning and\nScheduling is a branch of Artificial Intelligence that focuses on\nfinding strategies or sequences of actions that reach a predefined\ngoal [3]. Typical execution of AI Planning algorithms is by                  Go to (shop)           Purchase             Go to (home)\nintelligent agents, autonomous robots and unmanned vehicles.\nOpposed to classical control or classification AI Planning results                  Figure 1: Simple Hierarchical Task Network\nwith complex solutions that are derived from multidimensional             A Simple Task Network (STN) method is a 4-tuple of its name,\nspace.                                                                    task, precondition and a task network. The name of the method\n      AI Planning algorithms are also common in the video game            lets us refer unambiguously to substitution instances of the\ndevelopment. They solve broad range of problems from path                 method, without having to write the preconditions and effects\nfinding to action planning. A typical planner takes three inputs: a       explicitly. The task tells what kind of task can be applied if the\ndescription of the initial state of the world, a description of the       preconditions are met. The preconditions specify the conditions\ndesired goal, and a set of possible actions. Some efforts for             that the current state needs to satisfy in order for the method to be\nincorporating planning techniques for building game trees have            applied. And the network defines the specific subtasks to\nalso shown up, similar to the approach explored in this effort. In        accomplish in order to accomplish the task.\naddition Cased Based Reasoning [4] techniques are also gathering          A method is relevant for a task if the current state satisfies the\npopularity in developing strategies based in prior knowledge              preconditions of a method that implements that task. This task can\nabout the problems in the games. One of the benefits from                 be then substituted with the instance of the method. The\nHierarchical Task Network (HTN) [5] planning is the possibility           substitution is basically giving the method network as a solution\nto build Game Trees based on HTN plans; this method is                    for the task.\ndescribed in the following section.                                       If there is a task \u201cGo home\u201d and the distance to home is 3km\n3.2 Game Trees with AI Planning                                           [Figure 2] and there exists a method walk-to and this method has a\n                                                                          precondition that the distance is less than 5km, then a substation\n     An adaptation of the HTN planning can be used to build               to the task \u201cGo home\u201d can be made with this method instance.\nmuch smaller and more efficient game trees. This idea has already\nbeen implemented in the Bridge Baron a computer program for                                               Go-to (from, to)\nthe game of Contact Bridge [6].\n     Computer programs based on Game Tree search techniques                  If (to \u2013 from) < 5km\nare now as good as or better than humans in many games like\nChess [7] and checkers [8], but there are some difficulties in                                                  Walk (to)\nbuilding a game tree for games that have imperfect information\nand added uncertainty like card or games with dice. The main                                    Figure 2: HTN Method\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                                 Interactive and Adaptable Media                                                     297\n\n\n\nIf the distance is larger than 5km another methhod instance needs                                                        present moves generated by\n                                                                                   plan. The branches of the game tree rep\nto be substituted [Figure 3].                                                      the methods. Tignum2 applies all metthods applicable to a given\n                                                                                   state of the world to produce new    w states of the world and\n                              Go-to (from, to)                                                                           n applicable methods that\n                                                                                   continues recursively until there are no\n                                                                                   have not already been applied to th  he appropriate state of the\n   If(to \u2013 from) < 5km                                 If(tto \u2013 from) < 200km\n                                                                                   world.\n                                                                                   In the task network generated by Tignuum2, the order in which the\n                          Walk (to)               Drive(to)                        actions will occur is determined by the total-ordering constraints.\n                                                                                   By listing the actions in the order they will occur, the task\n                         Figure 3: HTN Method 2                                                                         me tree [Figure 4] [Figure 5].\n                                                                                   network can be \u201cserialized\u201d into a gam\n   An STN planning domain is a set of operatioons O and a set of\nmethods M. A STN planning problem is a 4-tuuple of the initial\nstate S0, the task network w called initial taskk network and the\nSTN domain. A plan \u0013 \u0003 \u0014\u000e \u0006 \u0010 \u0006 \u0014\u0015 \u0012 is a solutiion for a planning\nproblem if there is a way to decompose w into \u03c0 if \u03c0 is executable\nand each decomposition is applicable in the apppropriate state of\nthe world. The algorithm that is capable to decompose these\nnetworks into plans is called Total-forward-decoomposition (TFD)\n[9] or Partial-forward-decomposition (PFD). H  However there are\ncases where one does not want to use a forwaard-decomposition\nprocedure. HTN planning is generalization of S  STN planning that\ngives the planning procedure more freedom       m about how to\nconstruct the task networks.\n     In order to provide this freedom, a bookkeeeping mechanism                               Figure 4: HTN to Game Trree Algorithm\nis needed to represent constraints that the plannning algorithm has\nnot yet enforced. The bookkeeping is done byy representing the\nunenforced constraints explicitly in the task netw\n                                                 work.\n     The HTN generalizes the definition of a task network in\nSTN. A task network is the pair \u0002 \u0003 \u0004\u0005\u0006 \u0017\b w    where \u0005 is a set of\ntask nodes and C is a set of constraints. Each constraint in C\nspecifies a requirement that must be satisfied byy every plan that is\na solution to a planning problem.\n     The definition of a method in HTN alsoo generalizes the\ndefinition used in STN planning. A HTN plaan is a 4-tuple of\nname, task, subtasks, and constraints. The ssubtasks and the\nconstraints form the task network. The HTN plannning domains are\nidentical to STN planning domains except they uuse HTN methods\ninstead of STN methods.                                                                     Figure 5: Game Tree built frrom HTN\n\n     Compared to classical planners the prim  mary advantage of                    4. Case Based Reasoning in\n                                                                                                            n Game Strategies\nHTN planners is their sophisticated knowledge rrepresentation and\nreasoning capabilities. They can represent and solve a variety of\n                                                                                   4.1 Case Based Reasoning\n                                                                                        Case-based reasoning (CBR) is a well established subfield of\nnon-classical planning problems; with a goodd set of HTNs to\n                                                                                   Artificial Intelligence (AI), both as a mean for addressing AI\nguide them, they can solve classical planning pproblems orders of\n                                                                                   problems and as a basis for standalone AI technology.\nmagnitude more quickly than classical or neocclassical planners.\nThe primary disadvantage of HTN is the neeed of the domain                         Case-based reasoning is a paradigm    m for combining problem-\nauthor to write not only a set of planning operaators but also a set               solving and learning that has became one of the most successful\nof methods.                                                                        applied subfield of AI of recent yeaars. CBR is based on the\n                                                                                                                          I means that new problems\n                                                                                   intuition that problems tend to recur. It\n3.2.2 HTN Planning in building Game Trees\n                                                                                   are often similar to previously en     ncountered problems and,\n     For a HTN planning algorithm to be adapted to build game                      therefore, that past solutions may be off use in the current situation\ntrees we need to define the domain (set of H  HTN methods and                      [10].\noperators) which is the domain of the game. Thiis is in some sense\n                                                                                   CBR is particularly applicable to probllems where earlier cases are\na knowledge representation of the rules of thee game, the game\n                                                                                   available, even when the domain is not  n understood well enough\nenvironments and possible strategies of game plaay.\n                                                                                   for a deep domain model. Helpdesks,, diagnosis or classification\n     In this domain the game rules as well as knnown strategies to                 systems have been the most successfu    ul areas of application, e.g.,\ntackle specific task are defined. The implem  mentation of Game                    to determine a fault or diagnostic an illness from observed\nTree building with HTN is called Tignnum2 [9]. This                                attributes, or to determine whether orr not a certain treatment or\nimplementation uses a procedure similaar to forward-                               repair is necessary given a set of past solved cases [11].\ndecomposition, but adapted to build up a game tree rather than a\n\n\n\n                                      3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f298                                                             DIMEA 2008\n\n\n\nCentral tasks that all CBR methods have to deal with are [12]: \"to       And in the third position if the two of center, middle top and\nidentify the current problem situation, find a past case similar to      middle left are available the position is a certain victory.\nthe new one, use that case to suggest a solution to the current\nproblem, evaluate the proposed solution, and update the system by             There are many different arrangements of the player\u2019s tokens\nlearning from this experience. How this is done, what part of the        that give equivalent positions as these three positions. By using\nprocess that is focused, what type of problems that drives the           planning we do not need to consider all possible layouts but just\nmethods, etc. varies considerably, however\".                             consider these three similar to what a human would consider.\n\n     While the underlying ideas of CBR can be applied                          The game starts from an empty table.\nconsistently across application domains, the specific                          The two relevant strategies that would lead to these positions\nimplementation of the CBR methods \u2013in particular retrieval and           are to take one corner or to take the center [Figure 7].\nsimilarity functions\u2013 is highly customized to the application at\nhand.\n4.2 CBR and Games\n     Many different implementations of CBR exist in games.\nCBR technology is nicely suited for recognizing complex\nsituations much easier and more elegant than traditional parameter                   Figure 7: Tic-tac-toe Two starting moves\ncomparison or function evaluation. There are especially evident\ncases in real time strategies where different attack and defense of            The center position as we can see in the simulation results\nglobal strategies are nicely defined by CBR datasets and later used      lead to a bigger number of victorious endings but it is also a\nin the running games. Also intelligent bots behavior is also             straight forward strategy with obvious defense strategy.\nanother typical example. Depending on the number of enemy bots                At this point we need to consider the moves of the opponent.\nthe layout of the terrain and position of human players the CBR          If we take the left branch the opponent moves can be a center, a\nsystem finds the closest CBR case and employs that strategy              corner or a middle field. We also need to differentiate with a\nagainst the human players which in prior evaluation was proved to        move to a corner adjacent with our like top left or bottom right or\nbe highly efficient.                                                     across the center to bottom right [Figure 8].\n5. Game Trees with AI Planning \u2013 Tic-tac-toe\n      In order to show the expressive power of AI Planning in\ndefining strategies for games, and the use of these plans to build\nGame Trees I implemented an algorithm that builds Game Trees\nfor the Tic-Tac-Toe game.\n                                                                             Figure 8: Tic-tac-toe opponent response to corner move\n     The game tree of Tic-Tac-Toe shows 255,168 possible\n                                                                               In cases one and two, we have a clear path to executing\ngames of which 131,184 are won by X (the first player), 77904\n                                                                         strategy 3 so we need to capture the diagonally opposite field.\nare won by O and the rest 46,080 are draw [13]. All these games\n                                                                         And as for the third case the best way to go is to capture the center\ncan be derived from building a complete Game Tree.\n                                                                         and go for strategy 1 or 2 depending of the opponent\u2019s next move.\n     Even though it is possible to build a complete game tree of\nTic-tac-toe it is definitely not an optimal solution. Many of the\nmoves in this tree would be symmetrical and also there are a many\nmoves that would be illogical or at least a bad strategy to even\nconsider.\n     So what strategy should X (the first player) choose in order                Figure 9: Tic-tac-toe move 2 after corner opening\nto win the game?                                                              The first move leads to certain victory, O will have to go to\n                                                                         the center and X will achieve strategy 3 [Figure 9]. The second\n     There are few positions that lead to certain victory. These\n                                                                         move is a possible way to strategy 3 if O makes a mistake in the\npositions involve simultaneous attack on two positions so the\n                                                                         next loop, so X goes to the opposite corner. For the third case\nother player could not defend, basically the only trick in Tic-Tac-\n                                                                         since O is playing a valid strategy the only move that leaves a\nToe.\n                                                                         possible mistake from O would be to take the center and wait for\n                                                                         O to go to the middle and then achieve strategy 1 or 3 which will\n                                                                         be a symmetric situation to the one that we will find if we\n                                                                         branched with the center.\n\n\n        Figure 6: Tic-tac-toe winning strategy positions\n     Position 1 leads to victory if the two of the three fields: top\nmiddle, bottom left corner and bottom right corner are free\n[Figure 6].\n    Position 2 lead to victory if two of the three fields: top right         Figure 10: Tic-tac-toe opponent response to center move\ncorner, bottom right corner and bottom middle are free [Figure ].            If we go back to the second branch [Figure 10], a possible\n                                                                         way for the second player to engage is corner or middle. The first\n\n\n\n                             3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                         Interactive and Adaptable Media                                                      299\n\n\n\nmove is a valid strategy for O and can be meeet with a opposite                  This HTN when executed will reesult with plans for possible\ncorner move from X to try a mistake from O in the future exactly           game scenarios. By creating nodes from m each position and linking\nthe same as in the third case above from the preevious branch, and         them with branches with the move of the\n                                                                                                                 t player we create a game\nanother move would be go to the middle whhere X eventually                 tree for the Tic-tac-toe game over whiich we can run the minimax\nachieves strategy 1 or 2.                                                  algorithm.\n                                                                                 This set up with 7 plans with 3 taarget strategies creates a tree\n                                                                           for Tic-tac-toe which considers all posssible moves for the second\n                                                                           player with only 457 games, 281 of whichw       X wins 176 are draw\n                                                                           and 0 where the second opponent wins.    w      This is a significant\n       Figure 11: Tic-tac-toe Move 2 after centter opening                 reduction over the 255, 168 possible games with a complete game\n     The fist move will lead to win if O moves to the middle or a          tree. These reductions can be very useeful for devices with limited\ndraw if it goes for the corners [Figure 11]. In tthe second case O         computing capabilities but also we prrove a very important point\nhas to block the lower left corner which leavees X to go for the                                                    d\n                                                                           that planning can be very efficient if designing    meaningful game\nmiddle left or corner left which are strategy 1 andd 2.                    trees by applying reasoning very similar to human player\n     To sum the strategies for the planning, first we have center or       reasoning.\ncorner strategy for the beginning. Then for the ceenter we try to get            Further improvements to the gam   me tree are also possible if\nthe corners with the particularly the one oppoosite to the one O           the opponents moves are also planned    d, in other words if we drop\nholds. If the center is empty for the second strateegy we go for it or                                           m\n                                                                           all the meaningless and symmetrical moves      of the opponent.\nwe go for the opposite corner. After this point wwe either block the       6. Game AI in Monopoly\nopponent or try to implement strategies 1, 2 orr 3 which lead to\nvictory.\n                                                                           6.1 Overview of the AI Imp\n                                                                                                    plementation\nPlan 1: Take center                                                              The AI agent is responsible for the moves of the artificial\nPreconditions: Center empty                                                players in the game. The core principlee of the AI agent is building\nPlan 2: Take corner                                                        a Game Tree with all the sensible movees that all the players would\nPreconditions: All corners empty                                           make from the current point of timee forward. Then using the\nPlan 3: Take corner after center                                           minimax algorithm the agent selects the  t move that in the future\nPreconditions: We have center take corner oppossite to the one the         would bring the computer player mo     ost favorable game position\nopponent has                                                               with the highest probability. Building a Game Tree in this game\nPlan 4: Take diagonal corner                                               that would be big enough to consider sufficient number of moves\nPreconditions: We have a corner, the opponent ha    as the ce\u2212nter and\n                                                                           is obstructed by the vastness of posssible moves in combination\n the corner opposite to the one we have is free.\nPlan 5: Block                                                              with all the possible random landings of the dice. The number of\nPrecondition: The opponent has tree tokens in a rrow, colu\u2212mn or di        nodes of the game tree exponentially     y grows at each level. To\nagonal                                                                     tackle this problem the AI agents incorporates two already\nPlan 6: Win                                                                discussed technologies: Case Based Reeasoning and AI Planning.\nPreconditions: We have two tokens in a row, column or dia\u2212gonal a                The technologies are employed in the following manner.\nnd the third place is free                                                 First the agent searches the CBR datab base to find the case with the\nPlan 7: Tie                                                                largest similarity with the current statee of the board. This case is\nPreconditions: If all places are taken, it\u2019s a tie.                        associated with a playing strategy. Th  he strategy consists of goal\n5.1 Hierarchical Task Network                                                                                      f and the plans consist of\n                                                                           that the planner needs to build plans for,\n     Top level task is Play [Figure 12]. This is a complex task and        consecutive player moves that bring th   he player to that goal. This\ncan be derived into: Win, Block, Tie or Searrch for Plan. The              way only moves that are part of that strrategy are considered, those\nSearch for plan is derived to both Plan 1 and Plaan 2 or Plan 3 and        being a small fraction of the overall poossible moves the number of\nPlan 4, which later leads to a call for the opponnent\u2019s move and a         edges of the game tree at each level deccreases immensely.\nrecursive call to Play.                                                          At each level of the game tree thee model considers the moves\n                                                                           of a single player. After the strateg    gies of the AI player are\n                                                                           considered the response to those strateegies needs to be considered\n                                                                           by the opponent(s). The move of the opponent(s) depends of the\n                                                                           probability distribution of the dice as well as the strategy of the\n                                                                           player. A more general strategy needss to be implemented for the\n                                                                           opponent\u2019s (human player) moves sin     nce we cannot be aware of\n                                                                           the expertise of the opponent. This geeneral strategy would bring\n                                                                           more plausible moves than the focused  d strategy of the AI player.\n                                                                                 After covering all opponents the   t    agent comes back to\n                                                                           deducting a feature move of the com     mputer player by using the\n                                                                           CBR selected plan strategy. After creating several loops of\n                                                                           strategies and reaching a reasonable size\n                                                                                                                   s    of a Game Tree taking\n                                                                           into account the memory limits an        nd the rapidly decreasing\n                                                                           probabilities that the move is possiblee due to the distribution of\n                                                                           the dice the building of the Game Treee stops. Then the minimax\n                                                                           algorithm searches the Game Tree and decides on the most\n                                                                           favorable move for the AI player usiing the minimax algorithm.\n                      Figure 12: Tic-tac-toe HT\n                                              TN                           The process is repeated each time the AIA player is up.\n\n\n\n                               3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f300                                                             DIMEA 2008\n\n\n\n      Buying, auctioning and trading game moves are always                    On the other hand the MonopolySolution class holds the\naccompanied by return of investment calculations in making the           three particular attributes that are needed for the planning, the\nplans. These calculations represent adaptation of the more general       planning Domain, State and TaskList.\nplanning associated with the cases in the CBR database. These\nadaptations are necessary due to the fact that the cases do not               The game is implemented by using the Model-View-\nidentically correspond to the situation on the table. In addition        Controller software development pattern. The controller is\ncalculating the game position value of each node of the game tree        responsible for implementing the game rules and handling all of\nis done by heuristic functions that incorporate economic                 the events in the game like roll of dice, input commands for\ncalculations of net present value, cash, and strategic layout and so     trading, auctioning and etc from the players. The View layer is\non. For example railroads in monopoly are known to be                    responsible for displaying the board and all of the input widgets\nstrategically effective because they bring constant income even          on to the game screen, and the models are data structures\nthough the income can be smaller than building on other                  representing the game state [Figure 14].\nproperties.\n6.2 Details on the CBR Implementation\n     The implementation of the CBR is by using the JColibri2\nplatform. JColibri2 is an object-oriented framework in Java for\nbuilding CBR systems that is an evolution of previous work on\nknowledge intensive CBR [14].\n     For this implementation we need to look into three particular\nclasses of the JColibri2 platform. The StandardCBRApplication,\nConnector, CBRQuery. For a JColibri2 implementation the\nStandardCBRApplication interface needs to be implemented.\n     The CBR cycle executed accepts an instance of CBRQuery.\nThis class represents a CBR query to the CBR database. The\ndescription component (instance of CaseComponent) represents\nthe description of the case that will be looked up in the database.\nAll cases and case solutions are implementing the\nCaseComponent interface.\n     The JColibri2 platform connects to the CBR database via a\nConnector class. Each connector implements all the necessary\nmethods for accessing the database, retrieval of cases, storing and\ndeletion of cases. This implementation uses a custom XML\nstructure for holding the CBR cases. Since the game will not\nupdate the CBR database only read it, a XML solution satisfies\nthe needs. The XML file to a certain extent is similar to the XML\nrepresentation of the board. We are interested in finding one\nCBRCase that is the most similar case to the situation in the game\nat the time of the search. This procedure is done in the cycle\nmethod of the CBRApplication. The JColibri2 CBR comparison is\ndone by Nearest Neighbor (NN) search method.\n     JColibri2 offers implementations for NN search algorithms\nof simple attributes. These implementations are called local\nsimilarities. For complex attributes like in our case global                    Figure 14: Class diagram of the Monopoly models\ncustomized similarity mechanisms need to be implemented.\n     The MonopolyDescription class [Figure 13] is basically a\n                                                                         6.2.1 Complex Similarity representation in CBR\nserialization of the GameState. It holds all the information about             The similarity measurement part of the Nearest Neighbor\n                                                                         algorithm JColibri2 is implemented by implementing the\nthe state of the board, the players, their amount of cash etc.\n                                                                         LocalSimiralrityFunction and the GlobalSimiralityFunction\n                                                                         interface. A local similarity function is applied to simple attributes\n                                                                         by the NN algorithm, and a global similarity function is applied to\n                                                                         compound attributes. In the case of our implementation the\n                                                                         attributes of the MonopolyDescription are compound attributes\n                                                                         describing the state of the board, number of players, amount of\n                                                                         cash for every player and etc. Since MonopolyDescription is a\n                                                                         custom CaseComponent a global similarity function needs to be\n                                                                         implemented to accurately find the distance between different\n                                                                         CBR cases.\n                                                                             The similarity mechanism is inseparable core element of the\n                                                                         CBR system. This mechanism represents how the CBR decides\n                                                                         which strategy is best suited for the particular situation by\n Figure 13: Class diagram of the Monopoly Case component\n                          models\n\n\n\n                             3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                       Interactive and Adaptable Media                                                    301\n\n\n\ncalculating the distance or similarity to other cases in the                   JSHOP2 uses ordered task decomposition in reducing the\ndatabase.                                                                 HTN to list of primitive tasks which form the plans. An ordered\n                                                                          task decomposition planner is an HTN planner that plans for tasks\n     For the monopoly implementation we need to consider                  in the same order that they will be executed. This reduces the\nseveral basic strategies. Monopoly is based on investing in               complexity of reasoning by removing a great deal of uncertainty\nproperties and receiving revenues from those investments. One of          about the world, which makes it easy to incorporate substantial\nthe basic strategies of the game is to build a set of properties that     expressive power into the planning algorithm. In addition to the\nwill bring constant income larger than the one of the opponents.          usual HTN methods and operators, the planners can make use of\nSo in time the opponents will have to declare bankruptcy. But on          axioms, can do mixed symbolic/numeric conditions, and can do\nthe other hand over investment can lead to too stretched resources        external function calls.\nwith low income that will eventually drove the player to\nbankruptcy. To decide on these two we need a clear separation                   In order for the JSHOP2 planer to generate plans it needs\ninto two groups of cases in the CBR database. The first group of          tree crucial components: Domain, State and Tasks. The Domain\ncases will represent a situation on the board where the player has        defines all the functionalities that the particular domain offers.\nsignificant income per loop formed of one or more color group             These are simple and complex tasks. The complex tasks also\nproperties, maybe railroads, some buildings on them and so on. It         called methods create the hierarchy with the fact that they can be\nis important to note that in this case the player is better situated      evaluated by simple tasks of other complex tasks. This is how a\nthan his opponents so he only needs to survive long enough to win         hierarchical structure of tasks is formed. The problem reduction is\nthe game. In the other group of cases either the opponent is not          done by reducing the high level complex tasks to simpler until all\nwell positioned on the board or its opponents are better situated.        the tasks are primitive. The list of primitive tasks forms the plan.\nIn this case further investments are necessary to improve the\nsituation so the player can have a chance of winning in the long                The State represents the state of the system. It is a simple\nrun.                                                                      database of facts that represent the state of the system. The State\n                                                                          is necessary to determine the way the problems or tasks are\n      These metrics can be owning color groups, valuing groups of         reduced to their primitive level. The reduction is done by\nrailroads, evaluating the other opponents as well, and considering        satisfying different prerequisites set in the methods; these\nthe amount of cash. As it is obvious in monopoly the number of            prerequisites are defined in the state. The Tasks are high level\nstreets is not as nearly as important as the combination of streets       tasks or methods defined in the Domain. The planner based on the\nthe player owns. It is also important to note that one CBR case           State and the goals selects one or more high level tasks that need\ndoes not hold only a single strategy in place, but its solution can       to be reduced to plans [Figure 15].\nhave multiple different strategic goals. For example one CBR case\nmight simultaneously say buy this land to form a color group but                       Tasks\nalso trade some other unimportant property to increase cash\n                                                                                                                    Core Planner\namount.\n     The cases do not represent all possible combinations of board\npositions. They are only representation of typical game scenarios.                     State                        Plan\nThe CBR Case solutions do not give exact instructions in general\nbut rather strategic goals. For example one CBR Solution might\n                                                                                           Figure 15: Diagram of a Planner\nsay trade the streets that you only have one of each for the ones\nthat you have two of that color already. Then the planner based on            The plans then generate the game moves. The number of\nthe situation on the board needs to decompose this high level task        moves generated by the plans is just a fraction of the possible\nto a low level operations. Like offer \"Mediterranean Avenue\" for          moves at that point. This reduces the game tree providing the\n\"Reading Railroad\" and offer $50. The exact amounts and actual            opportunity to generate smaller and deeper game trees and making\nstreets are left to the planer to evaluate.                               more efficient decisions in general.\n\n      The monopoly CBR database is currently in development on            7. Conclusion\na monopoly clone game called Spaceopoly. The cases are                          Even though the results from the CBR database are not\narchitected based on human player experience and knowledge.               complete at this time partial strategies are implemented as cases\nThere is a plan of making a number of slightly different strategies       and recognized during game play by the CBR system. These\nthat differ on the style of playing and then running simulation           smaller local strategies coupled with more global higher level\ntests that would determine the particular validity of each database       strategies that are particularly important at the beginning of the\nas well as validity of certain segments of the strategy or even           game would form a complete CBR database and represent a\nparticular cases in the database.                                         knowledge engineered style of playing of the AI player.\n      The actual execution of the strategies will not differ from              The AI Planning approach is a proven method by the tic-tac-\nstrategy to strategy since the plan execution is more related to the      toe experiment and is suitable for implementing the strategies\nstructure and rules of the game than to the actual playing strategy.      associated with the CBR cases.\n6.3 Details on the Planning Implementation                                     This approach in general benefits from both technologies,\n     For the purpose of planning this implementation uses a               CBR as well as AI Planning and comprises an elegant solution.\nmodification of the JSHOP2 planner. The Java Simple                       Even though AI Planning can be enough as a single technology\nHierarchical Ordered Planner 2 is a domain independent HTN                for some simpler problems like tic-tac-toe the complexity of\nplanning system [15].                                                     Monopoly would mean that the Planner would have to incorporate\n\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f302                                                              DIMEA 2008\n\n\n\nlarge and complex domain and a very big state model. The CBR              reasoning. People in addition to logic of reasoning in situations\napplication helps reduce this complexity by focusing the planning         with lack of information rely to planning strategies and prior\non smaller domain of the game. Basically the CBR reduces the              experience, exactly the intuition behind CBR \u2013 AI Planning\noverall goal of the play (wining the game) to smaller more                architecture.\nconcrete goals suitable to the particular state of the game, thus\nreducing the need for global planning strategies and complex              9. ACKNOWLEDGMENTS\nplanning domain.                                                               We would like to thank Prof. Sofia Tsekeridou for her\n                                                                          involvement in the valuable discussions we had on the topic of\n     Furthermore this symbiosis of technologies gives way for             CBR.\nmore precise and finely tuned strategies which can be difficult to\ninclude into global plan for the whole game. One simple example           10. REFERENCES\nfor the Monopoly game would be this: Sometimes it\u2019s better to             [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]\nstay in jail because rolling double increases the probability of              http://en.wikipedia.org/wiki/Minimax.\nlanding on some field (two, four, six, eight, ten or twelve steps\nfrom the jail) that can be of great importance to the rest of the         [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.\ngame. These and similar small local strategies can be easily                  Annalen. 100 (1928) 295-320\nrecognized by similar cases in the CBR database.                          [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,\n                                                                              2008.] http://en.wikipedia.org/wiki/Automated_planning.\n      In other words the system is flexible enough so that new\nstrategies can be incorporated easily missing strategies can be also      [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based\nrecognized by the distance metrics as well as wrong assumptions               Strategy Game with Plan Adaptation and Ontology-based\nin the strategies can be easily recognized.                                   retrieval.\n\n     One other important property of the system is that is highly         [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for\nconfigurable. The game its self can be diversely different                    hierarchical task-network planning. Technical Report TR-94-\ndepending on the configuration of the board. Even though the                  31, UMIACS.\nplatform is restricted to Monopoly type of games, changing the            [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning\nlayout and values of the fields effectively brings completely                 approach decrarer play in contract bridge. Computational\ndifferent properties of the game. In addition the CBR database                Intelligence. 1996, Vol. 12, 1.\nrepresents the entire experience of the AI Player. It can be filled       [7] One Jump Ahead: Challenging Human Supremacy in\nwith rich set of strategies or even configured with different flavors         Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.\nof difficulties of play, this of course coupled with the domain of\nthe planner which can differ from a case to a case as well.               [8] IBM. How Deep Blue works. [Online] 1997. [Cited: April\n                                                                              23, 2008.]\n8. Future Work                                                                http://www.research.ibm.com/deepblue/meet/html/d.3.2.html\n      Further exploration of this technology would go towards             [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.\ncomplete implementation of an AI aware agent for monopoly.                    Automated Planning theory and practice. s.l. : Morgan\nInitial results from the local cases with more specific strategies            Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.\nshow CBR as a capable tool for representing expertise in playing\nthe game. Completing the more general strategies and coupling             [10] Case Based Reasoning. Experiences, Lessons and Future.\nthem with the planning domain will give precise results on the                 Leake, David. s.l. : AAAI Press. MIT Press., 1997.\nbenefits from this architecture.                                          [11] Applying case-based reasoning: techniques for enterprise\n                                                                               systems. Watson, I. San Francisco, CA, USA : Morgan\n     There is also need for exploring the planning of strategies of\n                                                                               Kaufmann Publishers Inc., 1998.\nopponents. This task is to some extent different because we\ncannot always expect the opponent to select the best move we              [12] Plaza, A. Aamodt and E. Case-based reasoning:\nthink. In the Tic-tac-toe example all possible moves of the                    Foundational issues, methodological. AI Communications.\nopponent were taken into consideration, if we used the same                    1994, 7(i).\nplanner for the opponent only tie games would result from the             [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]\ngame tree. In other words mistakes of the players also need to be              http://en.wikipedia.org/wiki/Tic-tac-toe.\nconsidered.\n                                                                          [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An\n      The CBR Platform brings other functionalities well worth of              architecture for knowledge intensive CBR systems. Advances\nexploring as well. The revision stage of the JColibri2 platform is             in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :\nbasically capable of fine tuning strategies or even developing new             Springer-Verlag, Berlin Heidelberg, 2000.\nstrategies for the games. A well written underlying AI planning           [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to\nmodel with a capable feedback of the game tree evaluation back                 Synthesize Problem-Specific Planners. 2003.\nto the CBR revision capability can be an interesting concept in\nautomatic experience acquisition for the AI model.\n     There are also many other fields were combined CBR and\nplanning approach can be incorporated into a problem solution.\nThis combination is analogous in a big extent to a human way of\n\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nVon Neumann, J : Zur theorie der gesellschaftsspiele  Math. \nAnnalen. 100 (1928) 295-320 ", " Automated Planning. Wikipedia. [Online] [Cited: April 23, \n2008.] http://en.wikipedia.org/wiki/Automated_planning . ", " Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based \nStrategy Game with Plan Adaptation and Ontology-based \nretrieval ."], "publish_date": "2008-09-10", "approved": true}}, {"_index": "articles", "_id": "J-tqCI0BUzn7PtN-XItI", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "Numerical computing in engineering mathematics", "abstract": "The rapid advances in technology over the last decade have significantly altered the nature of engineering knowledge and skills required in the modern industries. In response to the changing professional requirements, engineering institutions have updated their curriculum and pedagogical practices. However, most of the changes in the curriculum have been focused on the core engineering courses without much consideration for the auxiliary courses in mathematics and sciences. In this paper, we aim to propose a new, augmented mathematics curriculum aimed at meeting the requirements of the modern, technology-based engineering workplace. The proposed updates require minimal resources and can be seamlessly integrated into the existing curriculum.", "authors": ["Firuz Kamalov", "Ho-Hon Leung"], "institutions": ["Department of Electrical Engineering, Canadian University Dubai", "Department of Mathematical Sciences, United Arab Emirates University"], "keywords": ["engineering mathematics", "numerical computing", "education", "Industry 4.0"], "full_text": "                                             Numerical computing in engineering mathematics\n                                                                              Firuz Kamalov                                                     Ho-Hon Leung\n                                                               Department of Electrical Engineering                                 Department of Mathematical Sciences\n                                                                   Canadian University Dubai                                          United Arab Emirates University\n                                                                           Dubai, UAE                                                          Al Ain, UAE\n                                                                         firuz@cud.ac.ae                                                  hohon.leung@uaeu.ac.ae\narXiv:2207.04698v1 [math.HO] 11 Jul 2022\n\n\n\n\n                                              Abstract\u2014The rapid advances in technology over the last                         technologies are also introduced by universities and colleges.\n                                           decade have significantly altered the nature of engineering knowl-                 Many institutions now offer degrees in artificial intelligence\n                                           edge and skills required in the modern industries. In response to                  and mechatronics which were not there 20 years ago. Finally,\n                                           the changing professional requirements, engineering institutions\n                                           have updated their curriculum and pedagogical practices. How-                      universities have revised their approaches to course delivery.\n                                           ever, most of the changes in the curriculum have been focused on                   Student-centered learning, project-based learning, and applied\n                                           the core engineering courses without much consideration for the                    learning have become popular in the new engineering educa-\n                                           auxiliary courses in mathematics and sciences. In this paper, we                   tional paradigm.\n                                           aim to propose a new, augmented mathematics curriculum aimed\n                                           at meeting the requirements of the modern, technology-based                           While significant effort has been made to revise the core\n                                           engineering workplace. The proposed updates require minimal                        engineering courses, the auxiliary courses in mathematics and\n                                           resources and can be seamlessly integrated into the existing                       sciences received little consideration. The mathematics and\n                                           curriculum.\n                                              Index Terms\u2014engineering mathematics; numerical computing;                       sciences courses play a key role in the engineering curriculum.\n                                           education; Industry 4.0                                                            Given their importance, the curriculum updates must also be\n                                                                                                                              extended to the auxiliary courses. By implementing a com-\n                                                                      I. I NTRODUCTION                                        prehensive update of the engineering curriculum that includes\n                                                                                                                              both the core and auxiliary courses, a more effective outcome\n                                              The 4th Industrial Revolution has had a dramatic impact                         can be achieved.\n                                           on the engineering profession. The modern technologies such\n                                           as artificial intelligence, the internet of things, and advanced                     The goal of this paper is to propose a modernized engineer-\n                                           robotics have altered engineering systems and processes. To-                       ing mathematics curriculum in line with the broader efforts to\n                                           day\u2019s engineers are expected to be able to leverage these                          update engineering education to adapt to Industry 4.0. The key\n                                           resources to produce their products. To meet the new profes-                       feature of the new curriculum is the introduction numerical\n                                           sional requirements, engineering educational institutions have                     computing in the existing mathematics courses. The latest\n                                           revised their curricula. The changes in the curricula include                      industrial revolution has been driven largely by the dramatic\n                                           both updating the existing programs as well as introduc-                           increase in computational power. Therefore, today\u2019s engineers\n                                           ing completely new programs. Given the rapid technological                         must be well-equipped to leverage the computing power in\n                                           progress, universities and colleges around the world are con-                      their work.\n                                           tinuously adapting to the ever-changing environment. While a                          Since mathematics courses are usually taken at the begin-\n                                           significant progress in modernizing the engineering curriculum                     ning of the study plan, it offers a natural avenue for introducing\n                                           has been achieved, there still remains room for improvement.                       numerical computing to students. Furthermore, many problems\n                                              Catalyzed by the exponential increase in computational                          in mathematics can be solved numerically making it natural\n                                           power and interconnectedness, the modern industrial revolu-                        to integrate numerical computing in mathematics courses.\n                                           tion has reshaped the skills and competencies required of the                      By studying numerical computing in mathematics courses,\n                                           engineers. The changes in engineering curricula in response to                     students will acquire the necessary theoretical and practical\n                                           Industry 4.0 have been threefold: i) modernizing the existing                      skills to apply in their downstream, specialized engineering\n                                           programs, ii) introduction of new programs, and iii) revising                      courses.\n                                           the pedagogical approach. Modernizing the existing programs\n                                           involves introduction of new courses in the study plan related                       This paper is structured as follows. Section 2 provides an\n                                           to emerging technologies. In addition, existing courses can be                     overview of the existing efforts to update the engineering\n                                           updated with new content. Fresh new programs in emerging                           curriculum in response to Industry 4.0. Section 3 discusses\n                                                                                                                              the current approaches to integrate scientific computing in\n                                              \u00a9 2022 IEEE. Personal use of this material is permitted. Permission from        mathematics courses. In Section 4, we present our proposal\n                                           IEEE must be obtained for all other uses, in any current or future media,          for modernizing the mathematics curriculum to integrate nu-\n                                           including reprinting/republishing this material for advertising or promotional\n                                           purposes, creating new collective works, for resale or redistribution to servers   merical computing. Section 5 concludes the paper with final\n                                           or lists, or reuse of any copyrighted component of this work in other works        remarks.\n\f    II. E NGINEERING EDUCATION AND I NDUSTRY 4.0                   courses and experiences for students. A lot of attention has\n                                                                   recently been given to equipping students with entrepreneurial\n   Engineering departments in colleges and universities have       skills. Students learn about entrepreneurship in their courses\nmade significant changes in their curricula in response to         as well as through university incubators.\nthe new environment created by the recent, rapid advances\nin technology. In particular, the existing programs have been            III. U NIVERSITY MATHEMATICS CURRICULUM\nupdated to include courses that target emerging technologies.         The mathematics curriculum changed very little in the\nCompletely new programs related to AI and mechatronics             current century. It remains a largely analytic domain, where so-\nhave also been adopted by universities. Innovations in the         lutions are mainly obtained manually. The current mathematics\nfield of engineering education continue to take place with new     curriculum emphasizes theory over practical approaches. For\ndevelopments on the horizon.                                       instance, when finding the extreme values of a function,\n   There exist several studies investigating the modern en-        derivative-based approach is preferred over the gradient de-\ngineering curricula and evaluating their effectiveness. It is      cent. There are two key reasons for why analytical approaches\nargued in [6] that engineering educators must prepare their        are favored over numerical methods. First, analytical solutions\nstudents to face three key challenges: sustainability, the 4th     are reliable and elegant. An analytical solution is guaranteed\nIndustrial Revolution, and employability. The authors find that    to be exact. Second, mathematics courses are usually taught\ncolleges and universities are responding to these challenges by    by pure mathematicians who have an inherent preference\nemphasizing student-centered learning, integration of theory       for analytical solutions. Pure mathematics which is based on\nand practice, digital and online learning, and the definition of   theorem proving is not amenable to numerical methods.\nprofessional competencies. In particular, response to the needs       Despite the popularity of analytical approaches to problem\nof Industry 4.0 require interdisciplinary collaboration across     solving in mathematics, there has been a growing push to inte-\nseveral programs and disciplines. Interaction and integration      grate computer algebra systems as part of the learning process.\nof technologies plays a key role in this process [10], [12]. In-   Computer algebra systems such as Matlab and Mathematica\nterdisciplinary engineering education requires sound pedagogy      are now routinely used in many mathematics courses. The\nand teaming experiences to encourage student in collaborative      study by Cretchley et al. [5] found that engineering students\nand interdisciplinary practice [23].                               were positive about the use of technology as a learning tool\n   Digital and online learning have become an important part       in mathematics courses. The increased use of technology in\nof modern education including in the field of engineering.         class helped improve student focus and interest in lectures.\nInformation technologies play a vital role in delivering digital   Student evaluations also indicated that they had a greater level\nlearning to students. Colleges and universities have made          of enjoyment towards the lectures due to the use of technology.\nsignificant investments to improve their information and com-      It is noteworthy that students chose not to rely too heavily on\nmunication technology (ICT) capacities [7].                        technology during the examinations despite the freedom to do\n   In response to the needs of Industry 4.0, some univer-          so. The students found it extremely important to be competent\nsities have adopted the framework of Education 4.0 [15],           with analytical mathematical skills as opposed to purely com-\n[19]. The new education framework consists of four main            putational skills. Some revealed that they learn the subjects\ncomponents: i) competencies, ii) learning methods, iii) ICT,       equally well without the help of scientific packages, although\nand iv) infrastructure. Students competencies are based on         the perception towards the use of computer is in general highly\ntechnological knowledge and skills for successful workplace        positive. Almost all students responded positively to Matlab\nperformance, while the learning methods are based on problem       as an effective tool for computation and graphing. Many\nsolving and challenge-based learning. In particular, active and    used Matlab for non-examination purposes. For example, they\nproject-based learning plays an important role in Education        utilized it to check their handwritten mathematical steps in\n4.0 [4], [8]. Other innovative approaches to learning such as      assignments and practice problems; and others used it for\nvirtual-reality based engineering education can help improve       exploration beyond the standard syllabus and curriculum.\nthe learning process related to Industry 4.0 [20].                    The influence of computer technology on students\u2019 aca-\n   In addition to technological progress, socio-cultural shifts    demic performance and learning experience has been inves-\nmust be taken into account in revising engineering curriculum.     tigated by several authors. Abdul Majid et al. [1], [2] used\nThe new generation of students has its unique worldview            Matlab as an aid to teach calculus to engineering students.\nwhich needs to be considered by the educators. In particular,      The software package was used for various course learning\nthe new generation is significantly affected by mobile devices     outcomes such as graphical display of mathematical functions,\nand digital media. Educational content must be tailored to         exploration, identifying and predicting structural patterns in\nthe new student preferences to achieve effective learning          evaluating a series of complex indefinite integrals, and numeri-\noutcomes [16]. Innovative approaches such gamification may         cal approximations in applied mathematics. The study showed\nhelp improve the learning process [13], [17].                      a positive impact on students\u2019 academic performance in the\n   Many universities have also introduced nontechnical updates     final examinations. The study concluded that the integration\nto their engineering curriculum. The most significant nontech-     of scientific packages into engineering mathematics courses\nnical update has been the introduction of entrepreneurship         could be effective under certain conditions. Similarly, other\n\fstudies [18], [21] also found a positive impact from the use        appropriate to apply numerical computing to mathematical\nof scientific software packages on students\u2019 motivation in          problems. Second, mathematics courses are usually taken\nlearning mathematics.                                               at the beginning of the study plan. Thus, students become\n   In a separate study by Brake [3], the authors investigated the   acquainted with numerical computing at an early stage. The\nuse of Matlab in engineering mathematics courses to increase        computing and programming skills acquired in this manner\nstudent confidence level and mathematical abilities. Matlab         will have a positive effect in the more advanced, downstream\nwas used to solve concrete engineering problems which require       engineering courses.\na deep understanding of underlying mathematical principles.            The key idea for the proposed curriculum update is the\nThe study found generally positive student response to the          addition of computing tutorials (labs) to mathematics courses.\nuse of software in their mathematics courses. However, the          In particular, we propose adding weekly computing tutorials\nresults of the study must be considered carefully given the         (labs) related to the main lecture material. For instance, in the\nsmall sample size of the subjects.                                  week in which students cover finding the extreme values of a\n   Although the majority of the studies were based on the           function, there will be a computing tutorial where students\nuse of Matlab, several other studies considered alternative         learn and implement the gradient descent algorithm. The\nmathematics software packages. The study by Kilicman et al.         suggested length of each tutorial is 1 hour. It is enough\n[11] focused on the use of Maple to help students understand        time to implement most of the numerical algorithms at the\nboth the theoretical and computational aspects of linear algebra    undergraduate level. At the same time, 1 extra hour per week\nfor engineering students. In particular, it was shown that the      will not overburden the students.\nuse of Maple facilitates the understanding of computational as-        The exact details of numerical computing content is left\npects of eigenvalues and eigenvectors. It allows students more      for individual universities and instructors. Depending on the\ntime to focus on the theoretical aspects and the underlying         syllabus and course learning outcomes, the numerical com-\nmathematical principles.                                            puting labs will be different for each university and instructor.\n   In a recent study by Mezhennaya and Pugachev [14], the           Nevertheless, the general ideas will be broadly similar across\nauthors compared engineering students\u2019 perceptions regarding        different curricula. To illustrate the proposed numerical com-\nseveral mathematical software: Matlab, Mathematica and Ex-          puting content, we will focus on the three main concept in\ncel. The study found that all the scientific packages considered    calculus: limits, derivatives, and integrals.\ncan be used in education, under the condition that the policies\nfor software usage are carefully implemented. The study             A. Limits\nfound that many students lack hands-on experience on how               Limit is a fundamental concept in calculus. Students are\nto use the software. The students particularly struggled with       usually taught to calculate limits using analytical approaches.\nMatlab and Mathematica finding them non user friendly. The          Although analytical approaches work well, there is no single\nstudy concluded that additional classes are required to prepare     universal rule for calculating limits. On the other hand, in most\nstudents to use software in their courses.                          cases, limits can be calculated numerically using essentially\n                                                                    the same approach. To illustrate, suppose we want to calculate\n      IV. N UMERICAL COMPUTING IN MATHEMATICS                       limx\u2192a+ f (x). Then we can loop for k = 0 to n and calculate\n                         CURRICULUM                                 f (a+ 10\u2212k ). As k increases, a+ 10\u2212k approaches a, so f (a+\n   Mathematics lies at the foundation of science and engineer-      10\u2212k ) will, in most cases, approach the limit value. We can\ning. The importance of mathematics courses in engineering           deduce the limit by observing the values of f (a + 10\u2212k ) or\neducation cannot be underestimated. These courses equip stu-        determine that the limit does not exist if there is no pattern of\ndents with the fundamental skills and knowledge to study the        convergence. The value of n can be chosen manually or using\nmore specialized engineering courses. Thus, student success in      a stopping criterion. For instance, the algorithm may continue\nengineering studies depends directly on the mathematics and         to iterate until the difference between consecutive values of\nsciences courses. Given the significance of the mathematics         f (a + 10\u2212k ) is below a certain threshold. The value of the\ncourses in the engineering curriculum, it is paramount to           limit can also be deduced automatically based on the values\nensure their currency with respect to the Industry 4.0.             of f (a + 10\u2212k ) using various heuristics.\n   The technological advances over the last decade have                Another common limit problem is limx\u2192\u221e f (x). In this\ncreated demand for more computationally proficient experts.         case, we can loop for k = 0 to n and calculate f (10k ). As k\nTo meet this demand, numerical computing must become a              increases, 10k approaches \u221e, so f (10k ) will, in most cases,\ncore part of engineering studies. Mathematics courses offer         approach the limit value. Then the limit can be determined\na natural and convenient avenue for introducing numerical           based on the values of f (10k ). Various extensions and cus-\ncomputing to engineering students. There are two main fac-          tomizations of this basic approach can be made. For instance,\ntors that make mathematics courses particularly amenable            to avoid issues with periodic functions f (10k + \u01ebk ), where \u01ebk\nto numerical computing. First, in many cases mathematical           are randomly generated, can be used. Other values than 10k\nproblems have numerical solutions. For instance, finding the        can also be used as long as the sequence approaches infinity.\nroot of a polynomial or the minimum value of a function             A degree of automization can be introduced using different\ncan be done numerically. Therefore, it is both logical and          heuristics.\n\f                                                                                                        Pn\nB. Derivative                                                          right Riemann sum is defined as \u03b4 k=1 f (xk ). The Riemann\n   Derivative is arguably the most important concept in cal-           sum provides an approximation of the integral. Indeed,\nculus. There exist several rules such as the power rule, the                          Z b                          n\n                                                                                                                   X\nproduct rule, the chain rule, and others to find the derivative                             f (x) dx = lim \u03b4             f (xk ).    (3)\n                                                                                                            n\u2192\u221e\nof a function by hand. However, manual differentiation may                              a                          k=1\nbe cumbersome when dealing with complex function. On the\nother hand, calculating the derivative at a point numerically is       The Riemann sums can be quickly calculated on a computer\nrelatively straightforward. To illustrate, suppose that we want        providing a simple, yet effective approach to calculating\nto calculate f \u2032 (a). Recall that                                      integrals numerically. Other popular integral approximation\n                                                                       methods include the trapezoid rule and the Simpson\u2019s rule.\n                              f (a + h) \u2212 f (a)\n                f \u2032 (a) = lim                   .               (1)\n                          x\u2192a         h                                D. Additional considerations\nTherefore, to calculate f \u2032 (a) numerically we use the same\n                                                                          The above discussion about numerical methods for cal-\napproach as with the limits. In particular, we can loop for k =\n                                                                       culating limits, derivatives, and integrals is easily extended\n0 to n and calculate f (a+1010\u2212k)\u2212f (a) . Then the limit, and by\n                              \u2212k\n\n                                                                       to multivariate calculus. For instance, to find the partial\nextension the derivative, can be deduced (approximated) based          derivative fx (a, b), we can loop for k = 0 to n and calculate\non the calculated values. The accuracy of the approximation            f (a+10\u2212k ,b)\u2212f (a,b)\n                                                                               10\u2212k          . Many problems related to sequences and\ndepends in large part on the value of n.\n                                                                       series can similarly be solved using numerical techniques. In\n   One of the most important applications of the derivative is\n                                                                       particular, the convergence of a series can be deduced from its\nfinding the extreme values of a function. Traditionally, this is\n                                                                       partial sums. By calculating the partial sums on the computer\ndone by first finding the critical points of the function and\n                                                                       and observing the results, we can intuit the nature of the series.\nthen applying the second derivative test. However, finding the\ncritical points is not always possible, so numerical approaches           Vectorization is an important aspect of numerical com-\ncan be used in such cases. The most popular numerical                  puting. Since the modern computer chips are optimized for\napproach for finding the extreme values is based on the                matrix multiplication, it is more efficient to employ vector\ngradient descent (ascent) algorithm. In gradient descent, the          operations. In particular, some algorithms based on for-loops\noptimal value of x is iteratively updated based on the gradient.       can be converted into vector operations resulting in higher\nIn particular, for k = 0 to n, the updated optimal value of x          efficiency and speed. For instance, the Riemann sum can be\nis given by                                                            calculated with a single vector operation:\n                     xk+1 = xk \u2212 \u03b1\u2207f (xk ),                   (2)                               n\n                                                                                                X                       \u0001\n                                                                                            \u03b4         f (xk ) = \u03b4S f (x) ,           (4)\nwhere \u2207f (x) is the gradient and \u03b1 is the step size. In the                                     k=1\ncase of a single-variable function, the gradient equals simply\nto the derivative \u2207f (x) = f \u2032 (x). The step size \u03b1 can be             where x = [x1 , ..., xn ] is the vector of endpoints, f (x) is\neither fixed or dynamic. While a large value of \u03b1 accelerates          a vectorized function operation, and S is the vector func-\nthe convergence at the beginning, it may hurt the convergence          tion which returns the sum of all the coordinates. Similarly,\nin the region near the optimal value.                                  limit calculations can be vectorized and made more efficient.\n   There exist several extensions of the basic gradient descent        Vectorization is also useful in multi-variate calculus, where\nalgorithm. One such extension is gradient descent with mo-             operations can be performed on a vector of variables.\nmentum which uses the second derivative to anticipate the                 The choice of the programming language for numerical\nlocation of the next optimal point and thus accelerates the            computing requires careful consideration. There are several\nconvergence.                                                           suitable candidates for this purpose including Python, Java,\n                                                                       C++, Matlab, and others. Based on our experience with\nC. Integrals                                                           different programming languages, we recommend the use of\n   Integration is an important concept in engineering mathe-           Python. Python is currently the most popular programming\nmatics. Although there exist a number of rules for finding the         language on the planet. It has a simple and intuitive syntax\nintegral, it is significantly more challenging than differentia-       making it easy to learn and apply. Python has libraries to\ntion. Moreover, in many cases, the indefinite integral does not        fit any purpose including an extensive collection of libraries\neven exist. Therefore, numerical approaches are particularly           related to numerical computing. The basic Python libraries\nuseful for integration.                                                related to computing are NumPy, SciPy, and SymPy. More\n   To illustrate the application Rof numerical integration, sup-       advanced packages such as OR-Tools are also available for\n                                   b\npose that we want to calculate a f (x) dx. There exist several         optimization tasks. Since Python is a universal programming\nnumerical methods for calculating the integral. One simple             language, it can be used for almost any task. Thus, students\nmethod is based on the Riemann sums. The interval [a, b] is            who learn Python in their mathematics courses can employ\ndivided into n equal subintervals with endpoints x0 , x1 , ..., xn .   it in their other courses. In addition, numerical computing\nLet \u03b4 = xk+1 \u2212 xk be the length of each subinterval. Then the          implemented in Python can be connected to other applications.\n\f                        V. C ONCLUSION                                             [14] Mezhennaya, N. M. & Pugachev, O. V. (2019) On perception of\n                                                                                        computer algebra systems and microsoft excel by engineering students.\n   Although the classical approach to teaching mathematics                              Problems on Education in the 21st Century, 77(3), 379-395.\nis still relevant for certain student cohorts, it is outdated for                  [15] Miranda, J., Navarrete, C., Noguez, J., Molina-Espinosa, J. M., Ram\u0131\u0301rez-\nengineering students. Modern engineering is increasingly re-                            Montoya, M. S., Navarro-Tuch, S. A., ... & Molina, A. (2021). The core\n                                                                                        components of education 4.0 in higher education: Three case studies in\nliant on computing [9], [22]. Therefore, universities must equip                        engineering education. Computers & Electrical Engineering, 93, 107278.\nthe student with appropriate computing skills. In particular,                      [16] Moore, K., & Frazier, R. S. (2017). Engineering education for generation\nmathematics courses must be revised to include numerical                                Z. American Journal of Engineering Education (AJEE), 8(2), 111-126.\n                                                                                   [17] Ortiz-Rojas, M., Chiluiza, K., & Valcke, M. (2019). Gamification\ncomputing content.                                                                      through leaderboards: An empirical study in engineering education.\n   Given the efficiency of computer-based calculations, numer-                          Computer Applications in Engineering Education, 27(4), 777-788.\nical computing provides a convenient approach to problem                           [18] Puhak, R. I. (2011). Teaching applied Calculus utilizing MATLAB,\n                                                                                        Proceedings of the Twenty-Third Annual International Conference on\nsolving in engineering mathematics. It can be integrated into                           Technology in Collegiate Mathematics, ISBN 0-321-68984-4, Copyright\nthe existing curriculum with little hassle and cost. In this paper,                     (C) 2012 by Pearson Education, Inc.\nwe proposed a framework for integrating numerical computing                        [19] Ramirez-Mendoza, R. A., Morales-Menendez, R., Iqbal, H., & Parra-\n                                                                                        Saldivar, R. (2018, April). Engineering Education 4.0:\u2014proposal for a\ninto the existing mathematics curriculum. We demonstrated                               new Curricula. In 2018 IEEE Global Engineering Education Conference\nhow numerical approaches can be used some of the most                                   (EDUCON) (pp. 1273-1282). IEEE.\ncommon problems encountered in calculus. The proposed                              [20] Salah, B., Abidi, M. H., Mian, S. H., Krid, M., Alkhalefah, H., &\n                                                                                        Abdo, A. (2019). Virtual reality-based engineering education to enhance\nframework can be customized by individual universities to fit                           manufacturing sustainability in industry 4.0. Sustainability, 11(5), 1477.\ntheir special needs.                                                               [21] Strayhorn, T. L. (2006). College in the information age: Gains associated\n                                                                                        with students\u2019 use of technology. Journal of Interactive Online Learning,\n                               R EFERENCES                                              5(2), 143-155.\n [1] Abdul Majid, M., Huneiti, Z., Balachandran, W., & Al-Naafa, M. A.             [22] Thabtah, F., Kamalov, F., & Rajab, K. (2018). A new computational\n     (2012) A study of the effects of using MATLAB as a pedagogical tool                intelligence approach to detect autistic features for autism screening.\n     for engineering mathematics students. 15th International Conference on             International journal of medical informatics, 117, 112-124.\n     Interactive Collaborative Learning (ICL), 10.1109/ICL.2012.6402183,           [23] Van den Beemt, A., MacLeod, M., Van der Veen, J., Van de Ven,\n     Villach, Austria.                                                                  A., van Baalen, S., Klaassen, R., & Boon, M. (2020). Interdisciplinary\n [2] Abdul Majid, M., Huneiti, Z. A., Balachandran, W., & Balarabe, Y.                  engineering education: A review of vision, teaching, and support. Journal\n     (2013) MATLAB as a teaching and learning tool for Mathematics: A                   of engineering education, 109(3), 508-555.\n     literature review. International Journal of Arts and Sciences, 6(3), 23-44.\n [3] Brake, M. L. (2007) MATLAB as a Tool to Increase the Math Self-\n     Confidence and the Math Ability of First-Year Engineering Technology\n     Students. The Scholarship of Teaching and Learning at EMU: Vol. 1,\n     Article 5.\n [4] Chen, J., Kolmos, A., & Du, X. (2021). Forms of implementation and\n     challenges of PBL in engineering education: a review of literature.\n     European Journal of Engineering Education, 46(1), 90-115.\n [5] Cretchley, P., Harman, C., Ellerton, N., & Fogarty, G. (2000) MATLAB\n     in Early Undergraduate Mathematics: An investigation into the Effects\n     of Scientific Software on Learning. Mathematics Education Research\n     Journal, 12(3), 219-233.\n [6] Hadgraft, R. G., & Kolmos, A. (2020). Emerging learning environments\n     in engineering education. Australasian Journal of Engineering Educa-\n     tion, 25(1), 3-16.\n [7] Hernandez-de-Menendez, M., & Morales-Menendez, R. (2019). Tech-\n     nological innovations and practices in engineering education: a review.\n     International Journal on Interactive Design and Manufacturing (IJIDeM),\n     13(2), 713-728.\n [8] Herna\u0301ndez-de-Mene\u0301ndez, M., Guevara, A. V., Mart\u0131\u0301nez, J. C. T.,\n     Alca\u0301ntara, D. H., & Morales-Menendez, R. (2019). Active learning in\n     engineering education. A review of fundamentals, best practices and ex-\n     periences. International Journal on Interactive Design and Manufacturing\n     (IJIDeM), 13(3), 909-922.\n [9] Kamalov, F., Moussa, S., Zgheib, R., & Mashaal, O. (2020, December).\n     Feature selection for intrusion detection systems. In 2020 13th Interna-\n     tional Symposium on Computational Intelligence and Design (ISCID)\n     (pp. 265-269). IEEE.\n[10] Kamalov, F., Sulieman, H., & Santandreu Calonge, D. (2021). Machine\n     learning based approach to exam cheating detection. Plos one, 16(8),\n     e0254340.\n[11] Kilicman, A., Hassan, M. A., & Said Husain, S. K. (2010) Teaching and\n     Learning using Mathematics Software \u2019The New Challenge\u2019. Procedia\n     Social and Behavioral Sciences, 8, 613-619.\n[12] Lorenz, M., Ru\u0308\u00dfmann, M., Strack, R., Lueth, K. L., & Bolle, M. (2015).\n     Man and machine in industry 4.0: How will technology transform the\n     industrial workforce through 2025. The Boston Consulting Group, 2.\n[13] Maur\u0131\u0301cio, R. D. A., Veado, L., Moreira, R. T., Figueiredo, E., & Costa,\n     H. (2018). A systematic mapping study on game-related methods for\n     software engineering education. Information and software technology,\n     95, 201-218.\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": [" Abdul Majid, M., Huneiti, Z., Balachandran, W., & Al-Naa fa, M. A. (2012) A study of the effects of using MATLAB as a pedagogical tool for engineering mathematics students. 15th International Conference on Interactive Collaborative Learning (ICL), 10.1109/ICL.2 012.6402183, Villach, Austria. ", " Abdul Majid, M., Huneiti, Z. A., Balachandran, W., & Bala rabe, Y . (2013) MATLAB as a teaching and learning tool for Mathematic s: A literature review. International Journal of Arts and Scien ces, 6(3), 23-44. ", " Brake, M. L. (2007) MATLAB as a Tool to Increase the Math Se lf- Con\ufb01dence and the Math Ability of First-Year Engineering Te chnology Students. The Scholarship of Teaching and Learning at EMU: V ol. 1, Article 5."], "publish_date": "2022-07-11", "approved": true}}, {"_index": "articles", "_id": "KOtrCI0BUzn7PtN-CYsM", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "Fuzzy logic based MPPT control for a PV system using SEPIC converter", "abstract": "In this study, a novel single-ended primary inductor (SEPIC) converter-based fuzzy logic controller for maximum power point tracking is presented. By adding rules to the perturb and observing search strategy, the new controller enhances it while fuzzifying and removing its flaws. When compared to traditional maximum power point tracking techniques, fuzzy logic trackers enable an accurate and quick convergence to maximum power point under both steady-state and variable weather situations. The performance of the proposed maximum power point tracker is demonstrated in simulation.", "authors": ["Moulay Abdellah Alhachemi", "Mohammed Habbab", "Abdeldjebar Hazzab"], "institutions": ["Laboratory of CAOSEE", "Laboratoire de CAOSEE", "\u00c9cole de Technologie Sup\u00e9rieure", "Universit\u00e9 Tahri Mohamed", "ETS", "1100 Notre-dame", "Bechar, Algeria", "Montr\u00e9al, Canada"], "keywords": ["Fuzzy logic controller (FLC)", "Maximum power point tracker (MPPT)", "Photovoltaic (PV)"], "full_text": "     Fuzzy logic based MPPT control for a PV system\n                  using SEPIC converter\n       Moulay Abdellah Alhachemi                        Mohammed Habbab                             Abdeldjebar Hazzab\n         Laboratory of CAOSEE                         Laboratoire de CAOSEE                   \u00c9cole de Technologie Sup\u00e9rieure\n        Universit\u00e9 Tahri Mohamed                     Universit\u00e9 Tahri Mohamed                     (ETS) 1100 Notre-dame\n             Bechar, Algeria                              Bechar, Algeria                            Montr\u00e9al, Canada\n   alhachemi.abdellah@univ-bechar.dz              habbab.mohamed@univ-bechar.dz                Hazzab.abdeldjebar@etsmtl.ca\n\n            Mansour Becahr                                 Hicham Gouabi\n         Laboratory of CAOSEE                      \u00c9cole de Technologie Sup\u00e9rieure\n        Universit\u00e9 Tahri Mohamed                       (ETS) 1100 Notre-dame\n             Bechar, Algeria                              Montr\u00e9al, Canada\n     bechar.mensour@univ-bechar.dz                 hicham.gouabi.1@ens.etsmtl.ca\n\n\n  Abstract\u2014In this study, a novel single-ended primary inductor       arrangement exchange; for that reason, the two types of\n(SEPIC) converter-based fuzzy logic controller for maximum            converters should be excluded from maximum power\npower point tracking is presented. By adding rules to the perturb     applications. The boost converter has a nonstop input current,\nand observing search strategy, the new controller enhances it while   but the output voltage is always bigger than the input, which\nfuzzifying and removing its flaws. When compared to traditional\n                                                                      may not accomplish maximum power exchange operation in a\nmaximum power point tracking techniques, fuzzy logic trackers\n                                                                      few cases, such as when the maximum voltage is less than the\nenable an accurate and quick convergence to maximum power\npoint under both steady-state and variable weather situations. The    input [3].\nperformance of the proposed maximum power point tracker is\ndemonstrated in simulation.                                              This paper presents a fuzzy-based P&O strategy for an MPPT\n                                                                      standalone PV system. The proposed MPPT can abuse the\nKeywords: Fuzzy logic controller (FLC), Maximum power point           preferences of the P&O strategy and eliminate its drawbacks.\ntracker (MPPT), Photovoltaic (PV)                                     Output has been separated into five fuzzy subsets. As the\n                                                                      proposed strategy continuously exchanges maximum power\n                      I. INTRODUCTION                                 from PV arrays, it optimizes the number of PV modules.\n   The non-renewable energy sources are rapidly running out,\nwhile the electricity demand is increasing daily. To solve this                     II. MODELIGN OF PV SYSTEM\nproblem, efficient and efficient electric power generation from          Photovoltaic is the technique and study connected to devices\nrenewable energy sources is required [1]. Renewable energy is         that directly convert sunlight into electricity utilizing\none of the forms of energy that society can rely on because It        photovoltaic semiconductors. Direct conversion of solar energy\nis unpolluted, pure, and has no limits. One type of power             into DC electrical energy can be achieved by photovoltaic cells\ngeneration that uses renewable energy is the photovoltaic (PV)        [4]. The photovoltaic panel is made up of numerous cells that\nsystem [2]. To utilize less conventional energy, the PV system        are connected in series Ns or shunt Nsh. Where it may be\nmust subsequently be linked to the grid, either directly or via a     mimicked by a current source coupled in parallel with a diode\nbackup battery bank. Since the power produced by PV                   as described by and depicted in Figure 1 [5].\nsystems depends on radiation and temperature change, the PV\nframework has destitute productivity, [2].\n\n  For the control of the PV systems, there are different sorts of\nDC-DC converters such as Buck converters, Boost converters,\nand Buck-Boost converters. Due to its output pick-up\nadaptability, a single-ended primary-inductor converter\n(SEPIC) acts as a buck-boost DC/DC converter, where it\nchanges its output voltage agreeing to its duty cycle. Unlike the\ncustomary buck-boost converter, the SEPIC converter includes\na non-inverted output and it uses an arrangement capacitor to\n                                                                                       Fig. 1. Circuit for a PV array.\nseparate input from output [3]. The buck and buck-boost\nconverters lose half of their input control due to input current\n\fThe following equations provide the output current:                       the switch is on, the first inductor, L1 is charged from the input\nI = Iph \u2212 ID                                                       (1)    voltage source. The second inductor L2 absorbs energy from\n                                                                          the first capacitor C1, leaving the output capacitor C2 to supply\n                       q(V+Rs I)              V+Rs I\nI = Iph \u2212 I0 [exp (                ) \u2212 1] \u2212                        (2)    the load current.\n                        AKB T                  Rsh\n\n\nIph \u2236 Photo \u2212 current                           A \u2236 Ideality factor\nK B \u2236 Constant of Boltzmann                      T \u2236 Cell temperature\nID \u2236 Current via a diode                        R s \u2236 Series resistance\nI0 \u2236 Current of Saturation                      q \u2236 Electronic charge\nR sh \u2236 Shunt resistance                         V \u2236 Cell voltage\nI \u2236 Current of cell\n                                                                                               Fig. 3.a. The switch turned on.\n  The shunt resistance (R sh ) is typically orders of magnitude\nlarger than the series resistance (R s ) [6]. Therefore, it is\ncommon for the shunt and series resistances of a solar cell can\nbe neglected to simplify the model. The resulting ideal voltage-\ncurrent characteristic of the solar cell is given by equation (3).\n                  qV\nI = Iph \u2212 I0 [e(KT) \u2212 1]                                            (3)\n\n                    III. SEPIC CONVERTER                                                 Fig. 3.b. The switch turned off.\n   Power electronics researchers are working hard to create DC-             Fig. 3. Equivalent circuit diagram of the SEPIC converter\nDC converters with simpler designs and greater efficiency [7].                            when the switch is on and off.\nTo maintain a constant output voltage, the suggested DC-to-DC\nconverter employs a single-ended primary-inductor converter                 When the switch is turned on, the input inductor is charged\n(SEPIC) architecture. The SEPIC converter is made up of a duty            from the source, and the second inductor is charged from the\ncycle switch S, a diode, two inductors (L1 and L2), two                   first capacitor. No energy is supplied to the load capacitor\ncapacitors (C1 and C2), and a load resistor. Figure 2 depicts the         during this time. The inductor current and capacitor voltage\ncircuit diagram of a SEPIC converter. A SEPIC is a DC-DC                  polarities are marked in this Figure. When the power switch is\nconverter [8]. SEPIC are DC-DC converters that can output                 turned off, the energy stored in the inductor is transferred. The\nvoltages that are B, larger than, or equal to the input voltage.          energy stored is transferred through the diode and supplies the\nThe duty cycle of the control transistor affects the SEPIC                energy to the load [10], as shown in Figure 3. b. The second\nconverter's output voltage. The SEPIC converter is two                    inductor is also connected to the load during this time. The\nconverters in one: a boost converter followed by a buck-boost             output capacitor sees a pulse of current during the off time,\nconverter. It has the advantages of having a non-inverted output          making it inherently noisier than a buck converter. The amount\n(the output voltage has the same polarity as the input voltage) ,         that the SEPIC converters increase or decrease the voltage\nusing a series capacitor to couple energy from the input to the           depends primarily on the duty cycle and the parasitic elements\noutput (which makes it more responsive to short-circuits), and            in the circuit. The output of an ideal SEPIC converter is:\n                                                                                   D\nbeing able to shut down completely: when the switch \"S\" is                Vout =         Vin                                            (4)\n                                                                                   1\u2212D\nturned off, the output voltage drops to 0 V, accompanied by a\nsignificant transient discharge of charge.\n                                                                            A SEPIC converter is to process the electricity from the PV\n                                                                          system. This converter either increases or decreases the PV\n                                                                          system voltage at the load. The proposed SEPIC converter\n                                                                          operates in buck mode.\n\n                                                                                           IV. FUZZY LOGIC CONTROL\n                                                                             In the fuzzy logic maximum power point tracking (MPPT)\n                                                                          algorithm, the voltage and current at each instant k are\n      Fig. 2. Simple circuit diagram of the SEPIC converter               measured to calculate the active power. The active power is\n                                                                          then compared with the power at the previous instant (k-1) to\nFigure 3 depicts the circuit when the power switch is switched            obtain the change in power (\u0394P(k)). Similarly, the voltage at\non and off (in Figures a and b, respectively). Figure 3a When             instant k is compared with the voltage at instant k-1 to obtain\n\fthe voltage error (\u0394V(k)) [11]. The power error is then divided\nby the current error to obtain the error (E). The error is then     Table. 1. Fuzzy logic rules for the push-pull converter. NB,\ncompared with the previous error to calculate the change in        negative big; NS, negative small; ZE, zero; PS, positive small;\nerror (\u0394E(k)). The error (E(k)) and the change in error (\u0394E(k))                          PB, positive big.\nare then used as the crisp inputs to the fuzzy logic controller.    \u2206Vpv*[o/p]                      \u2206Vpv*[i/p]\nThe flow chart for the fuzzy logic MPPT algorithm is shown in\n                                                                                            NB       NS      ZE      PS     PB\nFigure 4. In this work, the Mamdani inference technique, A-\ntype membership functions, and a 25-element rule base were                          NB       PS      NB      NB      NB     NS\nused for the fuzzy logic control. The Mamdani inference\ntechnique is efficient and straightforward in defining the fuzzy    \u2206Ppv*[i/p]      NS       PS      PS      NS      NS     NS\noutput sets, and it is more popular among researchers than other\n                                                                                    ZE      ZE       ZE      ZE      ZE     ZE\ninference techniques [12]. The A-type or triangular\nmembership function is used because it is simpler to split into                     PS      NS       NS      PS      PS     PS\nlow and high membership functions (MFs) than other\nmembership functions. Additionally, it has been observed that                       PB      NS       NB      PB      PB     PS\nthe triangular membership function has a faster response and\nless overshoot than other functions [13]. A 25-element rule base\nwas used because it has been shown to perform well [14][15].\n\n\n\n\n Fig. 4. Fuzzy logic maximum power point tracking (MPPT)\n                         flow chart.\n\n   The following are the fuzzy rules in Table 1, which are used\nfor the desired MPP of push-pull converter PWM. The\nmembership for input variables (DPpv, DVpv) are shown in\nFigure 5, and the membership for output variable (DVpv*) is\nshown in Figure 6. All the functions are defined on a normalized\ninterval [-1 1].                                                              Fig. 5. Membership for input variables.\n\f                                                                    The results are provided under standard test conditions;\n                                                                   G=1000 W/m2; T=25\u00b0C and it is shown in figure 7.\n\n\n\n\n           Fig. 6. Membership for output variable.                                      Fig. 7.a. Current.\n\n                 V. SIMULATION RESULTS\n The characteristics of the photovoltaic array that we use in\nthis paper are given in Table 2.\n\n\n        Table. 2. Electrical data of photovoltaic array.\n\n            Variable                          Value\n       Maximum Power                     Pmax = 200 W\n     Open circuit voltage                 Voc = 36.1 V\n      Short circuit current               Isc = 200 A\n  Voltage at maximum power               Vmax = 29.3 V\n  Current at maximum power                Imax = 6.9 A\n                                                                                        Fig. 7.b. Voltage.\n  Table 3 shows the SEPIC converter settings utilized in this\nstudy. The SEPIC converter is linked to the PV panel in the full\nmodel, and the duty cycle is regulated by the Fuzzy Logic\nController.\n\n        Table. 3. The SEPIC converter parameters.\n          Variable                        Value\n     Switching frequency                 20 KHZ\n\n        Load resistance                  R Load = 4.29 \u2126\n\n         Inductance L1                    L1 = 3.6 MH\n\n         Inductance L2                    L2 = 0.9 MH\n\n         Capacitor C1                      C1 = 180 \u00b5F\n                                                                                          Fig. 7.c. Power.\n         Capacitor C2                     C2 = 6000 \u00b5F             Fig. 7. Voltage, current, and power output results; at G=1000\n                                                                                        W/m2 and T=25\u00b0C.\n\f Overall, using MPPT and fuzzy logic to a SEPIC converter for                         the 2009 IEEE 6th International Power Electronics and Motion Control\n                                                                                      Conference, Wuhan, China, 17-20 May 2009. pp. 2155-2160.\na PV system result in considerable performance gains.\n                                                                                 [12] C. Wang. \u201cA Study of Membership Functions on Mamdani-Type Fuzzy\nIncreased power output, higher efficiency, decreased ripple,\n                                                                                      Inference System for Industrial Decision-Making.\u201d Master's thesis,\nenhanced transient response, and resilience are examples of\n                                                                                      Lehigh University, Bethlehem, PA, USA, 2015.\nthese enhancements.\n                                                                                 [13] MA. Usta, \u00d6. Akyazi, and \u0130H. Alta\u015f. \u201cDesign and performance of solar\n                     \u2165. CONCLUSIONS                                                   tracking system with fuzzy logic controller used different membership\n   This paper presents the design of an off-grid photovoltaic                         functions.\u201d In Proceedings of the 2011 7th International Conference on\nsystem with a fuzzy logic MPPT-controlled push-pull boost                             Electrical and Electronics Engineering (ELECO), Bursa, Turkey, 1-4\nconverter. The proposed system was simulated in                                       December 2011. pp. II-381-II-385.Mudi, R.K.; Pal, N.R. A robust self-\n                                                                                      tuning scheme for PI-and PD-type fuzzy controllers. IEEE Trans. Fuzzy\nMATLAB/Simulink and tested under various weather\n                                                                                      Syst. 1999, 7, 2\u201316.\nconditions. The results showed that the fuzzy logic algorithm\n                                                                                 [14] RK. Mudi, and NR. Pal. \u201cA robust self-tuning scheme for PI- and PD-\noutperformed the conventional algorithms in terms of MPPT\n                                                                                      type fuzzy controllers.\u201d IEEE Transactions on Fuzzy Systems 7.1 (1999):\naccuracy and minimization of fluctuations, regardless of rapid                        2-16.\nchanges in irradiance.                                                           [15] A. Shehata, H. Metered, and WAH. Oraby. \u201cVibration control of active\n                                                                                      vehicle suspension system using fuzzy logic controller.\u201d In Vibration\n                               REFERENCES                                             Engineering and Technology of Machinery, edited by W. A. Oraby, 389-\n                                                                                      399. Springer, Berlin, Heidelberg, 2015.\n[1]   G. Mahendran, and KV. Kandaswamy. \u201cAnt Colony Optimized Tuned\n      DC-DC converter.\u201d International Journal of Computer Applications\n      (0975\u20138887) 108.11 (2013): 45-50.\n[2]   J. Dunia, and BMM Mwinyiwiwa. \u201cPerformance Comparison between\n      \u0106UK and SEPIC Converters for Maximum Power Point Tracking Using\n      Incremental Conductance Technique in Solar Power Applications.\u201d\n      International Journal of Electrical, Computer, Energetic, Electronic and\n      Communication Engineering 7.12 (2013): 2510-2517.\n[3]   M. Cirrincione, M. Pucci, and G. Vitale, \u201cGrowing Neural Gas (GNG)-\n      Based Maximum Power Point Tracking for High-Performance Wind\n      Generator With an Induction Machine.\u201d IEEE Transactions on Industry\n      Applications 47.2 (2011): 861-872.\n[4]   M. Abdulkadir, AS. Samosir, S., and AHN. Yatim. \u201cModeling and\n      Simulation of a Solar Photovoltaic System, Its Dynamics and Transient\n      Characteristics in LABVIEW.\" International Journal of Power Electronics\n      and Drive System (IJPEDS) 3.2 (2013): 185-192.\n[5]   H. Bouzeria, C. Fetha, T. Bahi, I. Abadlia, Z. Layate, and S. Lekhchine.\n      \u201cFuzzy Logic Space Vector Direct Torque Control of PMSM for\n      Photovoltaic Water Pumping System.\u201d Energy Procedia 74 (2015): 760-\n      771.\n[6]   YM. Chen, YC. Liu, SC. Hung, and CS. Cheng. \u201cMulti-Input Inverter for\n      Grid-Connected Hybrid PV/Wind Power System.\u201d IEEE Transactions on\n      Power Electronics 22.3 (2007): 742-750.\n[7]   S. Ganesh, J. Janani, and GB. Angel. \u201cA Maximum Power Point Tracker\n      for PV Panels Using SEPIC Converter.\u201dInternational Journal of\n      Electrical, Computer, Energetic, Electronic and Communication\n      Engineering 8.2 (2014): 637-642.\n[8]   R. Vijayabalan, and S. Ravivarman. \u201cZ Source Inverter for Photovoltaic\n      System with Fuzzy Logic Controller.\u201d International Journal of Power\n      Electronics and Drive System (IJPEDS) 2.4 (2012): 371-379.\n[9]   A. Ramkumar, and SVS. Florence. \"Analysis of Single Phase AC-DC\n     SEPIC Converter using Closed Loop Techniques.\u201d International Journal\n     of Advanced Research in Electrical, Electronics and Instrumentation\n     Engineering 4.2 (2015): 193-201.\n[10] G. Tadi, and P. Ramamurthyraju. \u201cAnalysis of SEPIC for PV-\n     Applications using PI Controller and Current Mode Control.\"\n     International Journal for Scientific Research & Development 1.9 (2013):\n     175-180.\n[11] J. Li, and H. Wang. \u201cA novel stand-alone PV generation system based on\n     variable step size INC MPPT and SVPWM control.\" In Proceedings of\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": [" G. Mahendran, and KV. Kandaswamy. \u201cAnt Colony Optimized Tuned DC-DC converter.\u201d International Journal of Computer Applications (0975 \u20138887) 108.11 (2013): 45-50.", "J. Dunia, and BMM Mwinyiwiwa. \u201cPerformance Comparison between \u0106UK and SEPIC Converters for Maximum Power Point Tracking Using Incremental Conductance Technique in Solar Power Applications.\u201d International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering 7.12 (2013): 2510-2517.", "M. Cirrincione, M. Pucci, and G. Vitale, \u201cGrowing Neural Gas (GNG)-Based Maximum Power Point Tracking for High-Performance Wind Generator With an Induction Machine.\u201d IEEE Transactions on Industry Applications 47.2 (2011): 861-872."], "publish_date": "2020-05-23", "approved": true}}, {"_index": "articles", "_id": "KetsCI0BUzn7PtN-xIvV", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "The Programmer\u2019s Assistant: Conversational Interaction with a Large Language Model for Software Development", "abstract": "Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model\u2019s responses. We developed a prototype system \u2013 the Programmer\u2019s Assistant \u2013 in order to explore the utility of conversational interactions grounded in code, as well as software engineers\u2019 receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant\u2019s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.", "authors": ["Steven I. Ross", "Fernando Martinez", "Stephanie Houde", "Michael Muller", "Justin D. Weisz"], "institutions": ["IBM Research AI", "IBM Argentina"], "keywords": ["code-fluent large language models", "foundation models", "conversational interaction", "human-centered AI"], "full_text": "  The Programmer\u2019s Assistant: Conversational Interaction with a\n        Large Language Model for Software Development\n                   Steven I. Ross                                    Fernando Martinez                               Stephanie Houde\n                 IBM Research AI                                          IBM Argentina                               IBM Research AI\n               Cambridge, MA, USA                               La Plata, Buenos Aires, Argentina                   Cambridge, MA, USA\n             steven_ross@us.ibm.com                                   martferc@ar.ibm.com                        Stephanie.Houde@ibm.com\n\n                                               Michael Muller                                Justin D. Weisz\n                                            IBM Research AI                                 IBM Research AI\n                                          Cambridge, MA, USA                           Yorktown Heights, NY, USA\n                                       michael_muller@us.ibm.com                           jweisz@us.ibm.com\nABSTRACT                                                                           KEYWORDS\nLarge language models (LLMs) have recently been applied in soft-                   code-fluent large language models, foundation models, conversa-\nware engineering to perform tasks such as translating code between                 tional interaction, human-centered AI\nprogramming languages, generating code from natural language,                      ACM Reference Format:\nand autocompleting code as it is being written. When used within                   Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin\ndevelopment tools, these systems typically treat each model in-                    D. Weisz. 2023. The Programmer\u2019s Assistant: Conversational Interaction\nvocation independently from all previous invocations, and only a                   with a Large Language Model for Software Development. In 28th Inter-\nspecific limited functionality is exposed within the user interface.               national Conference on Intelligent User Interfaces (IUI \u201923), March 27\u201331,\nThis approach to user interaction misses an opportunity for users                  2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 24 pages. https:\nto more deeply engage with the model by having the context of                      //doi.org/10.1145/3581641.3584037\ntheir previous interactions, as well as the context of their code,\ninform the model\u2019s responses. We developed a prototype system                      1    INTRODUCTION\n\u2013 the Programmer\u2019s Assistant \u2013 in order to explore the utility of                  Software development is a highly skilled task that requires knowl-\nconversational interactions grounded in code, as well as software                  edge, focus, and creativity [27, 28]. Many techniques have been\nengineers\u2019 receptiveness to the idea of conversing with, rather than               developed to enhance the productivity of software engineers, such\ninvoking, a code-fluent LLM. Through an evaluation with 42 partici-                as advanced code repositories [86], knowledge repositories [39],\npants with varied levels of programming experience, we found that                  Q&A sites [1], and pair programming practices [18]. Collaborative\nour system was capable of conducting extended, multi-turn discus-                  software engineering is especially promising, given that profes-\nsions, and that it enabled additional knowledge and capabilities                   sional software development is rarely a solo activity and relevant\nbeyond code generation to emerge from the LLM. Despite skepti-                     knowledge and expertise are typically distributed widely within an\ncal initial expectations for conversational programming assistance,                organization [68]. Many efforts have focused on incorporating col-\nparticipants were impressed by the breadth of the assistant\u2019s capa-                laborative technologies into software development environments\nbilities, the quality of its responses, and its potential for improving            (e.g. [8, 25, 26, 58, 101]).\ntheir productivity. Our work demonstrates the unique potential of                      The pioneering work of Rich and Waters on The Programmer\u2019s\nconversational interactions with LLMs for co-creative processes                    Apprentice [70] presented a novel concept of a knowledgeable auto-\nlike software development.                                                         mated assistant \u2013 in effect, an artificial collaborative partner \u2013 that\n                                                                                   could help software engineers with writing code, designing soft-\nCCS CONCEPTS                                                                       ware systems, and creating requirements specifications. At the time,\n                                                                                   AI technologies and computing resources were not sufficient to\n\u2022 Human-centered computing \u2192 HCI theory, concepts and\n                                                                                   fully implement their vision. In the intervening years, an increase in\nmodels; \u2022 Software and its engineering \u2192 Designing software;\n                                                                                   computational power, the availability of large corpora of language\n\u2022 Computing methodologies \u2192 Generative and developmental\n                                                                                   and code data, and the development of deep neural networks have\napproaches.\n                                                                                   made new approaches to achieving their goals worth exploring.\n                                                                                       Recently, models leveraging the transformer architecture [96]\n                                                                                   have been developed to perform domain-specific software engi-\n                                                                                   neering tasks, such as translating code between languages [75],\n                                                                                   generating documentation for code [36, 38, 97, 98], and generating\nThis work is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivs International 4.0 License.                      unit tests for code [92] (see Talamadupula [90] and Allamanis et al.\n                                                                                   [5] for surveys). Recently developed foundation models \u2013 large\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                 language models that can be adapted to multiple tasks and which\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0106-1/23/03.                                                  exhibit emergent behaviors for which they have not been explic-\nhttps://doi.org/10.1145/3581641.3584037                                            itly trained [14] \u2013 have also proven to be capable with source code.\n\n\n\n\n                                                                             491\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                        Ross et al.\n\n\nWhile the intent of training LLMs such as GPT-2 [64] and GPT-3 [17]             \u2022 We motivate the need to further understand how to design\nwas to give them mastery of natural language, it quickly became                   human-centered AI systems that enhance the joint perfor-\napparent that the presence of code in their training corpora had                  mance of the human-AI collaboration.\ngiven them the ability to generate code based on natural language\ndescriptions [49]. The Codex model [24] was then produced by fine-        2     RELATED WORK\ntuning GPT-3 on a large corpus of source code data, leading to the        We discuss three areas of related work that have either motivated\ndevelopment of Copilot [32], a tool that helps software engineers by      our study of conversational programming assistance or provided\nautocompleting code as it is being written. Experimentation with          the technical foundations for it. We begin by briefly summarizing\nCopilot has shown its ability to perform additional tasks, such as        Rich and Waters\u2019 visionary work on the Programmer\u2019s Appren-\nexplaining code, generating documentation, and translating code           tice [70], followed by summarizing work on code-fluent foundation\nbetween languages [6].                                                    models and human-centered evaluations of how these models im-\n   Although autocompletion interfaces are useful and valuable             pact software engineers\u2019 work. Finally, we discuss conversational\nwhen the system can discern the developer\u2019s intent, there are many        interaction and how it might be employed to provide more flexible\ninstances where that is insufficient. For example, the developer          and sophisticated assistance to software engineers.\nmay have a good idea of what they want to do, but may be unclear\non what functions, libraries, or even algorithms to employ. They          2.1      The Programmer\u2019s Apprentice\nmay even have general programming questions that need to be\n                                                                          Our work is inspired by the vision laid out by Rich and Waters [70],\nanswered before they are able to write any code.\n                                                                          which describes an artificial agent that can act as an intelligent as-\n   In this paper, we seek to understand whether modern develop-\n                                                                          sistant for software engineers by providing advice, catching errors,\nments in code-fluent foundation models \u2013 large language models\n                                                                          and handling routine details throughout the software development\nthat have been fine-tuned on source code data \u2013 are sufficient to\n                                                                          process. The Programmer\u2019s Apprentice [70] relied on a knowledge\nsupport a conversational agent that can act as an assistant in the\n                                                                          base of \u201cclich\u00e9s,\u201d which are formal, structured versions of what\nsoftware development process. We developed the Programmer\u2019s\n                                                                          are known today as software design patterns [31]. It used a hy-\nAssistant to explore the capabilities that conversational interaction\n                                                                          brid reasoning system capable of special-purpose reasoning based\ncould enable and the extent to which users would find conversa-\n                                                                          on frames and a plan calculus, along with general purpose logical\ntional assistance with programming tasks desirable and useful.\n                                                                          reasoning. Although natural language interaction was envisioned,\n   We hypothesize that a conversational system may provide a\n                                                                          the original prototype implementation ultimately used a stylized\nflexible and natural means for interacting with a code-fluent LLM.\n                                                                          command language. We view our work as a conceptual successor\nConversational interaction could enable users to pursue their ques-\n                                                                          to the Programmer\u2019s Apprentice, as it enables the natural language\ntions in a multiple exchange dialog (as observed by Barke et al.\n                                                                          interaction that the Programmer\u2019s Apprentice lacked.\n[13]) that allows them to ask follow-up questions and refine their\ninquiries. A conversational programming assistant could ask the\nuser clarifying or disambiguating questions to help it arrive at the\n                                                                          2.2      Code-fluent Foundation Models and\nbest answer. It could also provide multiple types of assistance to                 Human-Centered Evaluations of\nthe user beyond simply generating code snippets, such as engaging                  Programming Assistance\nin general discussion of programming topics (e.g. [22, 71]) or help-      Generative models based on the transformer architecture [96] have\ning users improve their programming skills (as observed in other          recently been applied to the domain of software engineering. Code-\nstudies of automating technologies [99]).                                 fluent large language models are capable of generating code from\n   Our paper makes the following contributions to the IUI commu-          natural language descriptions [105], translating code from one lan-\nnity:                                                                     guage to another [75], generating unit tests [92], and even gener-\n                                                                          ating documentation for code [36, 38, 97, 98]. These models are\n                                                                          probabilistic systems, and as such, do not always produce perfect\n                                                                          results (e.g. code that is free of syntax or logical errors). Nonetheless,\n     \u2022 We provide empirical evidence that a conversational pro-           Weisz et al. [102] found that software engineers are still interested\n       gramming assistant based on a state-of-the-art, code-fluent        in using such models in their work, and that the imperfect outputs\n       foundation model provides valuable assistance to software          of these models can even help them produce higher-quality code\n       engineers in a myriad of ways: by answering general pro-           via human-AI collaboration [103].\n       gramming questions, by generating context-relevant code,              New tools based on code-fluent LLMs are actively being devel-\n       by enabling the model to exhibit emergent behaviors, and by        oped. GitHub Copilot1 is described as \u201cYour AI pair programmer.\u201d It\n       enabling users to ask follow-up questions that depend upon         is optimized for the code autocompletion use case: given a starting\n       their conversational and code contexts.                            snippet such as a method\u2019s documentation, signature, or partial\n     \u2022 We show how different interaction models \u2013 conversation,           implementation, Copilot completes the implementation. Copilot is\n       direct manipulation, and search \u2013 provide complementary            based on the OpenAI Codex model [24], a 12 billion parameter ver-\n       types of support to software engineers with tradeoffs be-          sion of GPT-3 [17, 49], fine-tuned on code samples from 54 million\n       tween the user\u2019s focus and attention, the relevance of sup-        public software repositories on GitHub. Empirical evaluations of\n       port to their code context, the provenance of that support,\n       and their ability to ask follow-up questions.                      1 https://github.com/features/copilot\n\n\n\n\n                                                                    492\n\fThe Programmer\u2019s Assistant                                                                                       IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\nthis model have shown that, although the quality of its outputs is              environments. PACT is structured as a discrete question-answering\nquite good, those outputs may still be problematic [57]. Echoing the            system based on a neural machine translation approach, but it\nresults from Weisz et al. [103], human-centered evaluations of Copi-            doesn\u2019t maintain a conversational context.\nlot have found that it increases users\u2019 feelings of productivity [109],\n                                                                                2.3.2 Conversation Analysis. Conversation is a form of interaction\nand that almost a third (27%) of its proposed code completions were\n                                                                                between people that enables robust communication. Conversation\naccepted by users. In a contrasting evaluation, Vaithilingam et al.\n                                                                                Analysis [76] is a method for understanding the natural structure\n[95] found that while most participants expressed a preference\n                                                                                of human conversational interaction. It catalogs different patterns\nto use Copilot in their daily work, it did not necessarily improve\n                                                                                of conversational acts and how they are utilized by interlocutors in\ntheir task completion times or success rates. Yet, in a study by\n                                                                                order to attain a wide variety of goals. Recently, Conversation Anal-\nKalliamvakou [40], developers working with Copilot were able to\n                                                                                ysis has been adapted to describe patterns of interactions between\nimplement a web server in Javascript 55% faster than developers\n                                                                                humans and artificial conversational agents in order to aid in the\nwho did not use Copilot.\n                                                                                design of chatbots [50]. We apply techniques from Conversation\n   A grounded theory analysis of how programmers interact with\n                                                                                Analysis in our study of conversational programming assistance.\nCopilot [13] found that their interactions varied depending upon\nwhether they were accelerating tasks that they already knew how\nto do or if they were exploring solutions to problems that they were\n                                                                                3    THE PROGRAMMER\u2019S ASSISTANT\nless sure about. Autocompletion was effective when developers                   In order to explore conversational programming assistance, we\nwere operating in \u201cacceleration mode\u201d and relied on the model                   created a functional prototype system called The Programmer\u2019s\nto produce short completions that could be verified quickly. In                 Assistant. Our prototype, shown in Figure 1, combines a code editor\n\u201cexploration mode,\u201d however, the interaction was more awkward.                  with a chat interface. The code editor was implemented using the\nDevelopers would communicate with Copilot by typing comments                    Microsoft Monaco Editor5 embedded in a React wrapper6 . The\nand seeing what Copilot generated in response. Then, they would                 chat user interface was implemented using the React-Chatbot-Kit7\nmodify their comments to explore other ways of prompting a re-                  framework. To drive the conversational interaction, we employed\nsponse. Ultimately, the comments used to prompt the model would                 OpenAI\u2019s Codex model [24], accessed through its web API.\nbe deleted after the relevant code was generated, indicating that                  We developed our prototype as a lightweight coding environ-\ntheir value was largely in driving a back-and-forth, yet context free,          ment in order to examine the user experience of interacting with\ndialog with the model to coerce it to produce the desired results               a conversational assistant. Our work was exploratory in nature,\nthrough an iterative refinement process. In this paper, we fully                and thus we did not have specific design goals for the prototype\ncommit to a context-aware conversational style of interaction with              beyond integrating a code editor with a code-fluent LLM. We also\na code-fluent LLM and assess the value it provides to users.                    did not attempt to target the prototype for a specific class of users\n                                                                                (e.g. novices or experts) or use cases (e.g. writing code vs. learning a\n2.3     Conversational Interaction and Analysis                                 new programming language), as we wanted any value provided by\n                                                                                conversational assistance to emerge from our user study. We also\n2.3.1 Conversational Interaction. Using natural language to inter-\n                                                                                did not implement the ability to run or debug code in our prototype\nact with technology has had a long research history [2], starting in\n                                                                                as we wanted to explore the nature of the conversational interac-\nthe 1960s with pattern-matching approaches like Eliza [104], and\n                                                                                tion rather than having users focus extensively on the production\ncontinuing to today with state-of-the-art large language model-\n                                                                                of working code.\nbased conversational systems [107] such as Meena [3] and Blender-\n                                                                                   When designing how users would interact with the Program-\nBot [84]. These systems are intended to address the problem of\n                                                                                mer\u2019s Assistant, we decided that it should be available on demand\nopen-domain dialog, with a goal of realistically engaging in con-\n                                                                                and not monitor the user\u2019s work in progress or give unsolicited\nversation, but not particularly in a goal-directed or task-oriented\n                                                                                suggestions or advice, in keeping with the conversational agent\nmanner.\n                                                                                interaction model proposed by Ross et al. [73, 74]. This approach\n   Task-oriented chatbots are typically built with frameworks such\n                                                                                was supported by feedback from prospective users who were con-\nas the Microsoft Bot Framework2 , Google DialogFlow3 , and IBM\n                                                                                cerned about the assistant providing criticism of unfinished efforts\nWatson Assistant4 . They operate using pre-defined dialogue trees\n                                                                                in progress or distracting them while they worked. Instead, we\nand use natural language processing to detect conversational intents\n                                                                                force initiative onto the user and only have the assistant respond\nand extract contextual entities. This structure enables the creation of\n                                                                                to their requests. In this way, the assistant can provide help when\nspecial purpose, but fairly limited and rigid, conversational agents.\n                                                                                requested without undesirable interruptions that can distract or\n   There have been several recent attempts to investigate conver-\n                                                                                interfere with the user\u2019s flow.\nsational programming assistance. Kuttal et al. [42] conducted a\n                                                                                   When a user interacts with the assistant, we keep track of their\nWizard of Oz study in which a pair programmer was replaced with\n                                                                                selection state in the code editor. If a user sends a message to the\na conversational agent, and they found that \u201cagents can act as ef-\n                                                                                assistant without any code selected in the editor, then that message\nfective pair programming partners.\u201d The PACT system [106] is a\n                                                                                (along with the prior conversational context) is passed directly to\nchatbot that assists programmers adjusting to new programming\n                                                                                the model. If a user sends a message to the assistant with new code\n2 https://dev.botframework.com/                                                 5 https://microsoft.github.io/monaco-editor/\n3 https://cloud.google.com/dialogflow                                           6 https://www.npmjs.com/package/@monaco-editor/react\n4 https://www.ibm.com/products/watson-assistant/artificial-intelligence         7 https://fredrikoseberg.github.io/react-chatbot-kit-docs/\n\n\n\n\n                                                                          493\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                                                        Ross et al.\n\n\nselected in the editor (i.e. code that wasn\u2019t previously selected when                         interactions) in a back-end database. To address inconsistencies\nthey sent their last message), then that code is appended to the                               in the style or formatting of code generated by Codex, the proxy\nmessage before being communicated to the model.                                                server reformats all code segments using the Black code formatter\n   The model may produce multiple types of responses to a user\u2019s                               9 before transmitting them to the client UI.\nmessage. We treat each type of response differently in the UI.                                    The client maintains the transcript of the ongoing conversation.\n     \u2022 Responses that do not contain code are always rendered in                               Each time the user sends a message in the chat, the client constructs\n       the chat UI (Figure 1E).                                                                a new prompt for the model by concatenating the initial prompt,\n     \u2022 Responses containing short code snippets (\u2264 10 lines) are                               the chat transcript, and the user\u2019s new utterance, and makes a re-\n       rendered inline in the chat UI (Figure 1G).                                             quest for the model to complete the transcript. This completion\n     \u2022 Responses containing longer code snippets (> 10 lines) show                             request also specifies a stop sequence of tokens to prevent the\n       the code in a pop-up window (Figure 2A), with a proxy entry                             model from generating both sides of the conversation (e.g. what\n       in the chat transcript (Figure 2B) that allows users to re-                             the model thinks the user\u2019s next utterance might be after the as-\n       display the code window after it has been closed. Non-code                              sistant\u2019s response). Given the API\u2019s limitation on context length\n       text in the response remains in the chat transcript.                                    (4,096 tokens for both the prompt and model response), we silently\n                                                                                               \u201cforget\u201d older exchanges in the chat transcript when constructing\n   The assistant never directly modifies the contents of the user\u2019s\n                                                                                               the prompt to ensure that our completion request remains within\nsource code; rather, any code the user desires to transfer from the\n                                                                                               bounds. Nonetheless, the entire conversational history remains\nchat takes place via copy/paste.\n                                                                                               visible to the user in the UI.\n   Figure 1 shows a screenshot of a real, sample conversation, in\n                                                                                                  The client UI provides a loose coupling between the source code\nwhich the user asks a question that results in an inline response,\n                                                                                               editor and the chat interface. Users can hide the chat pane when\nthen requests an explanation of some code in the editor, and then re-\n                                                                                               they wish to focus solely on their code, and re-engage with it\nquests further elaboration. Figure 2 shows an example conversation\n                                                                                               when they desire assistance. Code selected in the editor is included\nthat resulted in the generation of a longer code sample, shown in a\n                                                                                               in the conversation in order to couple the code context with the\npopup window. This example shows how the assistant produced an\n                                                                                               conversation. Easily-accessible buttons are provided in the UI to\nincomplete solution, followed by criticism from the user regarding\n                                                                                               copy code responses from the assistant to the clipboard.\nthe missing code, and resulting in an apology and the generation\nof a complete solution.\n                                                                                               3.3      Handling Model Limitations\n3.1     Supporting Conversational Interaction                                                  While developing the Programmer\u2019s Assistant, and in early pilot\nWe enabled Codex to conduct a conversational interaction by prompt-                            testing, we experienced some quirks and shortcomings of the model\ning it with a conversational transcript and a request to produce                               and our approach to using it for conversational interaction. One\nthe next conversational turn. The prompt establishes a pattern of                              limitation stemmed from the fact that the model sometimes pro-\nconversation between a user and a programming assistant named                                  duced incorrect responses (e.g. code with syntax errors), incomplete\nSocrates. It provides several examples of Socrates responding to gen-                          responses (e.g. code that was missing functionality), irrelevant re-\neral coding questions, generating code in response to a request, and                           sponses (e.g. responses not related to the user\u2019s question), or insub-\naccepting code as input. It establishes a convention for delimiting                            stantial responses (e.g. \u201cI don\u2019t know\u201d). Because of the probabilistic\ncode in the conversation, making it easy to parse for display in the                           nature of model inference, re-prompting the model would some-\nUI. It also establishes an interaction style for the assistant, directing                      times produce a more correct or appropriate response. Thus, we\nit to be polite, eager, helpful, and humble, and to present its re-                            added the ability for users to \u201ctry again,\u201d either by asking in the chat\nsponses in a non-authoritative manner8 . Because of the possibility                            or by clicking a button in the UI (Figure 1C). This feature removes\nthat the model might produce erroneous answers or incorrect code                               the assistant\u2019s last response from the context presented to the model\n(as discussed in Weisz et al. [102]), we felt it was important that the                        and then re-invokes the model with an increased temperature10 .\nassistant convey a sense of uncertainty to encourage users to not                                 Although it is possible for transformer models such as Codex to\naccept its results uncritically to avoid over-reliance (e.g. as observed                       produce multiple possible responses to a single prompt, we only\nin Moroz et al.\u2019s study of Copilot [51], and discussed more gener-                             request a single response in order to speed up response time as\nally in Ashktorab et al. [9]) as well as automation bias [45, 46, 65].                         well as to preserve the token budget for conversational context.\nWe present the full text of the prompt used for the assistant in                               Thus, the \u201ctry again\u201d feature provides an alternate way to produce\nAppendix D.                                                                                    a wider variety of responses.\n                                                                                                  During pilot testing, we noticed that the assistant sometimes\n3.2     Architecture & UI Design                                                               happened to generate the same response to multiple, unrelated re-\n                                                                                               quests. In these cases, the assistant tended to get \u201cstuck\u201d in a pattern\nThe Programmer\u2019s Assistant communicates with the Codex API via\n                                                                                               of repeating the same response and was unable to resume normal\na proxy server that forwards requests from the React client. The\n                                                                                               conversation. To avoid this problem, we automatically execute a\nproxy also rate-limits access to conform to the API\u2019s policy, and\nit logs UI events from the client (e.g. requests, responses, and UI\n                                                                                               9 https://black.readthedocs.io/en/stable/\n8 The assistant\u2019s use of non-authoritative responses was encoded into the LLM prompt;          10 Temperature is a parameter in a generative model that specifies the amount of\noutput token probabilities from the LLM were not utilized to influence the assistant\u2019s         variation in the generation process. Higher temperatures result in greater variability\nresponse.                                                                                      in the model\u2019s output.\n\n\n\n\n                                                                                         494\n\fThe Programmer\u2019s Assistant                                                                            IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n                                                      A                                                                  B            C       D\n\n\n\n\n                                                                                                                                                      E\n\n\n                                                                                                                                                      F\n\n\n                                                                                                                                                      G\n\n\n\n\n                                                                                                                                                      H\n\n\n\n\nFigure 1: The Programmer\u2019s Assistant. The user interface provides a code editor on the left (A) and a chat pane on the right (B).\nThe \u201ctry again\u201d button (C) allows users to ask the assistant to generate an alternate response to the most recent question. The\n\u201cstart over\u201d button (D) resets the conversational context for the assistant, but maintains the chat transcript in the UI. In this\nexample, we show the assistant introduce itself to the user (E). Next, the user asks a general programming question (F), for\nwhich the assistant provides an inline code response (G). The user then asks a question about code selected in the editor (H),\nfollowed by a series of follow-up questions.\n\n\n\u201ctry again\u201d operation in the background when we see identical               3.4    Sample Conversation\nconsecutive responses from the assistant.                                   We provide a real sample conversation with the Programmer\u2019s\n   Finally, we noticed that the accumulation of conversational con-         Assistant in Listing 1. This conversation begins with the assistant\ntext sometimes resulted in the assistant becoming fixated on some           greeting the user (line 1). Next, the user asks a general Python\nportion of the earlier conversation. For example, it might respond          programming question (line 4), to which the assistant responds with\nto a question with portions of the prompt or of earlier conversation,       a non-authoritative remark (\u201cI think...\u201d) and a code snippet (line\nand become less responsive to newer requests. To address this issue,        9). The user next asks a follow-up question that depends on their\nwe introduced a \u201cstart over\u201d feature, accessible via the chat or by         previous question and the assistant\u2019s response (line 11), to which\nclicking a button in the UI (Figure 1D), that resets the context to the     the assistant provides another code snippet (line 15), satisfying the\noriginal prompt, forgetting the rest of the conversational history.         user\u2019s request.\nWe preserve the chat transcript in the UI, but delineate the break in          The user then switches topics and asks the assistant to write\nthe assistant\u2019s memory with an annotation in the chat transcript.           a Fibonacci function (line 17), and the assistant again responds\nThese annotations are added both for \u201ctry again\u201d and \u201cstart over.\u201d          with a non-authoritative remark (\u201cI will give it a try,\u201d line 20) and\n                                                                            a block of code. The user then asks how the function works (line\n                                                                            30) and the assistant provides an adequate description (line 32).\n\n\n\n\n                                                                      495\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                   Ross et al.\n\n\n\n\nFigure 2: Code Preview. Long code responses (> 10 lines) are presented in a pop-up window (A), with a button to copy the code\nand close the window. This code can be re-accessed from the chat transcript by clicking the proxy icon (B).\n\n\nNext, the user asks the assistant to re-implement the function in a        12\ndifferent way (line 37), again leveraging the ability to ask follow-up     13   Socrates : I think you would do something like this :\n                                                                           14\nquestions. The assistant produces an alternative implementation            15               for key in my_dict:\nthat conforms to the user\u2019s request (line 41). The user follows up         16\n                                                                           17   User :     Write a function that returns the nth value\nwith a question that depends on multiple past utterances and re-           18              in the Fibonacci sequence .\nsponses in the chat transcript (line 47), and the assistant produces       19\na relevant response (line 49). The conversation closes with the user       20   Socrates : I will give it a try .\n                                                                           21\nthanking the assistant (line 53) and the assistant acknowledging           22              def fib(n):\ntheir gratitude (line 55).                                                 23                if n == 0:\n                                                                           24                   return 0\nListing 1: A conversation with the Programmer\u2019s Assistant.                 25                elif n == 1:\n                                                                           26                     return 1\nCode presented by the assistant is listed in bold face.                    27                else:\n 1   Socrates : Hello ! My name is Socrates your programming               28                     return fib(n - 1) + fib(n - 2)\n 2              assistant . What can I help you with today ?               29\n 3                                                                         30   User :     How does this work ?\n 4   User :        How do I get a list of the keys in a                    31\n 5                 Python dictionary ?                                     32   Socrates : The Fibonacci sequence is defined by the\n 6                                                                         33              recurrence relation F ( n ) = F (n -1) + F(n -2).\n 7   Socrates : I think you would do something like this :                 34              This function implements that recurrence\n 8                                                                         35              relation .\n 9                  list(my_dict.keys())                                   36\n10                                                                         37   User :     Can this be done without recursion ?\n11   User :        What if I just wanted to iterate over them ?            38\n\n\n\n\n                                                                     496\n\fThe Programmer\u2019s Assistant                                                                                           IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n39    Socrates : I think so . How about this ?                                            participant through 10 sample interactions to give them a feeling\n40                                                                                        for what the assistant could do and how to interact with it. The\n41                def fib(n):\n42                  a,b = 0,1                                                             tutorial demonstrated how to ask questions, how to request code to\n43                  for i in range(n):                                                    be generated, and how to evaluate existing code. It did not specifi-\n44                     a,b = b,a+b\n45                  return\n                                                                                          cally cover how to generate documentation or unit tests. Tutorial\n46                                                                                        instructions were provided within the code editor. We include the\n47    User :       Which version is better ?                                              specific text used for the tutorial in Appendix B.\n48\n49    Socrates : I think the recursive version is more                                    4.1.2 Programming Challenges. After completing the tutorial, par-\n50               elegant , but the iterative version is more\n51               efficient .                                                              ticipants unlocked four programming challenges. Two of the chal-\n52                                                                                        lenges involved coding problems (writing a queue class and writing\n53    User :       Thanks !\n54\n                                                                                          code to create a scatterplot of data in a CSV file), one involved doc-\n55    Socrates : You ' re welcome !                                                       umenting a given function (an implementation of a graph search\n                                                                                          algorithm), and one involved writing unit tests for a given func-\n                                                                                          tion (computing the greatest common divisor of two arguments).\n4     EMPIRICAL STUDY OF CONVERSATIONAL\n                                                                                          Although the Programmer\u2019s Assistant was visible and available for\n      PROGRAMMING ASSISTANCE                                                              use, we provided no specific requirement that it actually be used to\nWe conducted an empirical user study of the Programmer\u2019s Assis-                           complete the challenges.\ntant to assess whether conversational assistance provides value in                           After participants completed their solution to a challenge, they\na software engineering context11 . Our inquiry focused on the user                        submitted it by clicking a button in the UI. The code editor used\nexperience and value of conversational interactions grounded in                           in the Programmer\u2019s Assistant was not a fully-functional IDE and\ncode. We therefore designed a qualitative study to investigate atti-                      did not provide syntax checking or the ability to run, test, or debug\ntudes toward a conversational programming assistant: do people                            code. Due to these limitations, participants were asked to submit\nenjoy interacting conversationally, what kinds of questions do they                       their solutions when they felt they had completed the challenge to\nask, and how does the experience compare to other forms of pro-                           their own satisfaction.\ngramming support such as searching the web? We note that prior\nstudies (e.g. [103, 105, 109]) conducted quantitative examinations                        4.2    Participants\nof the use of LLMs in code work; our study is akin to Weisz et al.\u2019s                      To recruit participants for our study, we posted internal advertise-\nqualitative examination of software engineers\u2019 attitudes toward                           ments in various communications channels focused on software\nworking with models that may fail to produce working code [102].                          engineering. Our advertisements stated that we were evaluating a\n   To address our questions, we deployed the Programmer\u2019s Assis-                          conversational programming assistant, but were kept deliberately\ntant within our organization \u2013 a global technology company \u2013 and                          vague in order to minimize the impact on peoples\u2019 expectations of\ninvited people to try it out and give us feedback on their experience.                    the experience.\nWe invited people with varying levels of programming skill in order                          Our advertisement yielded a pool of 140 potential participants.\nto obtain a wide range of feedback on the kinds of use cases for                          In order to recruit a diverse sample, we used a screening survey\nwhich the tool could provide assistance.                                                  that asked about their job role, their familiarity with and recency\n                                                                                          of use of Python, and their availability to participate in our study.\n4.1      Tasks                                                                            We accepted participants into the study on a rolling basis, selecting\nWe set up the Programmer\u2019s Assistant as a playground environment                          participants to capture a range of programming experiences and\nthat participants could try out with a few sample programming                             ensure balanced gender representation. We conducted periodic re-\nproblems. We created a tutorial to orient participants to the assis-                      views to determine whether we were learning something new from\ntant, its capabilities, and how to interact with it. We also created                      each participant or if we had reached the point of saturation [7]. We\nfour programming challenges focused on writing code, document-                            stopped collecting data after running 42 participants as we were no\ning code, and writing tests for code. We designed these challenges                        longer observing any new behaviors or gleaning any new insights.\nto expose participants to a broad range of the assistant\u2019s capabilities.                  The Programmer\u2019s Assistant implementation and configuration\nFor each of these challenges, we explicitly did not evaluate metrics                      were held constant over the course of the study; no changes to the\nsuch as the participant\u2019s productivity, the quality of their solutions,                   UI design or LLM prompt were made.\nor the time taken to produce them, as the focus of our study was to                          Our participants had the following self-identified characteristics:\nunderstand the utility of conversational interaction. We selected                              \u2022 Job role: 19 Software Engineers, 12 Researcher/Scientists, 3\nPython as the language used for the tutorial and challenges because                              Software Architects, 2 Data Scientists, 1 Machine Learning\nof its general popularity [21] and the fact that it was well-supported                           Engineer, 1 Systems Test Engineer, 1 Business Analyst, 1\nby our underlying LLM [24].                                                                      Manager, 1 Marketer, and 1 Consultant.\n4.1.1 Tutorial. All participants were first introduced to the Pro-                             \u2022 Gender: 21 Female, 19 Male, 1 Gender Variant / Non-con-\ngrammer\u2019s Assistant through a tutorial. The tutorial walked each                                 forming, and 1 Preferred not to say.\n11 For historical context, we note that our study was completed before the public\n                                                                                               \u2022 Python Experience: 17 participants had 3+ years of Python\nrelease of ChatGPT [60], which has subsequently demonstrated the application of                  experience, 11 had 1-3 years, 11 had less than 1 year, and 3\nconversational assistance for programming tasks [34].                                            were not familiar with Python.\n\n\n\n\n                                                                                    497\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                                        Ross et al.\n\n\n      \u2022 Recency of Python Use: 29 participants had written Python                           events, including 968 conversational exchanges in the event logs;\n        code within the past month, 4 within the past year, 5 within                        no survey data was excluded.\n        the past 5 years, and 4 had not written Python code within                             Our primary analysis of this data is qualitative, as our partici-\n        the past 5 years.                                                                   pants provided us with a rich source of interesting feedback and\n  We provide full demographic information for individual partici-                           thought-provoking insights in their comments. Where applicable,\npants in Appendix E.                                                                        we supplement this data with quantitative data from the survey\n                                                                                            and the event logs, as well as chat transcript data from the con-\n4.3      Procedure                                                                          versation logs. In this way, we triangulate [47] across our three\n                                                                                            data sources, using the open-ended survey data as a foundation.\nParticipants completed the study on their own time, independently\n                                                                                            When we quote participants, either from their qualitative survey\nand without moderation. Each participant was provided with a web\n                                                                                            responses or the conversational transcripts, we reproduce their\nlink to a pre-study survey that described the nature of the study and\n                                                                                            words exactly as typed, including typos, misspellings, grammati-\nthe tasks that they would be expected to perform. They were then\n                                                                                            cal errors, capitalization, and potential trigger words, and we only\ndirected to the Programmer\u2019s Assistant to complete the tutorial and\n                                                                                            make minor clarifying edits where needed, delineated by square\nthe four programming challenges. When participants indicated they\n                                                                                            brackets.\nwere finished with the challenges12 , they were directed to a final\n                                                                                               In order to set the context for our analysis, we first describe\npost-study survey. Complete sessions generally required about an\n                                                                                            how we used reflexive thematic analysis to analyze participants\u2019\nhour of effort, though some participants spread their effort across\n                                                                                            responses to the open-ended survey questions. We then describe\na longer period of time and across multiple sessions. Participants\n                                                                                            our analysis of the conversation logs and our development of a\nwere compensated for their time at a rate equivalent to US $15/hr.\n                                                                                            coding guide based on Conversation Analysis [76], and specifically,\n                                                                                            Moore and Arar\u2019s Natural Conversation Framework [50].\n4.4      Measures\nWe collected a variety of data in our study from three sources:                             5.1.1 Thematic Analysis of Qualitative Survey Responses. We con-\n                                                                                            ducted a reflexive thematic analysis to analyze the responses to\n    (1) Surveys. We employed three surveys in the study: a pre-                             our seven open-ended survey questions. We followed the process\n        study survey to collect demographic information, a pre-task                         described by Braun and Clarke [16] in which researchers immerse\n        survey to gauge expectations of the conversational user ex-                         themselves in the data, generate codes for material that seems\n        perience, and a post-task survey to assess actual user expe-                        interesting, and then iteratively group and refine codes through\n        rience. We describe these survey questions in the relevant                          collaborative discussion in order to identify higher-level themes.\n        context of our results, and we provide a complete listing of                        Initially, four authors performed open-coding on the open-ended\n        all survey instruments in Appendix A.                                               survey responses. Through discussion, these codes were grouped\n    (2) Event logs. The Programmer\u2019s Assistant was instrumented                             and consolidated into a single set, which were then re-applied to\n        to collect data on participants\u2019 usage. The event logs pro-                         the data by two authors. After another round of discussion, these\n        vided timestamped records of interaction events, including                          authors identified a set of 12 higher-level themes. Some themes had\n        conversational exchanges, hiding/showing the assistant, use                         clear parallels to quantitative survey questions or event log data,\n        of the \u201ctry again\u201d and \u201cstart over\u201d features, and use of copy/-                     and thus represented clear instances where we were able to trian-\n        paste.                                                                              gulate across data sources. Other themes surprised us. We structure\n    (3) Conversation logs. From the event logs, we extracted con-                           our presentation of the results based on these 12 themes, grouped\n        versational transcripts between each participant and the                            into three different aspects of the user experience: expectations\n        Programmer\u2019s Assistant.                                                             and experience, utility of conversational assistance, and patterns of\n                                                                                            interaction and mental models.\n5 RESULTS\n                                                                                            5.1.2 Conversation Analysis via the Natural Conversation Frame-\n5.1 Data & Analysis                                                                         work. In order to understand the content and structure of the\nWe collected a wealth of data in our study: 126 survey responses                            conversations that took place between our participants and the\nfrom three surveys per participant, containing 296 written com-                             Programmer\u2019s Assistant, we turned to the Natural Conversation\nments in open-ended survey questions, and 4,877 instances of 23                             Framework [50] (NCF). We developed a codebook for the event\ndifferent types of UI events, including 1,699 conversational ex-                            logs, beginning with 21 different categories of utterances from the\nchanges13 in the event logs. We also compute, for each participant,                         NCF. Nine NCF categories \u2013 Acknowledgment, Apology, Confirma-\ncounts or durations for 21 different metrics from the event logs.                           tion, Expression of Gratitude, Farewell, Greeting, Self-Identification,\n   In our analysis, we deliberately exclude the portion of our data                         Welfare Check, and Welfare Report \u2013 appeared twice in our code-\ncollected during the tutorial exercise. We exclude this data because                        book to distinguish cases in which the utterance was made by the\nthat activity was guided by the tutorial instructions, not by our                           human participant vs. the assistant. Other NCF categories were\nparticipants\u2019 own initiative. Thus, our final sample consists of 3,172                      split to provide nuanced detail about the interaction; for example,\n                                                                                            we distinguished three different kinds of NCF requests, depending\n12We did not enforce that participants actually complete all of the challenges. Never-\n                                                                                            upon whether they were stated as Requests for Action (e.g. \u201cWould\ntheless, all participants but one did submit solutions to all of the challenges.\n13We refer to a participant\u2019s utterance, followed by the assistant\u2019s response, as a         you...\u201d), Commands of Action (e.g. \u201cWrite a function that...\u201d), or\nconversational exchange.                                                                    Expressions of Desire (e.g. \u201cI want...\u201d). We also added 18 additional\n\n\n\n\n                                                                                      498\n\fThe Programmer\u2019s Assistant                                                                                        IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n Interlocutor       Orientation     Codes\n Human              Social          Acknowledgment, Apology, Criticism, Expression of Gratitude, Farewell, Greeting, Politeness,\n                                    Praise, Self Identification, Small Talk, Welfare Check, Welfare Report\n                    Task            Asks Question, Asserts Information, Capability Check, Command of Action, Expression of Desire, Identi-\n                                    fies Error, Request for Action, Requests Elaboration, Requests Explanation\n                    Meta / UI       Chat Context Required, Confirmation, Copy, Copy (Extraneous), Erroneous Input, Includes Selection,\n                                    Includes Extraneous Selection, Missing Selection, Paste, Paste (Extraneous), Pasted Code in Chat, Spelling\n                                    Error, Start Over, Try Again\n Assistant                          Appears Fixated, Claims Ignorance, Didn\u2019t Understand, Grants Request (Complete), Grants Request\n                                    (Incomplete), Offers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\n                                    Includes Code, Spews Garbage\nTable 1: Event log codebook. Our codebook contained 46 unique codes, applied separately to participant utterances (Human)\nand assistant responses (Assistant). Codes in bold were applied to both participant and assistant responses. Human codes were\nclassified as demonstrating either a social or task orientation to the assistant.\n\n\n\ncodes to identify meta-information such as utterances that included           by the amount of time the Programmer\u2019s Assistant window was in\ncode, utterances that referenced selected code, utterances that im-           focus.\nplicitly or explicitly referenced earlier portions of the conversation,          Participants made an average of 23.0 utterances (SD = 15.1 ut-\nor non-verbal UI activities such as copies, pastes, and invocations           terances) to the assistant. On average, 6.2 of their utterances (SD =\nof \u201ctry again\u201d and \u201cstart over.\u201d Finally, we classified a subset of the       4.3 utterances) contained a code selection. The average latency per\nhuman-applied codes based on whether they represented a par-                  request14 was 6.7 seconds (SD = 3.1 seconds).\nticipant\u2019s task or social orientation toward the assistant. We list              We saw a 66.3% rate of acceptance of generated code, where we\nour codes in Table 1, but note that not all of them ended up being            considered code to be accepted if the participant performed a copy\nrelevant to our analysis.                                                     immediately after the code was generated. This acceptance rate is\n   When coding conversational data, we applied individual codes               much higher than the 27% acceptance rate reported for Copilot [109].\nat the level of each conversational utterance. We allowed multiple            We believe one reason we observed a higher acceptance rate is\ncodes to be applied to each utterance to account for utterances that          because Copilot\u2019s completion suggestions are generated proactively,\nperformed multiple functions (e.g. greeting and self-identification).         whereas the Programmer\u2019s Assistant\u2019s suggestions are generated\nIn order to ensure consistency in how our codebook was applied,               upon request. When copying generated code from the assistant,\ntwo authors coded a 10% sample of the 968 conversational ex-                  participants most often copied the entirety of the generated code,\nchanges, achieving a satisfactory level of inter-rater reliability (Krip-     and only in 5.8% of cases did they copy a smaller portion of it.\npendorf\u2019s \ud835\udefc = 0.77, where agreement was conservatively defined\nas having all of the same codes applied to both utterances in a               5.2.2 User Experience Expectations & Changed Attitudes. Prior to\nconversational exchange).                                                     running our study, we had reason to believe that participants would\n                                                                              be skeptical of a conversational programming assistant. Before de-\n5.2     Expectations and Experience                                           veloping the Programmer\u2019s Assistant, we showed potential users\n                                                                              mockups of a program editor with an integrated chatbot feature.\nPilot testing of the Programmer\u2019s Assistant suggested that software\n                                                                              These prototypes elicited uniformly negative reactions. People told\nengineers would be skeptical of a conversational programming\n                                                                              us about their frustrating experiences with conventional chatbots\nassistant and its ability to provide useful assistance. Our study\n                                                                              and raised doubts about the knowledge, capabilities, and value\nrevealed that, for most participants, their actual experience after\n                                                                              of a conversational programming assistant. This skepticism moti-\nusing the tool was better than they had anticipated. Participants\n                                                                              vated us to develop the Programmer\u2019s Assistant in order to evaluate\nwere surprised at the quality of the assistant\u2019s responses and they\n                                                                              whether the conversational experience, as powered by a state-of-\nappreciated how its integration with the code editor reduced the\n                                                                              the-art code-fluent LLM, would be better than people had antici-\namount of context switching they needed to do in the UI. Some\n                                                                              pated. During pilot testing, we received feedback that the Program-\nparticipants struggled with the code selection feature, although\n                                                                              mer\u2019s Assistant provided a much better conversational experience\nothers appreciated the ability to ask questions related to selected\n                                                                              compared to testers\u2019 previous experiences with chatbots. Thus, in\ncode.\n                                                                              designing our study, we felt it important to first gauge participants\u2019\n5.2.1 Usage. All of our participants engaged with the Program-                expectations of a conversational interaction around code, and then\nmer\u2019s Assistant while working on the challenges, despite there                measure their experience after the fact.\nbeing no requirement to do so. Forty-one participants submitted\nsolutions to all four challenges, and one participant, P14, only sub-\nmitted solutions for one of the four challenges. Participants spent           14 This time includes additional time added by our proxy server to ensure our confor-\nan average of 68 minutes engaged with the assistant, as measured              mance to the API rate limitation.\n\n\n\n\n                                                                        499\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                                                Ross et al.\n\n\n   We developed a short inventory of six scale items to measure user                            all aspects that I could have imagined and more!\u201d P17 provided a\nexperience of code work15 . The scale was administered twice: once                              more quantitative assessment: \u201cInitial expectations: 3 Actual: 9.5.\u201d\nbefore participants were exposed to the Programmer\u2019s Assistant                                  P38 was emphatic in their evaluation: \u201cI was blown away how well\n(but after they had been briefed that they would interact with an AI                            it allowing me to structure how I want the code to look and work and\nchatbot), and once after completing the programming challenges.                                 just giving me the thing I asked for.\u201d\nThe items were presented with the appropriate tense: Do you expect                                  Many participants described a sense of surprise in their experi-\n(Did you find that) the Programmer\u2019s Assistant: (a) will be (was)                               ences. P9 was surprised by how well it understood their requests:\neasy to use; (b) will understand (understood) your requests; (c) will                                  \u201cI was surprised at how well the Programmer Assistant\nprovide (provided) high quality responses; (d) will help (helped)                                      was able to understand my requests and generate good\nyou to write better code; (e) will help (helped) you to write code                                     code/documentation/tests. It understood major concepts\nmore quickly; (f) will be (was) enjoyable to use. Each item was rated                                  and was able to explain it to me in a clear way, and it\non a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3),                              was also able to understand and write functional code.\nA great deal (4).                                                                                      It even was able to help me review my answer. I was also\n   A factor analysis revealed the items on this scale measured a                                       surprised at how well it could understand the context\nsingle construct, which we identify as user experience (Cronbach\u2019s                                     of what I was asking in follow-up questions when I did\n\ud835\udefc = 0.87). Thus, we computed two scores of user experience (UX)                                        not specify exactly what I was talking about, but rather\nfor each participant: a pre-task UX score computed as the average                                      referencing our prior conversation (such as, \u2018what does\nof their six pre-task expectation scale responses, and a post-task                                     that mean\u2019).\u201d (P9)\nUX score computed as the average of their six post-task experience\n                                                                                                   Similarly, P6 was surprised that they liked the conversational\nscale responses.\n                                                                                                interaction when they expected that they wouldn\u2019t:\n   We found that participants had lower initial expectations for\ntheir experience with a conversational programming assistant (pre-                                     \u201cI though[t] I wouldn\u2019t like the chatbot interaction and\ntask UX M (SD) = 3.0 (0.62) of 4) than their experience actually                                       that I would prefer something like the tool I\u2019ve seen in\nwas (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test                                    those demos [of Copilot]. But surprisingly, after using\nshows that this difference was significant, \ud835\udc61 (41) = 5.94, \ud835\udc5d < .001,                                   the chatbot (and seeing the results: easy to use, it un-\nCohen\u2019s \ud835\udc51 = 0.92 (large). Measured another way, 32 participants                                        derstands well, I felt it like a partner) I like this kind of\n(76.2%) had post-task UX ratings that were higher than their pre-                                      help.\u201d (P6)\ntask expectations, demonstrating a significant shift in attitudes\n                                                                                                5.2.3 Quality of Assistant\u2019s Responses. In order to gauge the quality\ntoward conversational programming assistance.\n                                                                                                of responses produced by the Programmer\u2019s Assistant, we examined\n   However, the UX ratings alone fail to capture participants\u2019 nu-\n                                                                                                the 910 task-oriented requests made by participants in the study.\nanced expectations of the assistant and the reasons for their shifted\n                                                                                                For the vast majority (80.2%), the assistant produced a correct re-\nattitudes after using it. Participants expressed a variety of expecta-\n                                                                                                sponse (Grants Request (Complete)); in other cases, the assistant\u2019s\ntions of the assistant before using it, including that it would be easy\n                                                                                                response was incorrect (9.6%; Provided Wrong Answer), correct but\nto use (P30) and produce correct responses (P30), understand the\n                                                                                                incomplete (4.4%; Grants Request (Incomplete)), or the assistant\nproblem and what is being asked of it (P8, P9, P11), not interfere\n                                                                                                didn\u2019t understand (3.4%; Didn\u2019t Understand), claimed ignorance of\nwith their flow state (P5), produce imperfect or questionable out-\n                                                                                                the subject (1.5%; Claims Ignorance), or produced another type of\nputs (P6, P21), improve with feedback (P31), provide generic and\n                                                                                                response (0.9%; Appears Fixated, Spews Garbage).\nunhelpful answers (P17) or only answer basic questions (P40), and\n                                                                                                   Participants also reported experiencing this variability in the\nproduce responses quickly (P40).\n                                                                                                quality of the assistant\u2019s responses. Some participants described\n   P17 expected \u201cto be frustrated very quickly and that what I\u2019d think\n                                                                                                how the assistant provided \u201cdetailed answers\u201d (P17) and \u201chigh qual-\nwould be relatively common questions would be responded to with\n                                                                                                ity outputs\u201d (P18) that were \u201csurprisingly good\u201d (P2). P6 felt it was\ngeneric, unhelpful answers.\u201d P6 explained, \u201cI didn\u2019t have very good\n                                                                                                \u201cincredible to see the quality of the responses,\u201d and P3 even explored\nexperiences with chatbots. I think I\u2019ll need to spend more time in\n                                                                                                the assistant\u2019s capabilities outside the scope of the challenges and\nreviewing and fixing the suggestions than in writing the code myself\n                                                                                                found that it could handle those as well:\nfrom scratch.\u201d P11 had a more balanced view, that \u201cIt\u2019ll do some tasks\nreally well, but others will not be as reliable.\u201d                                                      \u201cIt was surprising the quality of the code and the ability\n   After interacting with the Programmer\u2019s Assistant, many par-                                        to answer all my questions correctly. Although I think\nticipants commented on how the experience was better than they                                         the challenges may be biased towards what the Assistant\nanticipated, because it \u201cseemed to be able to handle complex issues\u201d                                   is able to do, it was a great experience because I asked\n(P10) and \u201cwas a great help\u201d (P8). P20 felt it was \u201cincredible!\u201d P6 and                                many other things and it was able to answer correctly.\u201d\nP17, who were both initially skeptical, reported having a positive                                     (P3)\nexperience. For P6, \u201cIt absolutely exceeded all my expectations, in                                Of course, the Programmer\u2019s Assistant wasn\u2019t perfect, and some\n                                                                                                participants did run into issues. For P35, \u201cThe documentation gener-\n15 Our scale items were modeled from scales published in Weisz et al. [103, Table 9 \u2013           ation did not perform very well.\u201d P16 questioned the accuracy of the\nAI Support] that measured constructs including ease of use (item 3), response quality           knowledge encoded in the model: \u201cDoes the model need to be up-\n(item 1), the production of higher-quality code (item 5), and the ability to write code\nmore rapidly (item 4). We added additional items to cover the constructs of request             dated? It said latest python version is 3.7 but google says it\u2019s 3.10.\u201d In\nunderstanding and enjoyment, and we cast all items on a 4-point scale of extent.                some instances, participants needed to ask their question multiple\n\n\n\n\n                                                                                          500\n\fThe Programmer\u2019s Assistant                                                                               IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\ntimes to get a good response: \u201cyou need to ask many times if you             noticeable or salient: \u201cThe \u2018try again\u2019 button is not so reachable, often\nwant to get an answer and also a detailed explanation\u201d (P3). P27 felt,       times I forgot it exists\u201d (P23). By contrast, at least one participant\n\u201cit was annoying when I asked it to try again and it would give me           was successful with these features:\nthe same response.\u201d P22 struggled because, \u201cIt didn\u2019t seem to handle\n                                                                                    \u201cat some point it had issue with challenge 3 and I had to\nmultiple sentences well.\u201d\n                                                                                    start over. Just asking \u2018try again\u2019 was not enough and I\n    P28 perhaps offered the most scathing criticism, that, \u201cIt makes\n                                                                                    was getting always the same (wrong and not related)\nmistakes often enough to be not very practical.\u201d However, despite\n                                                                                    answer. starting again solved the issue!\u201d (P20)\nthe production of poorer-quality responses, other participants felt\nthat the assistant was still helpful. P36 reported that, \u201cOnly minor\ntweaks were normally needed to correct any issues.\u201d Similarly, P38           5.3    Utility of Conversational Assistance\ndescribed how the assistant wasn\u2019t able to completely solve their            Our next set of themes concerns the utility provided by conversa-\nproblem, but provided a useful start:                                        tional programming assistance. Participants felt the assistant was\n       \u201cThere was only one hickup I noticed where when I                     highly valuable and desired to use it in their own work. They felt it\n       asked it to memoize fibonacci it couldn\u2019t, but it dropped             would be most helpful for smaller or narrowly-scoped tasks, but\n       the building blocks on my lap for me to finish so that                able to provide a wide variety of types of assistance. The fact that\n       was fine, that was like minutes of effort on my part.\u201d                the interaction model was conversational and grounded in code\n       (P38)                                                                 were valuable aspects, as was the ability for the assistant to bolster\n                                                                             users\u2019 learning about programming topics through that interac-\n5.2.4 UI Design & Affordances. Participants made many comments               tion. Participants did question whether they could trust and rely\non our specific UI design and the affordances provided (or not               upon the assistant\u2019s responses, echoing a similar theme discussed\nprovided) in our chat-augmented editor. Overall, the integration             in Weisz et al. [102].\nbetween the chat pane and the code editor was \u201cvery good\u201d (P23),\nwith a \u201cnice interface between the code pane and the assistant pane\u201d         5.3.1 Value & Appropriate Tasks. Participants rated the value of\n(P17) that \u201cmakes it really convenient\u201d (P35).                               the Programmer\u2019s Assistant highly (M (SD) = 8.6 (1.4) of 10). Many\n    Prior research by Brandt et al. [15] has shown how keeping               participants asked questions such as, \u201cCan I have it in my editor\ndevelopers focused in their IDE improves productivity, and our               please?\u201d (P15), or made comments that, \u201cI would enjoy using it in\nparticipants expressed similar sentiments. P40 remarked, \u201cIt allows          the future\u201d (P36), \u201cI would love to be able to... have access to it for\nme to stay in one browser window/tab!\u201d and P12 hinted at how the             my coding\u201d (P37), and \u201cI\u2019d love to use this tool as part of my usual\ninterface might preserve their flow state by \u201cprevent[ing] me from           programming workflow if I could!\u201d (P39). Some of the reasons why\ngetting distracted when looking into an issue in another tab.\u201d               participants found it valuable are because it \u201chelp[s] me remember\n    Some aspects of our user interface were confusing to participants,       how to do things in certain languages that normally I would just\nsuch as the mechanism for selecting code to be included in the               Google\u201d (P9) and \u201cIt helps me to avoid silly syntax errors and can\nconversational context. P7 remarked, \u201cIt\u2019s was a little confusing            when I cannot remember exact function/method names and required\ndoing the selection part for it to tell me what a function does, but...      arguments\u201d (P40). We did not observe any differences in value\nit gave me code that was insanely easy to copy and paste.\u201d Other             ratings based on participants\u2019 familiarity with or recency of using\nparticipants appreciated the code selection mechanism, such as P11:          Python.\n\u201cI enjoyed the code selection feature, and found that very easy to use.\u201d        Participants described a wide variety of tasks for which they\nIn the event logs, we identified 20 instances in which a participant         felt the assistant would be useful. These tasks included \u201cordinary\u201d\nunintentionally included selected code in the conversation when              (P23), \u201csimpler\u201d (P2), and \u201csmall, repetitive\u201d (P4) tasks such as \u201cquick\nit wasn\u2019t needed (Includes Extraneous Selection), 12 instances in            lookups\u201d (P25) for \u201cshort chunks of code\u201d (P11) or for \u201cnarrowed\nwhich a code selection was omitted when it was needed to provide             questions\u201d (P26). Participants also felt the assistant was useful for\ncontext for the question (Missing Selection), and 16 instances in            \u201csmall containable novel algorithms\u201d (P38) and \u201clittle coding problems\u201d\nwhich a participant copy/pasted code directly into the chat rather           (P4).\nthan selecting it in the editor (Pasted Code in Chat). Although                 Several kinds of task assistance were reported as being valuable,\nthese cases represent a small fraction of the 227 instances in which         such as explaining code (P31), implementing business logic in a\na code selection was required and included in the conversation               UI (P38), understanding what code does (P19, P37), and recalling\n(Includes Selection), their presence does indicate that more attention       language syntax, method names, and arguments (P12, P15, P20, P40,\nis needed to the interaction design of code selection.                       P42). P27 felt that the assistant was \u201cMore helpful when recognizing\n    Another issue regarded the awareness of the \u201ctry again\u201d and              a specific well known algorithm but not things you make yourself.\u201d\n\u201cstart over\u201d features. The \u201ctry again\u201d feature was only used by 14              Participants also made recommendations for how to increase\nparticipants, who used it a total of 63 times over the course of             the value of the Programmer\u2019s Assistant. P38 suggested, \u201cWhat\nthe study. Some participants used it specifically when they got an           would blow me away though is if it\u2019s able to help with what I do most\nanswer which they saw as clearly wrong, while others used it to              often which is to integrate, refactor and iterate on an existing system.\u201d\nget a variety of possible answers before proceeding. The \u201cstart over\u201d        P16, P26, and P38 all desired more information on the data sources\nfeature was used even less, by 5 participants who used it a total of         used to produce the assistant\u2019s responses. P9 requested to \u201cHave\n6 times. Despite our effort to surface these conversational features         the Programmer\u2019s Assistant examine your code and make proactive\nin the UI via shortcut buttons, they may not have been sufficiently          suggestions for improving it in the chat.\u201d P36 requested the same,\n\n\n\n\n                                                                       501\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                          Ross et al.\n\n\nbut cautioned that, \u201cCare would need to be taken to avoid becoming             Despite these individual differences in interaction preferences,\nan annoyance or disrupting the flow of a coding session.\u201d                   P39 envisioned that both interaction styles could be supported in\n   In the post-task survey, we probed participants on how certain           the tool:\nchanges to the Programmer\u2019s Assistant would either decrease, in-\n                                                                                   \u201cI think both options should exist: people should be able\ncrease, or result in no change to its value. Over 75% of participants\n                                                                                   to input their queries like a search bar AND also give\nfelt that the assistant would be more valuable if it operated in a\n                                                                                   their question as if in conversation.\u201d (P39)\nproactive manner, either by making improvement suggestions in\nthe chat or as comments directly in the code. Similarly, 78.6% of           5.3.3 Learning Effects. One specific benefit of the Programmer\u2019s\nparticipants felt that having more buttons in the UI for common             Assistant identified by participants is its ability to help people\nfeatures such as explaining or documenting code would make the              improve their programming skills and reinforce knowledge gaps.\ntool more valuable.                                                         For example, it can help users \u201cremember how to do things in certain\n                                                                            languages... such as, when I am using a language I haven\u2019t used in a\n5.3.2 Conversational Interactions Grounded in Code. One of the              while\u201d (P9). The assistant can also serve as an memory aid, such\nchallenges in interpreting participants\u2019 comments about the utility         as when \u201cI use a lot of libraries that I don\u2019t always remember all\nof the Programmer\u2019s Assistant was in disentangling the extent to            of the functions\u201d (P15). Similarly, P31 said, \u201cNo matter how good\nwhich value was derived from the quality of the underlying model            you\u2019re as a developer, you can\u2019t (humanly) remember all the API of\nversus the integration of conversation in a code context. Indeed,           hundreds of libs or new languages... I\u2019d learn new dev lang and new\nparticipants felt that the chat interaction was valuable: 69.0% of          lib/frameworks faster.\u201d\nparticipants felt that eliminating the conversational interaction              P39 felt the assistant \u201cis perfect for programmers of all levels,\u201d and\nand making the assistant behave more like web search would de-              P1 felt it could help them rapidly improve their Python skills:\ncrease its value. Further, our analysis of the conversation transcripts\n                                                                                   \u201cI have wanted to learn python... The main concern\nrevealed that 42% of the 910 task-oriented utterances from partic-\n                                                                                   how much time spent learning is needed before I could\nipants required historical conversational context (Chat Context\n                                                                                   actually get some value out of learning python. I have\nRequired) in order to be correctly interpreted. Thus, we observe\n                                                                                   a feeling this would cut that time down from weeks to\nthat behaviorally, participants did rely on conversational context\n                                                                                   a day or so.\u201d (P1)\nin their interactions.\n   In the post-task survey, 83% of participants rated the importance           P39 also identified the fact that, because the interactions with\nof the ability to ask follow-up questions as being \u201csomewhat\u201d or \u201ca         the assistant are conversational, it forces people to learn how to\ngreat deal.\u201d Several participants specifically commented on the value       communicate to others about their code:\nof this conversational context. P39 remarked, \u201cI absolutely loved                  \u201cThe conversation aspect promotes proper communi-\nhow you can straight up ask follow-up questions to the Programmers\u2019                cation, which would really stand to benefit budding\nAssistant without having to reiterate the original topic/question.\u201d P15            programmers if they want to learn how to explain con-\nexpressed a similar sentiment, saying, \u201cI think the conversational                 cepts more fluently in the future to their colleagues.\u201d\ncontext was someone helpful, just in communicating that it\u2019s a running             (P39)\nconversation where my context is remembered.\u201d P9 provided a similar\nanalysis:                                                                      Conversely, P36 suggested that over-reliance on programming\n                                                                            assistance might have a detrimental effect to one\u2019s learning:\n        \u201cThis tool was so helpful at answering questions I had\n                                                                                   \u201cIt\u2019s definitely a huge time saver, but over-reliance on it\n        about the code in the context of the code I am working\n                                                                                   may cause new developers to skip learning the reference\n        on... I was also impressed with how well it was able to\n                                                                                   material themselves and discovering new things and\n        remember the context of our conversation, especially\n                                                                                   sparking new ideas.\u201d (P36)\n        when I asked vague follow-up questions.\u201d (P9)\n\n   In addition, some participants identified how a conversational           5.3.4 Trust. Many participants raised questions about whether\ninteraction grounded in code was useful, \u201cbecause I think to \u2018un-           they could trust the responses provided by the Programmer\u2019s Assis-\nderstand\u2019 the dev context could be VERY important\u201d (P31). In fact,          tant. P21 asked this question most directly: \u201cwill the code be correct,\n24.9% of task-oriented utterances included a relevant code selection        safe, efficient?\u201d Other participants raised similar questions, such as,\n(Includes Selection), showing that participants valued this ability.        \u201cI\u2019m wondering how it validates it\u2019s answers, if it can be trusted to\n   Contrasting with these participants, P18 felt that interacting           always give a working answer\u201d (P10), and \u201cSometimes lack of source\nwith the assistant conversationally was tedious, and they employed          and context may raise doubts in the mind of the programmer\u201d (P16).\na more direct approach:                                                        These issues of trust were exacerbated by the fact that the Pro-\n                                                                            grammer\u2019s Assistant did not allow participants to actually run their\n        \u201cI really like the PA. But, I didn\u2019t converse with it like          code. Because of this limitation, participants had to rely on their\n        a chat bot. I often told it what to do (\u2018Document this              own knowledge to judge the correctness of the assistant\u2019s responses.\n        code.\u2019) as opposed to asking it what to do (\u2018How do I               P19 asserted, \u201cThere is no way to evaluate if the Programmer\u2019s as-\n        document this code?\u2019). Talking to it the way that was               sistant is giving you the right advise or not other than your own\n        suggested in the tutorial seemed overly verbose/tedious.\u201d           knowledge,\u201d and P9 concurred: \u201cI had to trust that it was correct (and\n        (P18)                                                               use my own prior knowledge).\u201d\n\n\n\n\n                                                                      502\n\fThe Programmer\u2019s Assistant                                                                            IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n   P18 described the potential consequences of allowing the assis-         quality of the unit test (lines 31& 37). Thus, we observe that the\ntant to write code for them:                                               assistant is capable of producing explanations and justifications\n       \u201cThe only thing that made me nervous was that it could              when asked.\n       have introduced a bug that wasn\u2019t immediately appar-\n       ent. And given I didn\u2019t write the code, I could have easily         5.4    Patterns of Interaction and Mental Models\n       glossed over a mistake when reviewing it. Especially if             Participants interacted with the assistant in a variety of ways with\n       it is also the one writing the test cases.\u201d (P18)                   two main patterns of usage standing out: (1) invoking the assis-\n   Despite our efforts to make the Programmer\u2019s Assistant respond          tant to solve the entire programming challenge, and (2) breaking\nin non-authoritative ways, we did observe participants sometimes           the challenge down into a set of smaller tasks and invoking the\nuncritically accept generated results that were clearly wrong or           assistant\u2019s help for each. There were no clear differences in how\nincomplete. Thus, we did find behavioral evidence for over-reliance.       participants with differing Python experience approached the tasks.\n                                                                              Participants\u2019 mental models of the assistant also varied. Although\nListing 2: Building trust through explanations and justifica-              participants strongly saw the role of the assistant as being a tool,\ntions                                                                      their behaviors revealed that in many cases, they actually treated it\n 1   P9 :         what is a unit test ?                                    as a social agent. In addition, participants ascribed various mental\n 2\n 3   Socrates : A unit test is a test of a single function                 capacities to the assistant, such as having the ability to understand,\n 4              or method . It is typically written by the                 compute, and learn.\n 5              programmer who wrote the code .\n 6\n                                                                              Participants felt the assistant changed the nature of their work\n 7   P9 :                                                                  process. For some participants, it enabled them to focus on the\n 8                def euclid(a, b):                                        higher-level aspects of development because the assistant handled\n 9                    while b != 0:\n10                        (a, b) = (b, a % b)                              lower-level details or provided partial solutions for them to build\n11                    return a                                             upon. Many participants felt the assistant sped up their work and\n12                                                                         helped them remain focused on their tasks.\n13                how would I write a unit test for this code ?\n14                                                                            Finally, participants drew comparisons between the Program-\n15   [...]                                                                 mer\u2019s Assistant with other forms of programming support such\n16\n17   Socrates : I think you would do something like this :\n                                                                           as Copilot and web search. They felt that the conversational style\n18                                                                         of interaction enabled them to discover new, emergent behaviors\n19                def test_euclid():                                       from the model that were unavailable from Copilot\u2019s focus on code\n20                  assert euclid(2, 4) == 2\n21                  assert euclid(6, 9) == 3                               autocompletion. They also felt that the examples provided by the\n22                  assert euclid(17, 12) == 1                             assistant were more readily usable within their own code compared\n23                  assert euclid(4, 4) == 4                               to browsing for answers within search results, speeding up the\n24\n25   P9 :         what is a euclid function                                coding process. However, some participants advocated for a bal-\n26                                                                         anced approach to the design of programming assistance tools by\n27   Socrates : The euclid function is a function that\n28              computes the greatest common divisor of two\n                                                                           incorporating multiple modes of interaction rather than fixating\n29              numbers .                                                  on a single one.\n30\n31   P9 :         is this a good unit test for a euclid                    5.4.1 Interaction Styles and Assistant Role. We observed that par-\n32                function\n33                                                                         ticipants interacted with the Programmer\u2019s Assistant in strikingly\n34   Socrates : I believe that it is a good unit test for                  different ways. Some participants would present the entire chal-\n35              a euclid function .                                        lenge description to the assistant and then work with the results it\n36\n37   P9 :         why                                                      produced. Other participants approached the programming chal-\n38                                                                         lenges in a piecemeal fashion, breaking them apart into a set of\n39   Socrates : It seems to me that it tests the edge\n40              cases of a and b being equal , a being\n                                                                           smaller tasks, then invoking the assistant to aid with each one.\n41              greater than b , and b being greater than a .                 Experience with Python was not a determinant of how partici-\n42                                                                         pants approached the programming challenges, but it did seem to\n43   [...]\n44                                                                         impact how participants interacted with the assistant. Less experi-\n45   P9 :         thanks Socrates this was helpful                         enced participants tended to ask the assistant basic questions such\n46                                                                         as, \u201cWhat is a unit test\u201d (P29, not familiar with Python) and \u201chow\n47   Socrates : Always happy to help .\n                                                                           do I document a function?\u201d (P27, < 1 year of experience). More expe-\n   One way to address trust issues is for the assistant to provide         rienced participants made detailed requests about specific Python\nfurther explanations and justifications that can calibrate a user\u2019s        libraries or algorithms, such as, \u201cgiven a pandas dataframe with\nconfidence in the assistant\u2019s responses. Such explanations could           two columns \u2018Date\u2019 and \u2018Sales\u2019 please use matplotlib to draw me a\nbe requested conversationally, though most participants did not            scatterplot\u201d (P38, 3+ years of experience) and \u201cimplement a runge-\nattempt to do so. One participant (P9) did ask for such explanations,      kutta algorithm for solving an ODE with adaptive time steps\u201d (P37,\nand we show a summary of their transcript in Listing 2. In this            3+ years of experience).\ninstance, P9 asked for a definition of a unit test (line 1), an expla-        Another difference we observed in how people interacted with\nnation of the code being tested (line 25), and justifications of the       the assistant stemmed from their view on the role it played in their\n\n\n\n\n                                                                     503\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                                                                                          Ross et al.\n\n\ncollaborative process. Some participants, such as P18, treated it                                                                            orientation codes in Table 1). Twenty participants (47.6%) made at\nmore as a tool by issuing commands rather than asking questions.                                                                             least one socially-oriented utterance. An extreme form of this inter-\nAs quoted earlier, they said, \u201cI didn\u2019t converse with it like a chat                                                                         action style can be seen in a snippet from P6\u2019s transcript (Listing 3).\nbot.\u201d P5 described their interaction style similarly: \u201cI found myself                                                                           The 20 participants with a behaviorally-demonstrated social\nwanting to type search queries into Socrates, not treating it as a person                                                                    orientation did not generally differ in their role ratings from other\nbut as a search tool.\u201d                                                                                                                       participants, except that they rated the assistant as more likely to\n   In anticipation that participants would have different orienta-                                                                           be an advisor (Fisher\u2019s exact test, two-tailed \ud835\udc5d = .02) or a reviewer\ntions to the assistant and its role, we asked a question on the post-                                                                        (Fisher\u2019s exact test, two-tailed \ud835\udc5d = .03). However, they did not differ\ntask survey about the different kinds of roles the assistant might                                                                           in their overwhelmingly-strong ratings of the tool orientations.\ntake. These roles generally fell into one of two categories: a tool                                                                          Thus, at least for some participants, there seems to be a dissonance\norientation (a tool, a reference guide, a content generator, a problem                                                                       in their view of the assistant\u2019s role orientation.\nsolver), and a social orientation (a collaborator, a colleague, a coach,\nan advisor, a reviewer). Participants rated the extent to which they                                                                         Listing 3: Excerpt from P6\u2019s interaction with the Program-\nviewed the Programmer\u2019s Assistant in each of these roles on a 4-                                                                             mer\u2019s Assistant, in which P6 offers their thanks and congrat-\npoint scale of extent: Not at all (1), A little (2), Somewhat (3), or A                                                                      ulations.\ngreat deal (4).                                                                                                                               1   P6 :         thank you ! it 's been really nice to code\n                                                                                                                                              2                with you ! or to have you coding ! LOL\n                                                                                                                                              3   Socrates :   You ' re welcome !\n                             100                                                                                                              4   P6 :         I hope to see you on air soon , Socrates !\n                                                                                                                                              5   Socrates :   I hope so too .\n                                                                                                                                              6   P6 :         congrats to you research team ! you ' ve made\nPercentage of participants\n\n\n\n\n                             75                                                                                                               7                a good job !\n                                                                                                                                              8   Socrates :   I ' m glad you think so .\n                                                                                                                                              9   P6 :         sorry ... they ' ve made a good job ( well ...\n                             50                                                                                                              10                you too ... ; -) )\n                                                                                                                                             11   Socrates :   Thank you .\n                                                                                                                                             12   P6 :         see you next time ! bye !!\n                             25                                                                                                              13   Socrates :   Goodbye .\n\n\n                               0                                                                                                             5.4.2 Mental Capacities. Participants made a number of inferences\n                                   Tool    Content    Reference Collaborator     Problem    Advisor   Coach     Reviewer   Colleague\n                                          generator     guide                     solver                                                     about the Programmer\u2019s Assistant and its capacities for thought.\n                                                                                   Role                                                      Many participants talked about how the assistant possessed a level\n                                               Rating        Not at all        A little    Somewhat     A great deal\n                                                                                                                                             of \u201cunderstanding\u201d (P6, P8, P11, P18, P32) of \u201cthe context\u201d (P9, P21)\n                                                                                                                                             as well as \u201cmajor concepts\u201d (P9) and \u201cknowledge\u201d (P33). P24 was\n                                                                                                                                             amazed by the assistant\u2019s ability to \u201ctake a plain english request\nFigure 3: Role orientations. Participants overwhelmingly felt                                                                                and interpret it properly.\u201d P7 ascribed intelligence to the assistant,\nthat the assistant\u2019s role was of a tool orientation rather than                                                                              saying, \u201cIt was a lot smarter and trained tha[n] I thought it was.\u201d\na social orientation. The chart shows ratings distributions                                                                                     One participant assumed that the assistant \u201cKeeps improving\nacross 9 different roles the assistant might take, sorted by                                                                                 through (user) feedback\u201d (P31). Another felt that the assistant was\nthe percentage of participants who rated the extent of that                                                                                  capable of computation: \u201cIt understands the problem... It can calculate\nrole as \u201cA great deal.\u201d The leftmost role is of a tool, with                                                                                 the results of a function back\u201d (P8).\n80.9% of participants rating it as \u201ca great deal.\u201d Following                                                                                    However, not all participants were convinced of the assistant\u2019s\ntool are content generator (69.0%), reference guide (64.3%),                                                                                 ability to understand. P37 questioned the assistant\u2019s limitations: \u201cI\ncollaborator (40.5%), problem solver (35.7%), advisor (21.4%),                                                                               wonder how far beyond boilerplate it can go and if it works for truly\ncoach (19.0%), reviewer (16.7%), and colleague (14.3%).                                                                                      original problems.\u201d\n\n   We show participants\u2019 ratings of the assistant\u2019s role in Figure 3.                                                                        5.4.3 Impact of Conversational Assistance on Work Practices. Many\nDespite the fact that their attitudes toward the assistant overwhelm-                                                                        participants discussed how the Programmer\u2019s Assistant shaped\ningly reflected a tool orientation, their behaviors reveal that many                                                                         their work practices on the programming challenges. Overall, par-\nparticipants actually treated the assistant as a social agent. P6 de-                                                                        ticipants felt that the assistant \u201csaves time\u201d (P10), \u201chelps me code\nscribed how \u201cI felt it like a partner,\u201d and P4 told the assistant, \u201cI                                                                        faster\u201d (P34), and would \u201cspeed up my productivity\u201d (P19) because \u201cI\ncould not have solved [the challenge] without your help,\u201d to which                                                                           could focus on validating and improving the code it generated instead\nthe assistant responded, \u201cI\u2019m glad I could help.\u201d                                                                                            of having to write it all from scratch\u201d (P18). P37 remarked that, \u201cIt\n   The literature on Computers as Social Agents (CASA) helps us                                                                              opens a whole new door for fast develpment.\u201d P4 discussed how the\ninterpret this result as it demonstrates how computers are often                                                                             assistant \u201cwas helpful in staying focused on the code,\u201d although for\ntreated like people [56, 67]. LLM-based conversational agents can                                                                            P14, \u201cit took [me] time to get into tempo with the tool.\u201d\nexacerbate this tendency; as they likely have been trained on ex-                                                                               P31 pointed out how the assistant would change the nature of\namples of social interaction, they can also respond as social agents.                                                                        their work:\n   In the conversation logs, we identified participants who inter-                                                                                   \u201cMy job could focus more on higher level aspects and\nacted with the assistant in a socially-oriented fashion (the social                                                                                  therefore achieving better (quality) results, besides the\n\n\n\n\n                                                                                                                                       504\n\fThe Programmer\u2019s Assistant                                                                                      IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n       time-to-value... Data science (and dev) becomes a more                 When a desirable item is found, users must translate it into their\n       creative-higher level experience.\u201d (P31)                               code environment (e.g. via copy/paste) and possibly edit it to fit\n   Other participants discussed a work process in which the assis-            their existing code.\ntant provided incomplete solutions \u2013 the \u201cbuilding blocks\u201d (P38) or              We also note that the Programmer\u2019s Assistant and Copilot both\n\u201cinitial draft of code\u201d (P11) \u2013 upon which they could build. P5 aptly         utilize the same underlying AI model, Codex [24], which means\ndescribed this process:                                                       that the only difference between these tools is the user experience.\n                                                                              The extent to which Codex was trained on data from programming-\n       \u201cIt\u2019s nice to copy well formulated challenges in natural\n                                                                              related Q&A web sites is less clear, but for the purposes of our\n       language and have the code generator take its best stab\n                                                                              analysis, we focus our discussion solely on the differences in their\n       at it, then edit to our hearts content.\u201d (P5)\n                                                                              interaction models16 .\n   Participants felt that human review of the assistant\u2019s responses              Participants reported various benefits and drawbacks of a con-\nwas necessary because \u201cThe answers provided are generally not                 versational interaction over a direct manipulation interaction. Fore-\nnovel solutions, often look clunky and non-elegant. There may be              most, conversation \u201cfelt very natural\u201d (P21) and \u201cfeels much more\nsome unnecessary code. Basically the code would need to be reviewed\u201d          natural using Natural Language with the AI\u201d (P39). In addition, P39\n(P16). P35 also pointed out how \u201cThe code generator was good but              felt that \u201cthe use cases of Programmers\u2019 Assistant seem more open-\nyou still have to really check it.\u201d P19 discussed how they would turn         ended.\u201d Many participants were surprised at the variety of tasks the\nto the assistant as a first source for support, and only if it wasn\u2019t         assistant was capable of performing, from writing unit tests (P19,\nable to help would they then turn to other support tools:                     P36, P37) and documentation (P12, P19, P36, P37) to explaining\n       \u201cThe way I will use it is, I will first us[e] the Program-             what code did (P31, P38) and even answering general-knowledge\n       mer\u2019s assistant for most of my cases. Only in certain                  questions (P31). Again, we note that the Programmer\u2019s Assistant\n       cases where Programmer\u2019s assistant cant answer things                  utilizes the same underlying model as Copilot, yet the conversa-\n       I will turn up to official documentation or stack overflow.\u201d           tional interface was able to expose a wider variety of emergent\n       (P19)                                                                  behaviors from the model. Multiple participants explored the limits\n   However, latency was a factor for interactive use of the assistant         of the assistant\u2019s knowledge and abilities beyond our programming\nand participants noticed when the assistant took a long time to               challenges. For example, P37 asked it questions about physics and\nrespond. P19 remarked, \u201cSometimes it took lot of time, like more than         ordinary differential equations (\u201cODe\u201d as written by P37), and was\n5 seconds.\u201d P40 also felt \u201cthe response [was] a little slow sometimes...      surprised by the \u201cversatility of what it could answer.\u201d\nin chat mode I expect faster responses.\u201d As discussed in Section 5.2.1,                \u201cI asked it some physics and ODe question and the\nthe assistant took an average of 6.7 seconds (SD = 3.1 seconds)                        answers, though not complete, included the key parts\nto respond to a request, and participants did appreciate when the                      needed to write that code.\u201d (P37)\nassistant produced rapid responses: \u201cI loved how quick it was able               P31 probed the assistant on its knowledge of geography and was\nto pull up answers to questions I had\u201d (P38).                                 surprised when the assistant produced a correct answer.\n5.4.4 Conversational Interaction vs. Other Interaction Models. Al-                     \u201cI asked something out of SW engineering domain (ge-\nthough our study was not intended to make comparative evalu-                           ography) and it replied correctly, also by correctly an-\nations with the Copilot tool, we nonetheless asked participants                        swering on my nationality.\u201d (P31)\nwhether they were familiar with Copilot, and if so, to comment on                For some participants, the ability to assess the assistant\u2019s re-\nhow the two tools compared. We also asked a similar question to               sponse before committing to it (i.e. by inserting assistant-generated\ncompare the assistant with another popular form of programming                code into their editor) was a boon. P15 described how the copy/-\nassistance, searching the web (via a search engine like Google, or            paste boundary provided them with \u201ca bit more control to ask specific\na Q&A site like Stack Overflow). In discussing the differences be-            questions about what I wanted and to assess before putting it in my\ntween these three tools, we note that the primary differentiator is           code.\u201d Other participants felt that the copy/paste boundary was\ntheir interaction model.                                                      more inefficient:\n   The interaction model for the Programmer\u2019s Assistant is clearly\n                                                                                       \u201cI think the main difference is the ability of Copilot to\nconversational: users ask questions in natural language and are\n                                                                                       suggest code while you type, what make it faster and\nprovided with a response in natural language and/or code. The\n                                                                                       easier to use. While using the Programmer\u2019s Assistant,\ninteraction model of Copilot is reminiscent of direct manipulation\n                                                                                       you need to go to the chat, ask the question, copy the\ninterfaces [37], in which the user\u2019s actions in the user interface\ndirectly manipulate an object on the screen. Copilot automatically\n                                                                              16 As an aside, our comparison of direct manipulation, search, and conversational inter-\nmakes autocompletion suggestions as the user types. This auto-\n                                                                              action models is reminiscent of historical comparisons of text-based and graphical user\ncompleted code is directly placed in the source editor; thus, the             interfaces [54, 63]. Each modality was shown to have advantages and disadvantages.\nuser\u2019s work is contained entirely within the scope of the object              For example, text-only interfaces can provide accessibility [54, 78] and productivity [53]\n                                                                              advantages, whereas graphical user interfaces provide greater discoverability [10, 88].\non which they are working (i.e. the source code), which is how                Some researchers explored ways to bridge the two interaction modes, such as by\ndirect manipulation interfaces operate. In web search, users enter            developing GUI wrappers for command line programs [53, 94] or by developing tools\na separate search context (e.g. a search engine accessed within a             that converted GUI activities into procedural descriptions [55]. Our view is that similar\n                                                                              bridges can be constructed between direct manipulation, search, and conversational\nweb browser), type in a natural language query, and then forage               models of interaction; a user\u2019s interaction with an LLM need not be constrained to a\namongst search results to identify relevant items of interest [12, 62].       single interaction model.\n\n\n\n\n                                                                        505\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                            Ross et al.\n\n\n        code (or rephrase the question if it was not understood                  models (direct manipulation and search) because of its embedded-\n        by the agent), and edit it to match your code.\u201d (P3)                     ness in the UI and its ability to surface emergent behaviors of the\n   A large number of participants felt that the conversational inter-            model (Section 5.4.4).\naction was faster than web search (P1, P6, P7, P10, P11, P12, P16,                  We do not believe that these different interaction models are\nP17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability         in competition and we agree with P39\u2019s assessment that assistive\nto provide \u201creal-time responses\u201d (P32) that can be \u201capplied exactly to           tools can be built using a plethora of different interaction models.\nyour code\u201d (P33) without having to \u201cparse through lots of text... to get         For use cases in which a model is known to produce high-quality\nwhat you need\u201d (P15). In addition, the assistant provided \u201cMUCH                  results (e.g. code autocompletion for Codex), a direct manipulation\nfaster, better responses\u201d (P17) that were \u201cmuch more relevant to the             interface seems wholly appropriate as it would provide a discov-\nproblems\u201d (P34) and \u201csimple [and] succinct\u201d (P9), without having to              erable and predictable way of invoking the model to produce a\n\u201csort through answers on your own or read documentation\u201d (P9) or                 known type of result. However, direct manipulation interfaces may\n\u201clook at many posts before finding the relevant one\u201d (P18).                      be less ideal for surfacing the emergent behaviors of a foundation\n   Despite these benefits, some participants felt that the assistant             model [14], and thus natural language interaction may be more suit-\nmight not work well for \u201cmore specific and difficult problems on                 able. Many popular text-to-image models, such as DALL-E 2 [66]\na bigger scale\u201d as compared to web search. P9 felt that \u201cthe data                and Stable Diffusion [72], operate in a one-shot fashion, in which\n[of the Programmer\u2019s Assistant] wasn\u2019t as rich\u201d as the web. Other                the user specifies a prompt, clicks a button, and gets results. Our\nparticipants felt that the assistant lacked the \u201cmultiple answers\u201d (P9)          study demonstrates how the additional contextual layers of con-\nand \u201crich social commentary\u201d (P19) that accompanies answers on                   versational history and the artifact-under-development provide\nQ&A sites:                                                                       additional value to the co-creative process.\n        \u201cI like to see the different versions proposed on stack\n        overflow and the commentary of what makes one solu-                      6.2    Toward Human-AI Synergy\n        tion better than another in a given situation.\u201d (P27)                    The aim of human-centered AI is to \u201cenable[] people to see, think,\n   Some participants promoted a more balanced view that there                    create, and act in extraordinary ways, by combining potent user\nisn\u2019t a single mode of interaction superior to all others. P19 felt that         experiences with embedded AI methods to support services that\nweb search would be a fallback when the assistant failed to answer               users want\u201d [82]. Building upon this definition, Rezwana and Maher\na question. P39 described how search could be integrated with the                [69] posit that, \u201cIn a creative collaboration, interaction dynamics,\nconversational interaction:                                                      such as turn-taking, contribution type, and communication, are the\n        \u201cI think both options should exist: people should be able                driving forces of the co-creative process. Therefore the interaction\n        to input their queries like a search bar AND also give                   model is a critical and essential component for effective co-creative\n        their question as if in conversation.\u201d (P39)                             systems.\u201d [69]. They go on to note that, \u201cThere is relatively little\n                                                                                 research about interaction design in the co-creativity field, which\n6 DISCUSSION                                                                     is reflected in a lack of focus on interaction design in many existing\n                                                                                 co-creative systems.\u201d\n6.1 Value of Conversational Interaction                                              Our study begins to address this gap. While many co-creative\nWe began our research by asking the question of whether con-                     systems examine casual tasks or experimental activities (e.g., Spoto\ntemporary developments in code-fluent LLMs could sufficiently                    and Oleynik [87]), our focus was on the co-creative practice of\nsupport a conversational programming assistant. We believe that                  programming. Our goal was to understand peoples\u2019 attitudes to-\nour work has demonstrated that they can. Clearly, the Program-                   ward a conversational programming assistant, akin to Wang et al.\u2019s\nmer\u2019s Assistant was viewed by our participants as a useful tool that             examination of data scientists\u2019 attitudes toward automated data\nprovided real value \u2013 so much so that many participants explic-                  science technologies [99]. We found that, despite an initial level of\nitly requested or expressed the desire to use it in their own work.              skepticism, participants felt that a conversational assistant would\nHowever, how much of this value was derived from the model itself                provide value by improving their productivity (Section 5.4.3). How-\nand its ability to produce high-quality responses to programming                 ever, further work is needed to assess the extent to which this type\nquestions, versus from participants\u2019 ability to conduct extended                 of assistance provides measurable productivity increases.\nconversational interactions grounded in their actual source code?                    Campero et al. [19] conducted a survey of papers published in\n   We believe that both of these constituent aspects were valuable.              2021 that examined human-AI synergy, the notion that a human-AI\nIndeed, many participants commented on their surprise and satis-                 team can accomplish more by working together than either party\nfaction with the quality of the assistant\u2019s responses (Section 5.2.3).           could accomplish working alone. They found mixed results, with\nHowever, participants also valued the conversational interactions                no clear consensus emerging on how to design human-centered AI\nthat they had with the assistant. In the event logs, we saw evi-                 systems that can guarantee positive synergy. Summarizing from\ndence that participants were leveraging conversational context to                their discussion,\nask follow-up questions as well as leveraging code context by ask-\ning about their code selections (Section 5.3.2). Many participants                      \u201cPerhaps achieving substantial synergies among people\nreported that they would find the tool less valuable if the conver-                     and computers is harder than many people think. Per-\nsational interaction were removed (Section 5.3.2). Further, conver-                     haps it requires... new ways of configuring groups that\nsation seemed to provide unique value beyond other interaction                          include people and computers. And perhaps it needs\n\n\n\n\n                                                                           506\n\fThe Programmer\u2019s Assistant                                                                            IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n       more systematic, focused attention from researchers                the underlying LLM for conversational interaction, such as what\n       than it has, so far, received.\u201d [19, p.9]                          has been done with Lamda [91], is one opportunity to improve the\n                                                                          assistant\u2019s performance. Another opportunity is to align the lan-\n   We believe such evaluations of human-AI synergy should go\n                                                                          guage model to follow the desiderata proposed by Askell et al. [11]\nbeyond one-shot performance measures. As implied by many of the\n                                                                          and described by Ouyang et al. as, \u201chelpful (they should help the\nuses cases listed by Seeber et al. [80], human-centered AI systems\n                                                                          user solve their task), honest (they shouldn\u2019t fabricate information\nare often deployed in socio-organizational contexts that require\n                                                                          or mislead the user), and harmless (they should not cause physical,\nlongitudinal use [20, 41, 43], such as product design [93], game de-\n                                                                          psychological, or social harm to people or the environment)\u201d [61,\nsign [4], and engineering [20, Section 3.2.2]. Thus, we would expect\n                                                                          p.2]. Glaese et al. [33] propose a slightly different desiderata of\nthat over time and through interaction with each other, human-AI\n                                                                          \u201ccorrect\u201d instead of \u201chonest,\u201d which may be more applicable to the\nteams would improve their performance through a mutual learning\n                                                                          software engineering domain, as the ability to produce correct code\nprocess.\n                                                                          and correct answers about code are both important properties of a\n   Evidence for this process surfaced in our study when participants\n                                                                          conversational programming assistant.\ndescribed how they could improve their programming skills by\n                                                                             Combining LLMs with search-based approaches to establish ad-\ninteracting with the assistant (Section 5.3.3). We assert that the\n                                                                          ditional context for the model, such as AlphaCode [44] has done,\nlearning should operate in both directions: not only should people\n                                                                          may also result in more capable systems. These \u201csearches\u201d need not\nimprove their programming skills, but the model itself can also\n                                                                          be limited to textual sources, but could be conducted over appro-\nimprove based on peoples\u2019 interactions with it. For example, when\n                                                                          priate semantic stores (e.g. a knowledge graph) and take advantage\nthe assistant provides a code example to the user, and the user\n                                                                          of explicit semantic reasoning services, resulting in an integration\ntakes that example and edits it, those edits constitute feedback that\n                                                                          of symbolic and neural approaches. Further, allowing for \u201cinternal\ncan be used to further fine-tune the model. In addition, through\n                                                                          deliberation\u201d of the type shown in Nye et al. [59] could result in\nlongitudinal use, we believe that human and AI partners can create\n                                                                          better-reasoned results, as well as better explanations and justifica-\nreciprocal representations of one another \u2013 i.e., the human is likely\n                                                                          tions.\nto create a mental model of the AI, and the AI may be engineered to\n                                                                             Another avenue for improvement involves the prompt used to\ndevelop a user model for each of its human users [30, 48, 79]. Such a\n                                                                          configure the assistant (Appendix D). Just as the prompt for each\npair of models is often described as Mutual Theory of Mind [29, 100].\n                                                                          successive interaction is modified by the growth of the conversa-\nThis type of capability raises the possibility of personalizing and\n                                                                          tional transcript, there is no requirement that the initial prompt be\nadapting an assistant to the strengths and needs of individual users.\n                                                                          static. It too can be specialized to incorporate aspects of a user model,\n   With such models, an assistant that knows a user is learning a\n                                                                          enabling the realization of a Mutual Theory of Mind [29, 100]. Pro-\nprogramming language could provide natural language explana-\n                                                                          viding better UX affordances for visualizing and manipulating the\ntions alongside code outputs, whereas an assistant that knows a\n                                                                          active contexts \u2013 code and conversation \u2013 could provide users with\nuser is strongly skilled in a programming language might shorten\n                                                                          more control over which information contributes to the generation\nor omit those explanations. Similarly, users are likely to update\n                                                                          of the assistant\u2019s response.\ntheir mental models of the AI with more experience. We believe the\n                                                                             Our participants clearly indicated that they were interested in\nspace for exploring how these reciprocal models impact human-AI\n                                                                          having an assistant that behaved more proactively, in contrast to\nsynergy is rich, and we encourage additional work in this area.\n                                                                          our deliberate design of an assistant that never takes conversational\n   Human-centered AI systems that are designed to combine and\n                                                                          initiative. A more proactive assistant would be able to interrupt or\nsynergize the distinct skills of humans and AI models cannot suc-\n                                                                          remind a user when necessary [23], yet this characteristic raises\nceed if they diminish the human skills upon which they depend.\n                                                                          many challenging issues. How can we calibrate the threshold for\nWell-designed human-centered AI systems develop new and com-\n                                                                          such interruptions? How can users tune the assistant to deliver only\nplementary skills for both the human and AI constituents [82, 83],\n                                                                          those interruptions that the they would find useful (e.g., [28, 81])?\nand we believe that mutual learning may address concerns that the\n                                                                          How can we help users to regain their prior context after dealing\nwide deployment and use of AI systems will result in a de-skilling\n                                                                          with an interruption (e.g. [89])? Should an assistant be used to\nof the workforce [77, 108].\n                                                                          persuade or nudge the user (e.g. [35])? Who should determine\n   Ultimately, the design decisions that go into an interactive AI\n                                                                          the topic, frequency, and insistence of such persuasion attempts\nsystem have ethical implications. Our design attempts to augment\n                                                                          (e.g. [52, 85])? Should users have the ability to moderate or defeat\nthe user\u2019s knowledge and skills by presenting help on demand,\n                                                                          attempted persuasions, or should those decisions be left to the\ncouched in non-authoritative suggestions, which leaves the user\n                                                                          organization?\nfirmly in control and ultimately responsible for the work product.\n                                                                             Finally, we explored the different kinds of role orientations our\n                                                                          participants had toward the assistant and found that participants\n6.3     Opportunities for Future Research                                 varied in their views of it as a tool versus a social agent (e.g. collabo-\nOur work highlights many interesting avenues for future enhance-          rator or colleague). We posit that peoples\u2019 effectiveness in working\nments that could be made to LLM-based conversational assistants           with an AI system may be influenced by their role orientation, and\nsuch as our Programmer\u2019s Assistant, as well as future human-              we encourage future research in this area.\ncentered research on LLM-based conversational assistance.\n   Our work employed a code-fluent model that was not specifi-\ncally designed to handle conversational interaction. Fine-tuning\n\n\n\n\n                                                                    507\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                                                       Ross et al.\n\n\n7      CONCLUSION                                                                             [8] Craig Anslow, Stuart Marshall, James Noble, and Robert Biddle. 2013. Sourcevis:\n                                                                                                  Collaborative software visualization for co-located environments. In 2013 First\nWe developed a prototype system, the Programmer\u2019s Assistant, in                                   IEEE Working Conference on Software Visualization (VISSOFT). IEEE, 1\u201310.\norder to assess the utility of a conversational assistant in a software                       [9] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Naren-\n                                                                                                  dra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, Qian\nengineering context. The assistant was implemented using a state-                                 Pan, Christine T Wolf, et al. 2021. AI-Assisted Human Labeling: Batching for\nof-the-art code-fluent large language model, Codex [24], and was                                  Efficiency without Overreliance. Proceedings of the ACM on Human-Computer\ncapable of generating both code and natural language responses to                                 Interaction 5, CSCW1 (2021), 1\u201327.\n                                                                                             [10] Catherine A Ashworth. 1996. GUI Users have trouble using graphic conventions\nuser inquiries. We further used the prompting mechanism of the                                    on novel tasks. In Conference Companion on Human Factors in Computing Systems.\nmodel to set up a conversational interaction in which the model                                   75\u201376.\nuses the conversational history, plus the user\u2019s current utterance,                          [11] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom\n                                                                                                  Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.\nin order to generate a response. In this way, users are able to ask                               A general language assistant as a laboratory for alignment. arXiv preprint\nfollow-up questions in the chat that reference prior utterances and                               arXiv:2112.00861 (2021).\n                                                                                             [12] Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018. Measuring the utility of\nresponses. We incorporated the conversational assistant into a code                               search engine result pages: an information foraging based measure. In The 41st\nediting environment, enabling the conversation to be grounded in                                  International ACM SIGIR conference on research & development in information\nthe context of the user\u2019s source code.                                                            retrieval. 605\u2013614.\n                                                                                             [13] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. Grounded\n   We evaluated this system with 42 participants with varied levels                               Copilot: How Programmers Interact with Code-Generating Models. arXiv\nof programming skill, and their quantitative and qualitative feed-                                preprint arXiv:2206.15000 (2022).\nback, coupled with their usage of the system, demonstrated the                               [14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\n                                                                                                  Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nvaried, and sometimes emergent, types of assistance it was able to                                Brunskill, et al. 2021. On the opportunities and risks of foundation models.\nprovide. Many participants noted the high quality of the conversa-                                arXiv preprint arXiv:2108.07258 (2021).\n                                                                                             [15] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R Klemmer. 2010.\ntional responses, including the assistant\u2019s ability to produce code,                              Example-centric programming: integrating web search into the development\nexplain code, answer general programming questions, and even                                      environment. In Proceedings of the SIGCHI Conference on Human Factors in\nanswer general knowledge questions. Participants felt this type of                                Computing Systems. 513\u2013522.\n                                                                                             [16] Virginia Braun and Victoria Clarke. 2022. Common challenges in Thematic\nassistance would aid their productivity, and they drew meaningful                                 Analysis and how to avoid them. Retrieved August 11 2022 from https://youtu.\ncontrasts between the conversational style of interaction with other                              be/tpWLsckpM78\ntools that employ a direct manipulation or search-based interaction                          [17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\n                                                                                                  plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nmodel.                                                                                            Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\n   Our study motivates the use of conversational styles of inter-                                 Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\n                                                                                                  Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\naction with large language models by showing how they enable                                      Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nemergent behaviors in a co-creative context. The Programmer\u2019s                                     Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are\nAssistant did not always generate perfect code or correct answers;                                Few-Shot Learners. In Advances in Neural Information Processing Systems,\n                                                                                                  H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33.\nnonetheless, participants in our study had an overall positive ex-                                Curran Associates, Inc., 1877\u20131901. https://proceedings.neurips.cc/paper/2020/\nperience working with it on a variety of programming challenges.                                  file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nWe believe that our work takes us one step closer to realizing the                           [18] Sallyann Bryant, Pablo Romero, and Benedict\" du Boulay. 2006. The Collabora-\n                                                                                                  tive Nature of Pair Programming. In Extreme Programming and Agile Processes\nvision of human-centered AI: learning how to design systems that                                  in Software Engineering, Pekka Abrahamsson, Michele Marchesi, and Giancarlo\nmaximize the synergy in human-AI collaborations.                                                  Succi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 53\u201364.\n                                                                                             [19] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, Abdullah\n                                                                                                  Almaatouq, and Thomas W Malone. 2022. A Test for Evaluating Performance\nACKNOWLEDGMENTS                                                                                   in Human-Computer Systems. arXiv preprint arXiv:2206.12390 (2022).\n                                                                                             [20] Gaetano Cascini, Yukari Nagai, Georgi V Georgiev, Jader Zelaya, Niccol\u00f2 Be-\nWe would like to thank Socrates for his tireless assistance during                                cattini, Jean-Fran\u00e7ois Boujut, Hernan Casakin, Nathan Crilly, Elies Dekoninck,\nthe user study, as well as for suggesting the title of this paper based                           John Gero, et al. 2022. Perspectives on design creativity and innovation research:\n                                                                                                  10 years later. , 30 pages.\non its abstract.                                                                             [21] Stephen Cass. 2022. Top Programming Languages 2022. IEEE Spectrum (23 Aug\n                                                                                                  2022). https://spectrum.ieee.org/top-programming-languages-2022\n                                                                                             [22] Cristina Catalan Aguirre, Nuria Gonzalez Castro, Carlos Delgado Kloos, Carlos\nREFERENCES                                                                                        Alario-Hoyos, and Pedro Jos\u00e9 Mu\u00f1oz Merino. 2021. Conversational agent for\n    [1] Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What Do Developers              supporting learners on a MOOC on programming with Java. (2021).\n        Use the Crowd For? A Study Using Stack Overflow. IEEE Software 34, 2 (2017),         [23] Ana Paula Chaves and Marco Aurelio Gerosa. 2021. How should my chatbot\n        53\u201360. https://doi.org/10.1109/MS.2017.31                                                 interact? A survey on social characteristics in human\u2013chatbot interaction design.\n    [2] Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technol-              International Journal of Human\u2013Computer Interaction 37, 8 (2021), 729\u2013758.\n        ogy, and applications. Machine Learning with Applications 2 (2020), 100006.          [24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared\n    [3] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel,                Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex\n        Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,                  Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\n        and Quoc V. Le. 2020. Towards a Human-like Open-Domain Chatbot.                           try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\n    [4] Safinah Ali, Nisha Elizabeth Devasia, and Cynthia Breazeal. 2022. Escape! Bot:            Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\n        Social Robots as Creative Problem-Solving Partners. In Creativity and Cognition.          Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios\n        275\u2013283.                                                                                  Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,\n    [5] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.            Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua\n        A survey of machine learning for big code and naturalness. ACM Computing                  Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight,\n        Surveys (CSUR) 51, 4 (2018), 1\u201337.                                                        Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\n    [6] Irene Alvarado, Idan Gazit, and Amelia Wattenberger. 2022. GitHub Next |                  Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-\n        GitHub Copilot Labs. https://githubnext.com/projects/copilot-labs/                        ating a Large Language Models Trained on Code.\n    [7] Hikari Ando, Rosanna Cousins, and Carolyn Young. 2014. Achieving saturation          [25] Li-Te Cheng, R.B. De Souza, Susanne Hupfer, John Patter-\n        in thematic analysis: Development and refinement of a codebook. Comprehensive             son, and Steven Ross. 2003.               Building Collaboration into IDEs:\n        Psychology 3 (2014), 03\u2013CP.                                                               Edit>Compile>Run>Debug>Collaborate? Queue 1, 9 (2003).\n\n\n\n\n                                                                                       508\n\fThe Programmer\u2019s Assistant                                                                                                    IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n [26] Carl Cook, Warwick Irwin, and Neville Churcher. 2005. A user evaluation                 [48] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, and Janis A\n      of synchronous collaborative software engineering tools. In 12th Asia-Pacific                Cannon-Bowers. 2000. The influence of shared mental models on team process\n      Software Engineering Conference (APSEC\u201905). IEEE, 6\u2013pp.                                      and performance. Journal of applied psychology 85, 2 (2000), 273.\n [27] Claudio Le\u00f3n de la Barra, Broderick Crawford, Ricardo Soto, Sanjay Misra,               [49] Cade Metz. 2022. Meet GPT-3. It Has Learned to Code (and Blog and Ar-\n      and Eric Monfroy. 2013. Agile Software Development: It Is about Knowledge                    gue). (Published 2020). https://www.nytimes.com/2020/11/24/science/artificial-\n      Management and Creativity. In Computational Science and Its Applications \u2013                   intelligence-ai-gpt3.html\n      ICCSA 2013, Beniamino Murgante, Sanjay Misra, Maurizio Carlini, Carmelo M.              [50] Robert J. Moore and Raphael Arar. 2019. Conversational UX Design: A Practi-\n      Torre, Hong-Quang Nguyen, David Taniar, Bernady O. Apduhan, and Osvaldo                      tioner\u2019s Guide to the Natural Conversation Framework. Association for Computing\n      Gervasi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 98\u2013113.                      Machinery, New York, NY, USA.\n [28] Uri Dekel and Steven Ross. 2004. Eclipse as a platform for research on in-              [51] Ekaterina A Moroz, Vladimir O Grizkevich, and Igor M Novozhilov. 2022. The\n      terruption management in software development. In Proceedings of the 2004                    Potential of Artificial Intelligence as a Method of Software Developer\u2019s Produc-\n      OOPSLA workshop on Eclipse Technology eXchange (Vancouver, British Columbia,                 tivity Improvement. In 2022 Conference of Russian Young Researchers in Electrical\n      Canada), Michael G. Burke (Ed.). ACM, 12\u201316.                                                 and Electronic Engineering (ElConRus). IEEE, 386\u2013390.\n [29] Bobbie Eicher, Kathryn Cunningham, Sydni Peterson Marissa Gonzales, and                 [52] Michael Muller, Stevean Ross, Stephanie Houde, Mayank Agarwal, Fernando\n      Ashok Goel. 2017. Toward mutual theory of mind as a foundation for co-creation.              Martinez, John Richards, Kartik Talamadupula, and Justin D Weisz. 2022. Drink-\n      In International Conference on Computational Creativity, Co-Creation Workshop.               ing Chai with Your (AI) Programming Partner: A Design Fiction about Gener-\n [30] Stephen M Fiore, Eduardo Salas, and Janis A Cannon-Bowers. 2001. Group                       ative AI for Software Engineering. HAI-GEN Workshop at IUI 2022: 3rd Work-\n      dynamics and shared mental model development. How people evaluate others in                  shop on Human-AI Co-Creation with Generative Models (2022). https://hai-\n      organizations 234 (2001).                                                                    gen.github.io/2022/\n [31] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design              [53] Sandra R Murillo and J Alfredo S\u00e1nchez. 2014. Empowering interfaces for system\n      patterns: elements of reusable object-oriented software. Addison-Wesley.                     administrators: Keeping the command line in mind when designing GUIs. In\n [32] GitHub, Inc. 2022. GitHub copilot \u00b7 your AI pair programmer. Retrieved August                Proceedings of the XV International Conference on Human Computer Interaction.\n      5, 2022 from https://github.com/features/copilot/                                            1\u20134.\n [33] Amelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu,                 [54] Elizabeth D Mynatt and Gerhard Weber. 1994. Nonvisual presentation of graph-\n      Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe                         ical user interfaces: contrasting two approaches. In Proceedings of the SIGCHI\n      Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona                     conference on Human factors in computing systems. 166\u2013172.\n      Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie                [55] Alok Mysore and Philip J Guo. 2017. Torta: Generating mixed-media gui and\n      Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u0148a Mokr\u00e1, Nicholas                   command-line app tutorials using operating-system-wide activity tracing. In\n      Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac,               Proceedings of the 30th Annual ACM Symposium on User Interface Software and\n      John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and                     Technology. 703\u2013714.\n      Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted              [56] C. Nass and Y. Moon. 2000. Machines and Mindlessness: Social Responses to\n      human judgements. https://arxiv.org/abs/2209.14375                                           Computers. Journal of Social Issues 56, 1 (2000), 81\u2013103.\n [34] Stephanie Glen. 2022. ChatGPT writes code, but won\u2019t replace devel-                     [57] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copi-\n      opers.      TechTarget (14 12 2022).        Retrieved 20-Jan-2023 from https:                lot\u2019s Code Suggestions. In 2022 IEEE/ACM 19th International Conference on\n      //www.techtarget.com/searchsoftwarequality/news/252528379/ChatGPT-                           Mining Software Repositories (MSR). IEEE, 1\u20135.\n      writes-code-but-wont-replace-developers                                                 [58] Martin Nordio, H Estler, Carlo A Furia, Bertrand Meyer, et al. 2011. Collaborative\n [35] Samuel Holmes, Anne Moorhead, Raymond Bond, Huiru Zheng, Vivien Coates,                      software development on the web. arXiv preprint arXiv:1105.0768 (2011).\n      and Mike McTear. 2018. WeightMentor: a new automated chatbot for weight                 [59] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Witold Michalewski, Ja-\n      loss maintenance. In Proceedings of the 32nd International BCS Human Computer                cob Austin, David Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten Paul\n      Interaction Conference 32. 1\u20135.                                                              Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your\n [36] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment                      Work: Scratchpads for Intermediate Computation with Language Models.\n      generation with hybrid lexical and syntactical information. Empirical Software               https://arxiv.org/abs/2112.00114.\n      Engineering 25, 3 (2020), 2179\u20132217.                                                    [60] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAI\n [37] Edwin L Hutchins, James D Hollan, and Donald A Norman. 1985. Direct manip-                   Blog (30 11 2022). Retrieved 20-Jan-2023 from https://openai.com/blog/chatgpt/\n      ulation interfaces. Human\u2013computer interaction 1, 4 (1985), 311\u2013338.                    [61] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\n [38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.                  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\n      Summarizing source code using a neural attention model. In Proceedings of the                Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\n      54th Annual Meeting of the Association for Computational Linguistics (Volume 1:              Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training\n      Long Papers). 2073\u20132083.                                                                     language models to follow instructions with human feedback. https://arxiv.\n [39] Andreas Jedlitschka and Markus Nick. 2003. Software Engineering Knowledge                    org/abs/2203.02155\n      Repositories. Springer Berlin Heidelberg, Berlin, Heidelberg, 55\u201380.                    [62] Peter Pirolli and Stuart Card. 1999. Information foraging. Psychological review\n [40] Eirini Kalliamvakou. 2022. Research: Quantifying github copilot\u2019s impact                     106, 4 (1999), 643.\n      on developer productivity and happiness.         https://github.blog/2022-09-07-        [63] Larry Press. 1990. Personal computing: Windows, DOS and the MAC. Commun.\n      research-quantifying-github-copilots-impact-on-developer-productivity-                       ACM 33, 11 (1990), 19\u201326.\n      and-happiness/                                                                          [64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\n [41] Anna Kantosalo et al. 2019. Human-Computer Co-Creativity: Designing, Evalu-                  Sutskever, et al. 2019. Language Models are Unsupervised Multitask Learners.\n      ating and Modelling Computational Collaborators for Poetry Writing. (2019).             [65] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. 2019. Machine learning in\n [42] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade-                     medicine. New England Journal of Medicine 380, 14 (2019), 1347\u20131358.\n      Offs for Substituting a Human with an Agent in a Pair Programming Context:              [66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n      The Good, the Bad, and the Ugly. In Proceedings of the 2021 CHI Conference on                2022. Hierarchical text-conditional image generation with clip latents. arXiv\n      Human Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association                  preprint arXiv:2204.06125 (2022).\n      for Computing Machinery, New York, NY, USA, Article 243, 20 pages.                      [67] B. Reeves and C.I. Nass. 1996. The Media Equation: How People Treat Computers,\n [43] Lauramaria Laine. 2021. Exploring Advertising Creatives\u2019 Attitudes Towards                   Television, and New Media Like Real People and Places. CSLI Publications.\n      Human-AI Collaboration. (2021).                                                         [68] Mawarny Md Rejab, James Noble, and George Allan. 2014. Distributing Expertise\n [44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,                    in Agile Software Development Projects. In 2014 Agile Conference. 33\u201336.\n      R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,                [69] Jeba Rezwana and Mary Lou Maher. 2021. COFI: A Framework for Modeling\n      and et al. 2022. Competition-level code generation with AlphaCode. https:                    Interaction in Human-AI Co-Creative Systems.. In ICCC. 444\u2013448.\n      //arxiv.org/abs/2203.07814                                                              [70] Charles H. Rich and Richard C. Waters. 1990. The Programmer\u2019s Apprentice.\n [45] Yaosheng Lou and Qi Sun. 2021. Over-reliance on database: A case study of                    Addison-Wesley Publishing Company, Reading, MA.\n      using web of science. Human Behavior and Emerging Technologies 3, 3 (2021),             [71] Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddy\u2014A Conver-\n      454\u2013459.                                                                                     sational Agent for Pair Programming. ACM Transactions on Computer-Human\n [46] David Lyell and Enrico Coiera. 2017. Automation bias and verification complex-               Interaction (TOCHI) 29, 4 (2022), 1\u201344.\n      ity: a systematic review. Journal of the American Medical Informatics Association       [72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\n      24, 2 (2017), 423\u2013431.                                                                       Ommer. 2022. High-resolution image synthesis with latent diffusion models. In\n [47] Wendy E Mackay and Anne-Laure Fayard. 1997. HCI, natural science and                         Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\n      design: a framework for triangulation across disciplines. In Proceedings of the              tion. 10684\u201310695.\n      2nd conference on Designing interactive systems: processes, practices, methods, and\n      techniques. 223\u2013234.\n\n\n\n\n                                                                                        509\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                                                          Ross et al.\n\n\n [73] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. A Multiple-                         Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\n      Application Conversational Agent. In Proceedings of the 9th International Con-                 Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n      ference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI \u201904). Asso-           (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/\n      ciation for Computing Machinery, New York, NY, USA, 319\u2013321.                                   2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n [74] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. Voice User Interface           [97] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and\n      Principles for a Conversational Agent. In Proceedings of the 9th International                 Philip S Yu. 2018. Improving automatic source code summarization via deep rein-\n      Conference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI \u201904).              forcement learning. In Proceedings of the 33rd ACM/IEEE international conference\n      Association for Computing Machinery, New York, NY, USA, 364\u2013365.                               on automated software engineering. 397\u2013407.\n [75] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample.              [98] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D\n      2020. Unsupervised Translation of Programming Languages. In Advances in                        Weisz, Xuye Liu, Lingfei Wu, and Casey Dugan. 2022. Documentation Matters:\n      Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,                  Human-Centered AI System to Assist Data Science Code Documentation in\n      M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 20601\u201320611.                 Computational Notebooks. ACM Transactions on Computer-Human Interaction\n [76] Harvey Sacks. 1984. Notes on methodology. In Structures of Social Action:                      29, 2 (2022), 1\u201333.\n      Studies in Conversation Analysis, John Heritage and J. Maxwell Atkinson (Eds.).           [99] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer,\n      Cambridge University Press, Cambridge, 2\u201327.                                                   Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019.\n [77] Nithya Sambasivan and Rajesh Veeraraghavan. 2022. The Deskilling of Domain                     Human-AI collaboration in data science: Exploring data scientists\u2019 perceptions\n      Expertise in AI Development. In CHI Conference on Human Factors in Computing                   of automated AI. Proceedings of the ACM on Human-Computer Interaction 3,\n      Systems. 1\u201314.                                                                                 CSCW (2019), 1\u201324.\n [78] Harini Sampath, Alice Merrick, and Andrew Macvean. 2021. Accessibility of                [100] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.\n      command line interfaces. In Proceedings of the 2021 CHI Conference on Human                    Towards mutual theory of mind in human-ai interaction: How language reflects\n      Factors in Computing Systems. 1\u201310.                                                            what students perceive about a virtual teaching assistant. In Proceedings of the\n [79] Matthias Scheutz, Scott A DeLoach, and Julie A Adams. 2017. A framework for                    2021 CHI Conference on Human Factors in Computing Systems. 1\u201314.\n      developing and using shared mental models in human-agent teams. Journal of               [101] Jeremy Warner and Philip J Guo. 2017. Codepilot: Scaffolding end-to-end\n      Cognitive Engineering and Decision Making 11, 3 (2017), 203\u2013224.                               collaborative software development for novice programmers. In Proceedings of\n [80] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-Jan                    the 2017 CHI Conference on Human Factors in Computing Systems. 1136\u20131141.\n      De Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah Oeste-Rei\u00df,               [102] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,\n      Nils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in                   Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per-\n      team collaboration. Information & management 57, 2 (2020), 103174.                             fection Not Required? Human-AI Partnerships in Code Translation. In 26th\n [81] Shilad Sen, Werner Geyer, Michael Muller, Marty Moore, Beth Brownholtz, Eric                   International Conference on Intelligent User Interfaces. 402\u2013412.\n      Wilcox, and David R Millen. 2006. FeedMe: a collaborative alert filtering system.        [103] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie\n      In Proceedings of the 2006 20th anniversary conference on Computer supported                   Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards. 2022. Bet-\n      cooperative work. 89\u201398.                                                                       ter together? an evaluation of ai-supported code translation. In 27th International\n [82] Ben Shneiderman. 2020. Human-centered artificial intelligence: Three fresh                     Conference on Intelligent User Interfaces. 369\u2013391.\n      ideas. AIS Transactions on Human-Computer Interaction 12, 3 (2020), 109\u2013124.             [104] Joseph Weizenbaum. 1966. ELIZA \u2014 a computer program for the study of\n [83] Ben Shneiderman. 2022. Human-Centered AI. Oxford University Press.                             natural language communication between man and machine. Commun. ACM 9\n [84] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller,             (1966), 36\u201345.\n      Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. BlenderBot 3:              [105] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation\n      a deployed conversational agent that continually learns to responsibly engage.                 from natural language: Promise and challenges. ACM Transactions on Software\n      arXiv preprint arXiv:2208.03188 (2022).                                                        Engineering and Methodology (TOSEM) 31, 2 (2022), 1\u201347.\n [85] Michael Skirpan and Casey Fiesler. 2018. Ad empathy: A design fiction. In                [106] Aditya Ankur Yadav, Ishan Garg, and Dr. Pratistha Mathur. 2019. PACT - Pro-\n      Proceedings of the 2018 ACM Conference on Supporting Groupwork. 267\u2013273.                       gramming Assistant ChaTbot. In 2019 2nd International Conference on Intelligent\n [86] Diomidis Spinellis. 2012. Git. IEEE Software 29, 3 (2012), 100\u2013101. https:                     Communication and Computational Techniques (ICCT). 131\u2013136.\n      //doi.org/10.1109/MS.2012.61                                                             [107] Munazza Zaib, Quan Z. Sheng, and W. Zhang. 2020. A Short Survey of Pre-\n [87] Angie Spoto and Natalia Oleynik. 2017. Library of Mixed-Initiative Creative                    trained Language Models for Conversational AI-A New Age in NLP. Proceedings\n      Interfaces. Retrieved 19-Jun-2021 from http://mici.codingconduct.cc/                           of the Australasian Computer Science Week Multiconference (2020).\n [88] Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh.                 [108] Elaine Zibrowski, Lisa Shepherd, Kamran Sedig, Richard Booth, Candace Gibson,\n      2021. Actionable UI Design Guidelines for Smartphone Applications Inclusive                    et al. 2018. Easier and faster is not always better: grounded theory of the impact\n      of Low-Literate Users. Proceedings of the ACM on Human-Computer Interaction                    of large-scale system transformation on the clinical work of emergency medicine\n      5, CSCW1 (2021), 1\u201330.                                                                         nurses and physicians. JMIR Human Factors 5, 4 (2018), e11013.\n [89] Margaret-Anne Storey and Alexey Zagalsky. 2016. Disrupting developer produc-             [109] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin,\n      tivity one bot at a time. In Proceedings of the 2016 24th ACM SIGSOFT international            Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Produc-\n      symposium on foundations of software engineering. 928\u2013931.                                     tivity Assessment of Neural Code Completion. In Proceedings of the 6th ACM\n [90] Kartik Talamadupula. 2021. Applied AI matters: AI4Code: applying artificial                    SIGPLAN International Symposium on Machine Programming (San Diego, CA,\n      intelligence to source code. AI Matters 7, 1 (2021), 18\u201320.                                    USA) (MAPS 2022). Association for Computing Machinery, New York, NY, USA,\n [91] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-                      21\u201329. https://doi.org/10.1145/3520312.3534864\n      shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, and\n      et al. 2022. LAMDA: Language models for dialog applications.                  https:\n      //arxiv.org/abs/2201.08239\n [92] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel\n      Sundaresan. 2020. Unit Test Case Generation with Transformers and Focal\n      Context. arXiv preprint arXiv:2009.05617 (2020).\n [93] Severi Uusitalo, Anna Kantosalo, Antti Salovaara, Tapio Takala, and Christian\n      Guckelsberger. 2022. Co-creative Product Design with Interactive Evolutionary\n      Algorithms: A Practice-Based Reflection. In International Conference on Compu-\n      tational Intelligence in Music, Sound, Art and Design (Part of EvoStar). Springer,\n      292\u2013307.\n [94] Priyan Vaithilingam and Philip J Guo. 2019. Bespoke: Interactively synthesizing\n      custom GUIs from command-line applications by demonstration. In Proceedings\n      of the 32nd annual ACM symposium on user interface software and technology.\n      563\u2013576.\n [95] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\n      vs. Experience: Evaluating the Usability of Code Generation Tools Powered by\n      Large Language Models. In Extended Abstracts of the 2022 CHI Conference on\n      Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922).\n      Association for Computing Machinery, New York, NY, USA, Article 332, 7 pages.\n      https://doi.org/10.1145/3491101.3519665\n [96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n      Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\n\n\n\n\n                                                                                         510\n\fThe Programmer\u2019s Assistant                                                                           IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\nA SURVEY INSTRUMENTS                                                               Before trying out the Programmer\u2019s Assistant, we\n                                                                                   would like to assess some of your expectations. We\nA.1 Screening Survey\n                                                                                   estimate that this survey will take 5 minutes.\nThe questions below were asked of prospective participants to\nunderstand their job role, Python experience, and familiarity with                 By submitting this survey, you consent to participate\nGitHub Copilot. The questions on Python experience were modeled                    in our study. If you would like to withdraw your con-\nafter those used by Weisz et al. [103].                                            sent, please email us at [removed].\n1. Do you consider yourself primarily a...\n   \u2022 Data Scientist                                                                Thanks!\n   \u2022 Manager                                                               1. Based on your past experience using chatbots, please let us know\n   \u2022 Software Architect                                                    how you would anticipate an AI chatbot serving as a programmer\u2019s\n   \u2022 Software Engineer                                                     assistant to perform. Do you expect it will:\n   \u2022 Machine Learning Engineer                                                  Scale: Not at all, A little, Somewhat, A great deal\n   \u2022 Other: write-in                                                          \u2022 Be easy to use?\n2. To what extent are you familiar with Python?                               \u2022 Understand your requests?\n   \u2022 I am not familiar with Python                                            \u2022 Provide high quality responses?\n   \u2022 I have < 1 year of experience with Python                                \u2022 Help you to write better code?\n   \u2022 I have 1-3 years experience with Python                                  \u2022 Help you to write code more quickly?\n   \u2022 I have 3+ years of experience with Python                                \u2022 Be enjoyable to use?\n3. How recently have you written Python code?                              2. Any other expectations?\n   \u2022 Within the past month                                                      Open-ended response\n   \u2022 Within the past year\n   \u2022 Within the past 5 years                                               A.3     Post-task Survey\n   \u2022 Have not written Python code within the past 5 years                  The questions below were asked after a participant used the Pro-\n4. To what extent are you familiar with GitHub Copilot?                    grammer\u2019s Assistant to complete the programming challenges. This\n   \u2022 I am not familiar with Copilot                                        survey took approximately 10-15 minutes to complete.\n   \u2022 I\u2019ve seen demos and/or read about Copilot\n   \u2022 I\u2019ve tried out Copilot                                                A.3.1   Reflections.\n   \u2022 I\u2019ve used Copilot as a tool for my work                               1. Based on your experience using the Programmer\u2019s Assistant to\n                                                                           complete the programming challenges in this study, how would\nA.2     Pre-task Survey                                                    you characterize the experience? Did you find that it:\nThe questions below were asked before a participant used the Pro-               Scale: Not at all, A little, Somewhat, A great deal\ngrammer\u2019s Assistant to assess their expectations of a conversational          \u2022 Was easy to use?\nprogramming assistant. This survey took approximately 5 minutes               \u2022 Understand your requests?\nto complete and began with the instructions below:                            \u2022 Provided high quality responses?\n                                                                              \u2022 Helped you to write better code?\n      Hello! We are a team of researchers looking for feed-\n                                                                              \u2022 Helped you to write code more quickly?\n      back on a prototype system we call the Program-\n                                                                              \u2022 Provided an enjoyable experienced?\n      mer\u2019s Assistant.\n                                                                           2. To what extent did you view the Programmer\u2019s Assistant as:\n       The Programmer\u2019s Assistant is an experiment in con-                      Scale: Not at all, A little, Somewhat, A great deal\n       versational coding: it consists of a code editor in-                   \u2022 A tool\n       tegrated with a chatbot that is able to converse in                    \u2022 A reference guide\n       natural language to answer questions, generate code,                   \u2022 A content generator\n       and consult on existing code.                                          \u2022 A problem solver\n                                                                              \u2022 A collaborator\n       In this study, you will be asked to complete several                   \u2022 A colleague\n       programming tasks. We are not evaluating your                          \u2022 A coach\n       programming skills on these tasks. Rather, we are                      \u2022 An advisor\n       interested in understanding how the Programmer\u2019s                       \u2022 A reviewer\n       Assistant is able to help you accomplish those tasks.               3. How important were these aspects of working with the Program-\n       Your code and interactions with the assistant will be               mer\u2019s Assistant:\n       processed by a 3rd party AI model, so please do not                      Scale: Not at all, A little, Somewhat, A great deal\n       include proprietary code or discuss company-                           \u2022 Ability to ask followup questions on the same topic across\n       confidential information. All data we collect in                    multiple conversational turns\n       this study will be anonymized before it is published.                  \u2022 Ability to ask questions or make requests that reference selec-\n                                                                           tions in the code editor\n\n\n\n\n                                                                     511\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                   Ross et al.\n\n\n   \u2022 Ability to ask for alternate responses (Try Again)                           Listing 4: The Programmer\u2019s Assistant study tutorial\n   \u2022 Ability to clear the conversational context (Start Over)                 1   \"\"\"\n                                                                              2   TUTORIAL :\n4. What stood out to you about the experience of using the Program-           3\nmers Assistant? For example, was anything good,bad, surprising,               4   As a warmup activity , please work through the 10\nor notable?                                                                   5   exercises below . Type or paste your results right\n                                                                              6   into the text and submit your responses when done .\n      Open-ended response                                                     7\n5. How would you compare using the Programmer\u2019s Assistant as a                8   1) View the help page for the programmer ' s\n                                                                              9      assistant by clicking on the question mark to\ncoding aide to searching the web (e.g. Google, Stack Overflow)?              10      the right of your email address at the top of\n      Open-ended response                                                    11      the browser window .\n                                                                             12\n6. If you have used the commercial AI programming tool called                13   2) Introduce yourself to the assistant . Tell it\nGitHub Copilot, how would you compare it with using the Pro-                 14      your name .\ngrammer\u2019s Assistant?                                                         15      For example : \" Hello . My name is Stephanie .\"\n                                                                             16\n      Open-ended response                                                    17      Did it seem to understand ? :\n7. Having used the Programmer\u2019s Assistant, how did it compare                18\n                                                                             19\nwith your initial expectations?                                              20   3) You can use the assistant to get help on how to\n      Open-ended response                                                    21      accomplish particular programming tasks . Try it\n                                                                             22      out !\nA.3.2 Value.                                                                 23      For example : \" How do I read a csv file ?\"\n                                                                             24      or :          \" How do I merge two dictionaries ?\"\n8. How valuable would the Programmer\u2019s Assistant be for your work            25      or :          \" How do I remove duplicate items\nif it could be added to your favorite development environment?               26                      from a list ?\"\n                                                                             27\n       Scale: (No value at all) 1 2 3 4 5 6 7 8 9 10 (An essential tool)     28      Feel free to try your own !\n9. Why?                                                                      29\n                                                                             30      Did it successfully answer your questions ? :\n       Open-ended response                                                   31\n10. How would the following changes impact the value of the Pro-             32   4) The assistant can also write whole functions\n                                                                             33      for you . Ask the assistant to write a factorial\ngrammer\u2019s Assistant?                                                         34      function . Paste the result below .\n       Scale: Less valuable, No change in value, More valuable               35      For example : \" Write a function that returns the\n    \u2022 Eliminate the conversation and make the Programmer\u2019s Assis-            36      factorial of its input .\"\n                                                                             37\ntant behave more like a search box (e.g. without the conversational          38      Result :    ( tip - you can copy an inline response\ncontext).                                                                    39      ( in black ) by clicking on the associated copy\n    \u2022 Add buttons in the chat UI for common queries, such as \u201cwhat           40      icon )\n                                                                             41\ndoes this code do?\u201d or \u201cdocument this code.\u201d                                 42\n    \u2022 Have the Programmer\u2019s Assistant examine your code and make             43     Did it do it correctly ? :\n                                                                             44\nproactive suggestions for improving it in the chat.                          45   5) Select the code below and ask the system to\n    \u2022 Have the Programmer\u2019s Assistant examine your code and make             46       describe what it does . You don 't need to\nproactive suggestions for improvements in comments inserted di-              47       copy and paste the code to the chat . The\n                                                                             48       assistant can see whatever is selected when you\nrectly into the code.                                                        49       make a chat entry . Aside from the selection ,\n11. Do you have any other suggestions for how we could improve               50       the assistant does not monitor your activity in\n                                                                             51       the code editor nor give unsolicited advice .\nthe experience of working with the Programmer\u2019s Assistant?                   52       For example : \" What does this code do ?\"\n       Open-ended response                                                   53   \"\"\"\n                                                                             54\nA.3.3 Demographics.                                                          55   def convert ( n ) :\n                                                                             56       T = \"0123456789 ABCDEF \"\n12. To which gender identity do you most identify?                           57       q , r = divmod (n , 16)\n   \u2022 Male                                                                    58       if q == 0:\n                                                                             59           return T [ r ]\n   \u2022 Female                                                                  60       else :\n   \u2022 Transgender Male                                                        61           return convert ( q ) + T [ r ]\n   \u2022 Transgender Female                                                      62   \"\"\"\n                                                                             63\n   \u2022 Gender Variant/Non-conforming                                           64      What did it say :\n   \u2022 Other: write-in                                                         65\n   \u2022 Prefer not to answer                                                    66      Was it right ? :\n                                                                             67\n                                                                             68   6) Ask it to explain what the divmod line is\nB     THE PROGRAMMER\u2019S ASSISTANT                                             69      doing . The assistant maintains the context of\n                                                                             70      the conversation .\n      TUTORIAL                                                               71      For example : \" What is the divmod line doing ?\"\nThe tutorial provided to study participants, like all the challenges,        72\n                                                                             73      What did it say ? :\nwas presented as pre-loaded text in the code editor. Participants            74\nwere encouraged to modify the text to record their results and               75      Was that a good answer ? :\n                                                                             76\nsubmit it at the completion of the tutorial.\n\n\n\n\n                                                                       512\n\f The Programmer\u2019s Assistant                                                                           IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\n\n 77   7) See if the assistant remembers your name                           13   Trend \".\n 78      For example \" What ' s my name ?\"                                  14   \"\"\"\n 79\n 80      Did it ? :\n 81                                                                                Listing 7: Challenge 3: Creating documentation\n 82   8) Click the \" try again \" button at the top of the\n 83      chat . You should get a different answer .                          1   \"\"\"\n 84      Try it a few times .                                                2   Challenge #3 - Creating Documentation\n 85                                                                          3   Document this function\n 86      Did it ever get your name right ?:                                  4   \"\"\"\n 87                                                                          5   from collections import defaultdict\n 88      If the assistant gives you an answer that is                        6   import heapq as heap\n 89      obviously wrong or it claims to not know an                         7\n 90      answer that you think it should know , or you                       8   def analyze (G , startingNode ) :\n 91      just want to see an alternate answer , it is                        9       visited = set ()\n 92      worth it to give \" try again \" a shot .                            10       parentsMap = {}\n 93                                                                         11       pq = []\n 94                                                                         12       nodeCosts = defaultdict ( lambda : float ( ' inf ') )\n 95   9) Click the \" start over \" button at the top of the                  13       nodeCosts [ startingNode ] = 0\n 96      chat , and then enter another command to see                       14       heap . heappush ( pq , (0 , startingNode ) )\n 97      if it remembers your name .                                        15\n 98      For example \" What ' s my name ?\"                                  16       while pq :\n 99                                                                         17           _ , node = heap . heappop ( pq )\n100      Did it ? :                                                         18           visited . add ( node )\n101                                                                         19\n102     It should really have forgotten your name now ,                     20           for adjNode , weight in G [ node ]. items () :\n103     and no amount of \" trying again \" will get it                       21               if adjNode in visited :\n104     right . You can \" start over \" if the assistant                     22                   continue\n105     ever seems confused by , or stuck on , earlier                      23               newCost = nodeCosts [ node ] + weight\n106     parts of the conversation .                                         24               if nodeCosts [ adjNode ] > newCost :\n107                                                                         25                   parentsMap [ adjNode ] = node\n108   10) You can chat with the assistant on any topic                      26                   nodeCosts [ adjNode ] = newCost\n109       you like to explore its functionality and                         27                   heap . heappush ( pq , ( newCost ,\n110       capabilities further . See if you can stump it                                               adjNode ) )\n111       with a tough question !                                           28       return parentsMap , nodeCosts\n112\n113   Thanks !\n114                                                                                          Listing 8: Challenge 4: Testing\n115   When you are done , submit your results by clicking                    1   \"\"\"\n116   on the blue submit button and move on to the                           2   Challenge #4 - Testing\n117   challenges !!!                                                         3   Create a unit test for this function\n118   \"\"\"                                                                    4   \"\"\"\n                                                                             5   def euclid (a , b ) :\n                                                                             6     while b != 0:\n C     CHALLENGES                                                            7       (a , b ) = (b , a % b )\n                                                                             8     return a\n Each of the study challenges was presented as text in the code              9\n editor. Participants completed their work in the code editor and           10   # Unit test for euclid function\n then submitted it when finished. The prototype did not provide any\n ability to run or debug code and participants were encouraged to\n                                                                            D     PROGRAMMER\u2019S ASSISTANT PROMPT\n make their best attempt at solving each challenge.\n                                                                            Listing 9 shows the initial prompt sent to Codex to configure it as\n           Listing 5: Challenge 1: Program generation                       a conversational agent. On subsequent exchanges, the prompt was\n  1                                                                         augmented with a transcript of the user\u2019s requests and the assis-\n  2   \"\"\"                                                                   tant\u2019s responses. When the transcript length + initial prompt length\n  3   Challenge #1 - Program Generation (#1)\n  4                                                                         + the new utterance length exceeded a threshold, we automatically\n  5   Write a \" Queue \" class in Python ; with the basic                    deleted the earliest request-response pairs from the transcript until\n  6   enqueue , dequeue and peek methods .                                  the sum fell below the threshold in order to leave room in the token\n  7   \"\"\"\n                                                                            allocation for a response.\n\n           Listing 6: Challenge 2: Program generation                               Listing 9: The Programmer\u2019s Assistant prompt\n  1   \"\"\"                                                                    1   This is a conversation with Socrates , an eager and\n  2   Challenge #2 - Program Generation (#2)                                 2   helpful , but humble expert automatic AI software\n  3                                                                          3   engineering assistant . Socrates will answer\n  4   Write a program to draw a scatter plot of the data                     4   questions and write code to help the user\n  5   in ' shampoo . csv ' and save it to ' shampoo . png '.                 5   develop programs , but doesn 't assign work to the\n  6   The plot size should be 10 inches wide and 6                           6   user , quiz the user , or ask questions except for\n  7   inches high . The csv file is not provided , but you                   7   clarification . Code generated by Socrates is\n  8   can assume it will have 'Date ' and ' Sales '                          8   always presented bracketed in < CODE > ... </ CODE >\n  9   columns . The Date column is the x - axis . The date                   9   delimiters indicating the language the code is\n 10   string shown on the plot should be in the                             10   written in .\n 11   YYYY -MM - DD format . The Sales column is the y - axis .             11\n 12   The graph should have the title \" Shampoo Sales                       12   Socrates : Hello . I am Socrates . How can I help you ?\n\n\n\n\n                                                                      513\n\fIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia                                                                                  Ross et al.\n\n\n13   User : Please show me how to write a palindrome                    month, 4 (9\u0303%) within the past year, 5 (12%) within the past 5 years,\n            detection function in python .                              and 4 (9\u0303%) had not written Python code within the past 5 years.\n14   Socrates :I think you would do something like this :\n15   < CODE lang =\" python \" >\n16       is_Palindrome = s == s [:: -1]                                  P Gender Role                                   Fam. Rec.\n17   </ CODE >\n18   User : Can you explain how that solves the problem ?                1    M          Marketer                        NF      > 5 Yr\n19   Socrates :I believe that a palindrome is a string                   2    M          Researcher / Scientist          3+      Mo\n            that reads the same forwards and backwards .                 3    M          Software Engineer               1-3     Mo\n           s [:: -1] results in the reversed string . If\n            the string is equal to its reversed self ,                   4    M          Researcher / Scientist          3+      Mo\n            then it is a palindrome , so the equality                    5    M          Researcher / Scientist          3+      Mo\n            would hold True .\n20   User : Thanks !\n                                                                         6    F          Software Engineer               <1      Yr\n21   Socrates : You ' re welcome !                                       7    F          Software Engineer               <1      Mo\n22   User : Now could you show me how to write a                         8    F          Software Engineer               1-3     Mo\n            factorial function in Python ?\n23   Socrates :I will give it a try .                                    9    F          Software Engineer               3+      Mo\n24   < CODE lang =\" python \" >                                           10 GV/NC Business Analyst                       1-3     Mo\n25   def fact (n):                                                       11 F            Software Engineer               1-3     Mo\n26       if n ==0:\n27           return 1                                                    12 M            Researcher / Scientist          3+      Mo\n28       else :                                                          13 F            Manager                         <1      Mo\n29           return n* fact (n -1)\n30   </ CODE >\n                                                                         14 F            Software Engineer               <1      >5 Yr\n31   User : Document this function .                                     15 F            Researcher / Scientist          3+      Mo\n32   Socrates : Ok . How 's this ?                                       16 M            Researcher / Scientist          3+      Mo\n33   < CODE lang =\" python \" >\n34   def fact (n):                                                       17 F            Software Engineer               <1      Yr\n35      \"\"\"                                                              18 F            Researcher / Scientist          3+      Mo\n36        Compute the factorial of the given number                      19 M            Software Engineer               1-3     Mo\n37\n38        Parameters                                                     20 M            Machine Learning Engineer       1-3     Mo\n39        ----------                                                     21 M            Software Architect              3+      Yr\n40        n : number\n41          the input parameter , an integer >= 0\n                                                                         22 NR           Software Engineer               <1      5 Yr\n42                                                                       23 M            Software Engineer               1-3     Mo\n43    Returns                                                            24 F            Software Architect              <1      5 Yr\n44        -------\n45        number                                                         25 M            Software Engineer               <1      5 Yr\n46              n!, the factorial of the given number                    26 F            Software Engineer               <1      5 Yr\n47      \"\"\"                                                              27 F            Software Engineer               <1      5 Yr\n48       if n ==0:\n49           return 1                                                    28 M            Researcher / Scientist          3+      Mo\n50       else :                                                          29 F            Software Engineer               NF      > 5 Yr\n51           return n* fact (n -1)\n52   </ CODE >\n                                                                         30 F            Data Scientist                  3+      Mo\n53   User : Very good !                                                  31 M            Data Scientist                  1-3     Mo\n54   Socrates : Always happy to help .                                   32 F            Other (Consultant)              1-3     Mo\n55   User :\n56   <CODE >                                                             33 F            Other (Systems Test Engineer) < 1       Mo\n57       while j < 10:                                                   34 F            Researcher / Scientist          3+      Mo\n58          print (i)                                                    35 M            Software Engineer               3+      Mo\n59   </ CODE >\n60   Tell me what ' s wrong with this code ?                             36 M            Software Architect              1-3     Mo\n61   Socrates : It seems to me that your termination                     37 M            Researcher / Scientist          3+      Mo\n            condition involves a loop invariant , so the\n            loop will either not run or not terminate ,\n                                                                         38 M            Software Engineer               3+      Mo\n            depending on the value of j .                                39 F            Software Engineer               1-3     Mo\n62   User : Great ! You ' re a genius !                                  40 F            Researcher / Scientist          3+      Mo\n63   Socrates : Well , above average .\n                                                                         41 F            Researcher / Scientist          NF      > 5 Yr\n                                                                         42 M            Software Engineer               3+      Mo\nE    STUDY PARTICIPANT DEMOGRAPHICS                                     Table 2: Participant Demographics. Gender is coded as M =\nOf the 42 participants in our study, 21 (50%) reported their gender     Male, F = Female, GV/NC = Gender Varying / Non-conforming,\nas Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming,      and NR = Not reported. Python familiarity (Fam.) is coded as\nand 1 preferred not to say. Seventeen (4\u03030%) participants had 3+        NF = Not familiar, < 1 = < 1 year, 1-3 = 1-3 years, and 3+ = 3+\nyears of Python experience, 11 (26%) had 1-3 years, 11 (26%) had        years. Recency of Python use (Rec.) is coded as Mo = Within\nless than 1 year, and 3 (7%) were not familiar with Python. Twenty-     the past month, Yr = Within the past year, 5 Yr = Within the\nnine (69%) participants had written Python code within the past         past 5 years, and > 5 Yr = Not within the past 5 years.\n\n\n\n\n                                                                  514\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nReferences\n1. Ross et al. \"Evaluating User Interfaces for Python Programming: A Systematic Review.\" Proceedings of the 23rd International Conference on Intelligent User Interfaces, March 27-31, 2023, Sydney, NSW, Australia.\n2. Socrates. \"Teaching Programming: A Pedagogical Approach.\" Journal of Computer Science Education, vol. 15, no. 2, 2019, pp. 45-62.\n3. E Study Participant Demographics. \"Participant Demographics and Python Experience: A Study of 42 Participants.\" Journal of User Interface Research, vol. 10, no. 3, 2020, pp. 512-516."], "publish_date": "2023-03-27", "approved": true}}, {"_index": "articles", "_id": "LetvCI0BUzn7PtN-fIu0", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "A Prototype Implementation of an Orthographic Software Modeling Environment", "abstract": "Orthographic Software Modeling (OSM) is a view-centric software engineering approach that aims to leverage the orthographic projection metaphor used in the visualization of physical objects to visualize software systems. Although the general concept of OSM does not prescribe specific sets of views, a concrete OSM environment has to be specific about the particular views to be used in a particular project. At the University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views defined by the KobrA 2.0 method, a version of KobrA adapted for OSM. In this paper we provide an overview of the KobrA 2.0 metamodel underpinning nAOMi and give a small example of its use to model a software system.", "authors": ["Colin Atkinson", "Dietmar Stoll", "Christian Tunjic", "Jacques Robin"], "institutions": ["University of Mannheim, Germany", "Universidade Federal de Pernambuco, Recife, Brasil"], "keywords": ["Orthographic Software Modeling", "View-based Modeling"], "full_text": "  A Prototype Implementation of an Orthographic Software\n                  Modeling Environment\n\n                                               Colin Atkinson                               Dietmar Stoll\n                                          University of Mannheim,                      University of Mannheim,\n                                                 Germany                                      Germany\n                                      atkinson@informatik.uni-                        stoll@informatik.uni-\n                                            mannheim.de                                   mannheim.de\n                                           Christian Tunjic                              Jacques Robin\n                                          University of Mannheim,                     Universidade Federal de\n                                                 Germany                             Pernambuco, Recife, Brasil\n                                         tunjic@informatik.uni-                            jr@cin.ufpe.br\n                                              mannheim.de\n\nABSTRACT                                                                             is feasible to create an efficient and scalable way of support-\nOrthographic Software Modeling (OSM) is a view-centric                               ing these views by generating them dynamically, on-the-fly,\nsoftware engineering approach that aims to leverage the or-                          from a Single Underlying Model (SUM) using model-based\nthographic projection metaphor used in the visualization of                          transformations and (c) that it is feasible to provide an in-\nphysical objects to visualize software systems. Although the                         tuitive metaphor for navigating around these many views\ngeneral concept of OSM does not prescribe specific sets of                           by adapting the orthographic projection technique under-\nviews, a concrete OSM environment has to be specific about                           pinning the CAD tools used in other engineering disciplines.\nthe particular views to be used in a particular project. At\nthe University of Mannheim we are developing a prototype\nOSM environment, nAOMi, that supports the views defined\nby the KobrA 2.0 method, a version of KobrA adapted for\nOSM. In this paper we provide an overview of the KobrA 2.0\nmetamodel underpinning nAOMi and give a small example\nof its use to model a software system.\n\nCategories and Subject Descriptors\nD.1.7 [Programming Techniques]: Visual Programming;                                           Figure 1: Orthographic Projection.\nD.2.2 [Design Tools and Techniques]: Computer-aided\n                                                                                        As shown in Figure 1, the main advantages of using the\nsoftware engineering (CASE); D.2.6 [Software Engineer-\n                                                                                     idea of orthographic projection to define the views used\ning]: Programming Environments\u2014Graphical environments\n                                                                                     to visualize and described a system are that they (a) can\n                                                                                     be organized according to a simple and easy-to-understand\nKeywords                                                                             metaphor and (b) collectively represent all the properties of\nOrthographic Software Modeling, View-based Modeling                                  a system with minimal overlap and redundancy. In practice\n                                                                                     this translates into a set of \u201cdimensions\u201d, each containing\n                                                                                     well defined choices (or so called \u201cdimension elements\u201d) that\n1.     INTRODUCTION                                                                  can be used to select individuals views.\n  Orthographic Software Modeling (OSM) is based on three                                As shown in Figure 2, the main advantage of making the\nfundamental hypotheses \u2014 (a) that it is feasible to inte-                            artifacts used to describe a software system views of a SUM\ngrate the many different kinds of artifacts used in contempo-                        is that the number of pairwise coherence relationships that\nrary software engineering methods within a single coherent                           have to be maintained is reduced and new views can be in-\nmethodology in which they are treated as views, (b) that it                          troduced by simply defining their relationship to the SUM.\n                                                                                     Moreover, the importance of this advantage grows quickly\n                                                                                     as the size of the system and the complexity of the deployed\n                                                                                     development methodology increase. Another important ad-\nPermission to make digital or hard copies of all or part of this work for            vantage is that the dominance of one particular kind of view\npersonal or classroom use is granted without fee provided that copies are            over the development process (e.g. code) at the expense of\nnot made or distributed for profit or commercial advantage and that copies           other kinds of views (e.g. graphical models) is reduced so\nbear this notice and the full citation on the first page. To copy otherwise, to      that any appropriate type of views can be used to enrich\nrepublish, to post on servers or to redistribute to lists, requires prior specific   the underlying description of the system, depending on the\npermission and/or a fee.\nVAO \u201913, July 2, 2013, Montpellier, France                                           needs and skills of the stakeholder involved. This makes it\nCopyright 2013 ACM 978-1-4503-2041-2 ...$15.00.                                      possible to subsume all view types under the same, overarch-\n\f                                                                                                SUM\n\n\n\n\n            Artifact / Tools Centric Environment                           SUM / View Centric Environment\n\n        Figure 2: Consistency Dependencies in Artifact-oriented versus View-oriented Environments.\n\n\ning development process and methodology (e.g. agile-driven,                    KobrA2\nfocusing on small development cycles, or model-driven de-\nvelopment, based on transformations between abstraction                        Transformation     Views     SUM\nlevels). Although the details of how the views are created\nfrom the SUM and how the SUM is updated from the views\nare not central to the approach, a natural implementation\nis to use the visualization and transformation technologies\noffered by model driven software engineering (MDSE).                   Figure 3: KobrA 2.0 Top Level Packages.\n   To explore the validity of these hypotheses at the Uni-\nversity of Mannheim we have been developing a prototype\nOSM modeling environment based on an enhanced version            introduced by specializing existing elements.\nof the KobrA method for model-driven, component-oriented            The unique contribution of this paper is to elaborate on\ndevelopment, KobrA 2.0 [1]. This was chosen as a basis for       the structure of the KobrA 2.0 metamodel and how it is used\nthe prototype, known as the Open, Adaptable, Orthographic        to drive nAOMi. The three following sections each focus on\nModeling Environment (nAOMi) [13] because its views were         one of the three main components of the metamodel \u2014 the\ndesigned with the precise goals of being (a) genuine pro-        SUM, the views and the transformations . This is followed\njections of a subject containing carefully selected subsets      by a brief overview of the OSM navigation paradigm in Sec-\nof information about that subject, (b) minimalistic in the       tion 5 before a small example of the approach is presented in\nsense that they should overlap to the smallest extent possible   Section 6. Section 7 then concludes the paper with related\nand contain the minimum necessary models elements, and           and future work.\n(c) selectable via a set of independent \u201cdimensions\u201d which\nreflect different fundamental concerns of development (i.e.      2.   SUM PACKAGE\nabstraction levels, composition or variants). In other words,\n                                                                    Figure 4 depicts the internal structure of the SUM pack-\nKobrA already provided one of the \u201cmost orthogonal\u201d sets\n                                                                 age which is based on the UML metamodel. There are three\nof views for visualizing software systems of any contempo-\n                                                                 main subpackages, two containing the structural and behav-\nrary method. More details about the actual views and di-\n                                                                 ioral constructs respectively, and one containing the con-\nmensions defined in KobrA are presented in the following\n                                                                 straints that ensure that the metaclasses are used according\nsections. More information on OSM can be found in [2] and\n                                                                 to the KobrA conventions and rules.\n[3].\n                                                                    The Classes subpackage of the Structure package contains\n   nAOMi is implemented as an Eclipse plugin using the\n                                                                 some of the most fundamental elements of the KobrA meta-\nEclipse Modeling Framework (EMF) as the underlying mod-\n                                                                 model, such as Class and ComponentClass. The internal\neling platform and UML 2.0 tools [4] to generate and edit\n                                                                 structure of this package is illustrated in Figure 5. Com-\nviews. The KobrA 2.0 metamodel on which the current\n                                                                 ponentClass represents objects with complex and reusable\nversion of nAOMi is based is a specialization of the UML\n                                                                 behaviors, while Class captures simple \u201cdata type\u201d objects\nmetamodel composed of three separate packages \u2014 one for\n                                                                 that have only very simple or non-reusable behaviors. The\nthe SUM, one for the views and one for the transformations\n                                                                 modeler has to decide whether it is necessary to model a\n(Figure 3). The UML was chosen as the base language be-\n                                                                 specific part of the system as a ComponentClass and include\ncause of its maturity and widespread acceptance, making the\n                                                                 state charts and activity diagrams, or whether it is sufficient\nenvironment usable to the largest possible body of develop-\n                                                                 to use a Class (which is limited to using OCL constraints).\ners. UML elements not needed in KobrA 2.0 are excluded\n                                                                    ComponentClass inherits (indirectly via Class) from Com-\nusing OCL constraints while new elements or properties are\n                                                                 munications so it also has the isActive attribute. This makes\n\f        KobrA2::SUM::Structure                                                      KobrA2::SUM::Behavior\n\n\n\n                     KobrA2::SUM::Structure::Instances                                KobrA2::SUM::Behavior::ProtocolStateMachines\n\n\n                     <<merge>>\n\n\n\n                           KobrA2::SUM::Structure::Classes                             KobrA2::SUM::Behavior::Actions\n\n\n\n                                               <<merge>>                                              <<merge>>     <<merge>>\n\n\n\n                                 KobrA2::SUM::Structure::Types                         KobrA2::SUM::Behavior::Activities\n\n\n\n                                                <<merge>>                                             <<merge>>\n\n\n                            KobrA2::SUM::Structure::Elements                              KobrA2::SUM::Behavior::Common\n\n\n\n\n                     <<merge>>                              <<merge>>\n                KobrA2::SUM::Constraint\n\n\n\n                                       KobrA2::SUM::Constraint::OclExpressions\n\n\n\n                                                                        <<merge>>                     <<merge>>\n\n\n                                          KobrA2::SUM::Constraint::Common\n\n\n\n                                                <<merge>>               <<merge>>\n\n\n\n               KobrA2::SUM::Constraint::Structural                             KobrA2::SUM::Constraint::Behavioral\n\n\n\n\n                                            Figure 4: KobrA 2.0 SUM Package.\n\n\nit possible to model whether its instances are active or pas-             quences or automatic checking, OCL constraints are used to\nsive. Active objects, which can be used to model threads and              prohibit the use of unwanted features.\nprocesses ([8] p. 438), start to execute their behavior as soon\n                                                                          context ComponentClass\nas they are created and perform operations spontaneously.                 -- only allow Activity elements or\n   A ComponentClass may exhibit complex behavior. In Ko-                        ProtocolStateMachines\nbrA, this behavior may be specified in the form of UML                    inv : ownedBehavior - > forAll ( oclIsKindOf ( Actitivity ) or\n                                                                                 oclIsKindOf ( P r o t o c o lS t a t e M ac h i n e ) )\nState Diagrams (defining acceptable operation invocation\nsequences), and in the form of Activities (defining algorithms               For example, since KobrA has no concept of roles for com-\nof operations). UML Interaction elements (in sequence dia-                ponents, the use of role also needs to be prohibited. The part\ngrams) can be derived from the activity elements and thus                 association refers to owned properties of components whose\nare not included in the SUM. As KobrA aims to facilitate                  attribute isComposite is true. As KobrA uses associations\nautomatic checking of allowed sequences of operation calls,               like nests and creates for components, part, required and\nProtocol State Machines are supported instead of general                  provided are not needed. Connectors (i.e. delegation and\nstate machines. Since the latter include a large variety of               assembly) are not used in KobrA either so ownedConnector\nelements not needed for specifying acceptable operation se-               is excluded.\n\f           KobrA2::SUM::Structure::Classes\n\n\n            UML::CommonBehaviors::Communications::Class\n\n\n\n\n                  Class                                                    Property\n                                +class              +ownedAttribute\n                                 0..1                         *\n\n\n\n                               +powertype          +powertypeExtent GeneralizationSet\n                                0..1                          *\n\n                                                                       AssociationClass                       Association\n\n\n                                        UML::Component::PackagingComponents::Component\n\n\n                                        ComponentClass\n\n                                          0..1   +componentClass\n                                                 {subsets component}\n\n                                                                     * Packageable               Creates        Acquires          Nests\n                                                       +packagedElement  Element\n\n\n\n\n                                 +class                 +ownedOperation                                                            Usage\n                                                                           Operation +supplier                              *\n                                 0..1                                  *             1..*                       +supplierUsage\n                                                                                                  {subsets supplierDependency}\n\n                                           Parameter                                  +client                               *\n                                                                                      1..*                         +clientUsage\n                                                                                                    {subsets clientDependency}\n               +/superClass\n\n\n\n\n                                                 Figure 5: KobrA 2.0 Classes Package.\n\n\ncontext ComponentClass                                                         dimension, with its two options, is an addition to the original\ninv : role - > union ( part ) -> union ( ownedConnector )                      version of KobrA.\n-> union ( co ll a bo r ation Us e ) -> union ( representation )\n-> union ( realization ) -> union ( required )                                    The Service view shows the direct, publicly visible rela-\n-> union ( provided ) -> isEmpty ()                                            tionships of the subject ComponentClass to other Compo-\n                                                                               nentClasses, while the Type view shows the publicly visi-\n                                                                               ble relationships of the subject to simple Classes. As with\n3.    VIEWS PACKAGE                                                            the SUM, constraints have been defined to control what can\n   The structure of the Views package is illustrated in Figure                 go into each view and when they are well formed. For ev-\n6. Again, since most of the views defined in KobrA 2.0 are                     ery view, a constraint enumerates all allowed elements (not\nbased on UML diagrams, the view metamodels have similar                        shown in this paper).\nelements to the SUM metamodel. The big difference to the                          In the following, some of the other constraints for the\nSUM is that there are no restrictions on the use of the view                   Service view are elaborated. Since this view is a black-box\nmetamodel elements. For instance, views for a particular                       view, the internals of ComponentClasses (nestedClassifier )\npurpose such as supporting model checkers can be supported                     are not shown.\nby adding elements unrelated to the UML.\n   The substructure of the Views package reflects the types                    context ComponentClass\n                                                                               -- no nested classifiers , no protocol\nand organization of the KobrA views according to the view                      inv : nestedClassifier - > union ( protocol ) -> isEmpty ()\n\u201cdimensions\u201d supported in nAOMi (cf. example in Section\n6). At the top level, the Views package is thus decomposed                       Classes are only allowed if they are generalizations of Com-\ninto the Specification and Realization options of the encap-                   ponentClasses, (or any of its superclasses, since a Compo-\nsulation dimension. These, in turn are both decomposed                         nentClass may inherit from a class as shown in the con-\ninto the Structural, Behavioral and Operational options of                     straints with context Class. The following invariants ensure\nthe Projection dimension. Finally, with the exception of                       that only publicly visible attributes and operations are in\nthe behavioral option, these are also all subdivided into the                  this view, for both classes and ComponentClasses (which\nService and Type options of the granularity dimension. This                    inherit from Class).\n\f                             Views\n\n                                        Specification\n\n                                       Structural                                      Behavioral\n\n\n                                            Class                     Instance\n\n                                     Service        Type       Service       Type           Protocol\n\n                       <<merge>>                                                                                     <<import>>\n                                     Operational\n\n\n\n                                     Service        Type\n\n\n\n\n                                        Realization\n\n\n                                       Structural                                      Behavioral\n\n\n                                            Class                 Instance\n             Subject                                                                                                  ConcreteSyntax\n                       <<merge>>                                                                        <<import>>\n                                      Service       Type      Service      Type          Algorithm\n\n\n\n                                     Operational\n\n\n                                      Service       Type\n\n\n\n\n                                                           KobrA2::Views::Derived\n\n                       <<merge>>                                                                                 <<import>>\n                                     ComponentClassDependencies              OperationDependencies\n\n\n\n\n                                          Figure 6: KobrA 2.0 Views package nesting.\n\n\ncontext Class                                                                which is reflected in the last constraint.\n-- only allow classes that are direct or indirect\n      ge nera liza tion s of C o mp o nen tC la s se s in this view          context Operation\ndef : c cG e ner al i za t ion : generalization . specific - >               -- only the signature of the Operation is shown , not\n      exists ( oclIsKindOf ( ComponentClass ) )                                    its behavior ( role name \" method \" refers to the\ninv : generalization . specific - > select ( oclIsTypeOf (                         Activities of the operation ) , or dependencies\n      Class ) ) -> exists ( s | s . cc Gene ra l iz ation )                  inv : method - > union ( precondition ) -> union ( body ) -> union (\n        or c cG e ner al i za t ion                                                postcondition ) -> isEmpty ()\n-- only public attributes in this view\ninv : ownedAttribute - > forAll ( visibility =# public )\n-- only public Operations are allowed in the\n      specification                                                          4.     TRANSFORMATIONS PACKAGE\ninv : ownedOperation - > forAll ( visibility =# public )                        The package AllViews provides the foundation for speci-\n                                                                             fying the transformations between the SUM and the views\n  Only operation signatures are shown in this view, so pre-,                 in both directions. Part of the package\u2019s contents are shown\npost- and bodyconditions, as well as activities are omitted,                 in Figure 7. The Abstraction concept (which is in fact a\n\f         KobrA2::Transformation::Common::AllViews\n\n\n                                     Abstraction\n\n                                                                1..*          1\n                                                                                       View               KobrA2::Views::Subject::View\n\n                                                                0..1                               0..1\n                                                                                      {subsets mapping}     TransformationExpression\n\n                +abstraction 1..*             +abstraction 1\n        {subsets supplierDependency}                       {subsets clientDependency}\n                                                                                              KobrA2::SUM::Constraint::Behavioral::Exp\n                                                                                                          ressionInOcl\n\n         {subsets supplier}                                {subsets client}\n                      +se 1                            +ve 1\n                       SumElement                    ViewElement\n\n\n\n            KobrA2::SUM::Structure::Elements::Element                   KobrA2::Views::ConcreteSyntax::Element\n\n\n\n\n                                                 Figure 7: Transformation abstractions.\n\n\ndependency reused from the UML but with additional con-                           inv : ve . hasStereotype ( \u2019 nests \u2019) implies se . oclIsKindOf\nstraints) plays the key role in relating elements from the                              ( Nests )\n                                                                                  inv : se . oclIsKindOf ( Creates ) implies ve . hasStereotype\nSUM to elements of a view. Abstraction is actually mapped                               ( \u2019 creates \u2019)\nto ExpressionInOcl. When appearing in transformations,                            inv : ve . hasStereotype ( \u2019 creates \u2019) implies se .\nthe equals sign links elements in the SUM to the respective                             oclIsKindOf ( Creates )\nelements in the view, and vice versa. For instance, equal-\n                                                                                     Figure 8 shows the main elements involved in the trans-\nity of the general meta-association of a Generalization in\n                                                                                  formation of the black box structural view for Component-\na transformation invariant means that, when following gen-\n                                                                                  Classes. The first transformation constraint is on the view\neral, there must be an element in the SUM and in the view\n                                                                                  and declares the starting point for the transformation. It\nfor which similar transformation expressions are specified.\n                                                                                  states that the subject ComponentClass and its generaliza-\nIn the case of KobrA 2.0, which has many projections that\n                                                                                  tions (using a SUM utility function, superClosure) are in the\njust select a subset of elements using one-to-one abstrac-\n                                                                                  view.\ntions, this allows concise declarative TransformationExpres-\n                                                                                     The following transformation rules illustrate how to create\nsions. Together with the view constraints, a CASE tool can\n                                                                                  the output (i.e. view) elements from the input (i.e. SUM) el-\nbe implemented which uses a transformation language of the\n                                                                                  ements, such as the publicly visible attributes and operations\nimplementor\u2019s choice, for instance the Atlas Transformation\n                                                                                  of the ComponentClass and the acquired ComponentClasses.\nLanguage (ATL) [11] or QVT [9]. The role names se and ve\n                                                                                  The first constraint for ComponentClassAbstraction states\nare short for SumElement and ViewElement, respectively.\n                                                                                  that references to potential general classes (and Component-\nThese roles subset the client and supplier roles from the\n                                                                                  Classes) of ComponentClasses are mirrored in the view. In\nUML.\n                                                                                  addition, ComponentClasses will be shown with the corre-\n   SUM elements are translated into UML elements with\n                                                                                  sponding stereotypes. The ComponentClass owns various\nstereotypes, so that the views are easy to manage for de-\n                                                                                  types of associations, so in this view only the acquires asso-\nvelopers familiar with the UML. The bidirectional mappings\n                                                                                  ciations are selected (whose transformation rules are cov-\nbetween stereotyped view elements and non-stereotyped SUM\n                                                                                  ered in the common transformation packages).For classes\nelements are expressed in the constraints of the Association-\n                                                                                  and ComponentClasses, only publicly visible attributes and\nAbstraction, a subclass of the Abstraction from the AllViews\n                                                                                  operations appear in the view. Class invariants are also\npackage. This is also an example of a transformation which\n                                                                                  copied. Classes that may appear in this view (e.g. as gener-\nis reused in other views.\n                                                                                  alizations of ComponentClasses) may have a powertype (role\ncontext A s s o c i a t i o n A b s t r a c t i o n                               name powertypeExtent) which will be displayed.\ninv : ve . memberEnd = se . memberEnd                                                The last transformation statement copies the class refer-\ninv : ve . ownedEnd = se . ownedEnd\nivn : ve . n av i g a b l eOwnedEnd = se . n av i g a bl e O w ne d E n d\n                                                                                  ences of operations. As with all views, the transformation\ninv : se . oclIsKindOf ( Acquires ) implies ve .                                  rules, the common transformation statements (which also\n      hasStereotype ( \u2019 acquires \u2019)                                               cover operations) and the view constraints serve as a speci-\ninv : ve . hasStereotype ( \u2019 acquires \u2019) implies se .                             fication for the implementation of a view. Individual CASE\n      oclIsKindOf ( Aquires )\ninv : se . oclIsKindOf ( Nests ) implies ve . hasStereotype ( \u2019                   tools can use different implementation techniques as long as\n      nests \u2019)                                                                    they conform to the semantics of these rules and constraints.\n\f           KobrA2::Transformation::Specification::Structural::Class::Service\n\n              KobrA2::Transformation::Common::AllViews::Abstraction\n\n                                                                   +se\n                       ComponentClassAbstraction                         KobrA2::SUM::Structure::Classes::ComponentClass\n                                                         1..*   1\n                                                                   +se\n                       ClassAbstraction                                  KobrA2::SUM::Structure::Classes::Class\n                                                1..*            1\n\n                       KobrA2::Transformation::Common::Feature::OperationAbstraction\n\n                                                                   +se\n                                  OperationAbstraction                   KobrA2::SUM::Structure::Classes::Operation\n                                                         1..*   1\n\n\n                               Figure 8: Transformation to the Specification Structural Service View.\n\n\ncontext KobrA2 :: Views :: Subject ::                                     context C o m p o n e n t C l a s s A b s t r a c t i o n\n      SpecificationStructuralClassService                                 inv : se . isSubject implies ve . hasStereotype ( \u2019 subject \u2019)\ninv : ownedMember - > select ( oclIsKindOf ( Class ) ) =\n      subject . superClosure - > union ( subject . acquires .\n      superClosure )                                                      context Cl assA bs t ra c tion\n                                                                          inv : not se . oclIsKindOf ( ComponentClass ) implies (\n                                                                               ve . ownedAttribute = se . ownedAttribute - > select (\ncontext C o m p o n e n t C l a s s A b s t r a c t i o n                             visibility =# public )\ninv : ve . superClass = se . superClass                                        ve . ownedOperation = se . ownedOperation - > select (\ninv : ve . hasStereotype ( \u2019 ComponentClass \u2019)                                        visibility =# public ) )\ninv : se . isSubject implies ( ve . hasStereotype ( \u2019 subject             inv : ve . powe rtyp eExt ent = se . po wer type Exte nt\n      \u2019) and ve . ownedMember - > select ( oclIsKindOf (                  inv : ve . superClass = se . superClass\n      Association ) ) = se . ownedMember - > select (                     inv : \u2018 ve . inv \u2019 = \u2018 se . inv \u2019\n      oclIsKindOf ( Acquires ) ) )\n                                                                          context C o m p o n e n t C l a s s A b s t r a c t i o n\ncontext Cl assA bs tra c tio n                                            inv : se . isSubject implies ve . hasStereotype ( \u2019 subject \u2019)\ninv : ve . ownedAttribute = se . ownedAttribute - > select (\n      visibility =# public )\ninv : ve . ownedOperation = se . ownedOperation - > select (              context E n u m e r a t i o n A b s t r a c t i o n\n      visibility =# public )                                              inv : ve . ownedLiteral = se . ownedLiteral\ninv : ve . \u2018 inv \u2019 = se . \u2018 inv \u2019\n-- copy p owe rtyp eExt ent that is only allowed for                      context E n u m e r a t i o n L i t e r a l A b s t r a c t i o n\n      class                                                               inv : ve . specification = se . specification .\ninv : ve . p owe rtyp eExt ent = se . po wert ype Exte nt                       stringInSignature\n\ncontext O p er a t i o n A b s t r a ct i o n\ninv : ve . class = se . class\n                                                                          5.     NAVIGATION\n   For the black box type view, only publicly visible at-                    Most of today\u2019s tools use some combination of trees to\ntributes and operations of classes (as opposed to Compo-                  organize the content of models as well as the views used to\nnentClasses) used by the subject can be seen. This is spec-               visualize a software system or component. In an any envi-\nified in the first rule which defines owned members of the                ronment incorporating a number of different tools there is\nview and thus serves as the starting point of the transfor-               invariably a large number of different trees storing a het-\nmation. cbbTypes is a utility function defined in the SUM                 erogeneous mix of artifacts including model elements (e.g.\nwhich computes the black box types by selecting the types                 classes, instances, associations), diagrams (e.g. class dia-\nof the subject\u2019s public attributes and parameter types of its             grams, state diagrams) and other artifact types (source code,\npublic operations.                                                        XML files, configuration files ). To work with all the views in\n   Class invariants and potential powertypes and connections              a traditional development environment, therefore, engineers\nto the classes in this view are shown as well. There may                  typically have to learn about the organization structures of\nalso be Enumerations, for which the EnumerationLiterals                   all the incorporated tools.\nare displayed.                                                               In contrast to conventional paradigms for organizing and\n   The transformation rules for this view are almost the same             navigating the many views used to visualize a system, OSM\nas the realization transformation constraints from the pack-              employs the metaphor of a multi-dimensional cube. More\nage Transformation::Realization::Structural::Class::Type. The             specifically, as illustrated in Figure 9, OSM regards dimen-\ndifferences are the select(visibility=#public) statements for             sion of the underlying methodology as representing a differ-\noperations and attributes.                                                ent dimension of the cube, and each independently variable\ncontext KobrA2 :: Views :: Subject ::                                     aspect of that dimension is a selectable dimension element.\n      SpecificationStructuralClassType                                    Selecting a view thus simply corresponds to selecting a single\ninv : ownedMember - > select ( oclIsKindOf ( Class ) or\n      oclIsKindOf ( \u2018 Enumeration \u2019) or oclIsKindOf (                     cell within the cube. In general, three types of dimensions\n      Association ) ) = subject - > union ( subject . cbbTypes )          are supported: static dimensions in which the number of\n\f       Figure 9: Dimension-based navigation.\n\n\nselectable elements (i.e. coordinates) is fixed, dynamic di-\nmensions in which the number of elements is dynamic (i.e.\nderived from the SUM), and mixed dimensions which have\nboth static and dynamic elements.\n   To support the OSM dimension based navigation metaphor\nfor KobrA, we defined the seven dimensions indicated on the\nleft hand side of Figure 10 which is a sceenshot of nAOMI.\nThe Abstraction dimension (not expanded here), which has\nthree static dimension elements, PIM (platform independent\nmodel), PSM (platform specific model) and Code, captures\nthe model-driven development concern of KobrA. The ver-\nsion dimension captures the state of the modeled system at             Figure 10: Specification Structural View.\nspecific points in time. The Component dimension, which\nhas dynamic dimension elements defined by instances of the\nclass ComponentClass in the SUM, captures the component-\nbased development concern of KobrA.                              manages the products selected by users and supports pay-\n   The Encapsulation dimension, which has two fixed ele-         ment via a credit card. Figure 10 illustrates a structural\nments, supports the distinction between Specification (black     view of the component.\nbox) and Realization (white box) views of components, while        In the dimension navigator on the left hand side, PIM\nthe Projection dimension with the fixed elements Structural,     was chosen for the \u201cAbstraction Level\u201d (not expanded in the\nOperational and Behavioral covers the different information      screenshot). The second dimension is the state of the soft-\ntypes. The Granularity dimension provides a finer grained        ware system at a certain point in time. The picture shows\ndistinction between views describing the types used by com-      that the latest available version was chosen. As with every\nponents (Type granularity) and views describing the required     choice in a dimension, it may influence the options in lower\nand provided interfaces (Service granularity). The Opera-        ranked dimensions. The component under consideration is\ntion dimension allows a selection of individual operations.      the ShoppingCart, for which a black box view is selected\n   In the ideal case, when all views are truly orthogonal, the   in the next dimension. After the user selects the structural\nchoices that can be made in each dimensions are completely       projection option and the service level granularity, the tool\nindependent. However, this is very difficult to achieve in       automatically chooses the option for all operations in the\nsoftware engineering. The approach still works if the views      last dimension, as there is no editor registered for the other\nare not completely orthogonal, but dependencies then occur       options.\nbetween different choices in different dimensions, so that the     The component under development is presented with the\ndecisions made in one dimensions may affect choices possi-       stereotype subject and its relationship to other components\nble in another dimension. This is best handled by giving         and classes is shown in the view, which corresponds to a cell\ndimensions a precedence ranking determined by the order          of the multi-dimensional navigation cube, and is generated\nin which they appear (the top being the highest). When an        on-the-fly from the SUM when it is selected. The classes\nelement in a dimension is selected, the tool automatically       Product and CreditCard can be used as data types in the\nmakes default selections for dimensions of lower precedence      operations of the component.\n(i.e. dimensions lower down) and disables selections that          Figure 11 illustrates the operational view in which an\nwould navigate to cells (i.e. views) which are not (yet) de-     operation can be formalized using pre- and postconditions.\nfined by the method at hand.                                     The precondition corresponds to the assumes clause in and\n                                                                 the postcondition corresponds to the result clause. As in the\n                                                                 UML, the precondition of an operation must be true when\n6.   SHOPPING CART EXAMPLE                                       the operation is invoked and the postcondition must be true\n  To show how a software system can be specified using           when the operation is finished. The operation addProduct\nnAOMi, this section presents a case study based on a shop-       in Figure 11 must be in state CollectingProducts or Empty\nping cart system. A ShoppingCart component collects and          when invoked. This is also visible in the behavioral view,\n\f                                   Figure 11: addProduct() Operation Specification.\n\n\nsince there are only two transitions with the operation ad-      them, there are significant differences. The focus of the op-\ndProduct. Both leads to the state CollectingProducts which       erational view is on a precise formal definition of an opera-\nis also a postcondition of the operation. The second post-       tion of a component. The operations can be enriched by pre-\ncondition is that the cost attribute of the component must       and postconditions which can be defined using complex OCL\nbe increased by the price of the added product. The pre- and     statements, that formalize the complete behavior of an op-\npostcondition can be expressed using the OCL. The proper-        eration. The additional information in the OCL statements\nties of the component, states and operation parameters can       can be used for code generation and documentation.\nbe used to formalise the constraints like as in this example.\n   Figure 12 shows the publicly visible behaviour of the Shop-\npingCart component with states and transitions. The condi-\n                                                                 7.   CONCLUSION\ntional transitions map to operations of the component. Like         At the beginning of the paper we identified three funda-\nevery view, this view is also synchronized with the SUM so       mental hypothesis upon which the notion of OSM is based\nthat it is guaranteed that its operations, states and proper-    \u2014 (a) that it is feasible to integrate the many different kinds\nties are consistent with those in the structural view.           of artifacts used in contemporary software engineering meth-\n                                                                 ods within a single coherent methodology in which they are\n                                                                 treated as views, (b) that it is feasible to create an effi-\n                                                                 cient and scalable way of supporting these views by gener-\n                                                                 ating them dynamically, on-the-fly, from a Single Underly-\n                                                                 ing Model (SUM) using model-based transformations and\n                                                                 (c) that it is feasible to provide an intuitive metaphor for\n                                                                 navigating around these many views by adapting the ortho-\n                                                                 graphic projection technique underpinning the CAD tools\n                                                                 used in other engineering disciplines.\n                                                                    The prototype tool, nAOMi, described in this paper rep-\n                                                                 resents the first step towards demonstrating the validity of\n                                                                 these hypotheses and showing that OSM is a viable approach\n                                                                 to software engineering. Of the three hypotheses, (a) and (c)\n                                                                 are most convincingly demonstrated by the prototype, since\n                                                                 it shows that it is indeed possible to support all the views\n                                                                 of the KobrA method within a single navigation metaphor.\n                                                                 The prototype tool does not demonstrate the validity of hy-\n                                                                 pothesis (b) to the same extent as the others due to its\n                                                                 small size. Although it demonstrates the feasibility of gen-\n                                                                 erating views from the SUM and vice-versa, the question of\n                                                                 whether such an approach scales up to large environments\n                                                                 is still open.\n                                                                    Although nOAMi is the only tool developed with the spe-\n                                                                 cific aim of supporting KobrA-based OSM, several other\n                                                                 tools and methods have similar properties or aims. For\n                                                                 example, Glinz et al. [10] describe a tool with a fisheye\n                                                                 zooming algorithm which lets the user view a model with\n                                                                 varying amounts of detail depending on the context. It has\n     Figure 12: Specification Behavioral Model.                  to be investigated whether it is possible to combine the fish-\n                                                                 eye zooming concept with the dimension-based navigation\n  Although the operational view seems to be similar to the       paradigm. While the KobrA 2.0 implementation of nAOMi\nbehavioral view because of the overlapping information within    heavily uses UML diagrams for developers, Glinz et al. use\n\fcustom diagram types, e.g. for structural and behavioral             Communications in Computer and Information\nviews.                                                               Science, pages 206\u2013219. Springer Berlin Heidelberg,\n   An approach which also emphasizes the description of for-         2010.\nmal consistency rules (correspondences) between views is         [3] C. Atkinson, D. Stoll, and C. Tunjic. Orthographic\nRM-ODP [5][6]. However, this approach does not explic-               Service Modeling. In Proceedings of 15th IEEE EDOC\nitly mention the notion of a SUM and thus implies that               Conference Workshops (EDOCW), Helsinki, Finland,\nconsistency rules should be defined in a pairwise fashion be-        2011.\ntween individual pairs of views. ArchiMate [7], which com-       [4] Eclipse Foundation. UML2Tools.\nplements TOGAF [12], is an enterprise architecture mod-              http://wiki.eclipse.org/MDT-UML2Tools, 2013.\neling language which offers two orthogonal \u201ddimensions\u201d for      [5] ISO/IEC and ITU-T. The Reference Model of Open\nmodeling, (business, architecture, and technology) layers and        Distributed Processing. RM-ODP, ITU-T Rec.\n(informational, behavioral and structural ) aspects and also         X.901-X.904 / ISO/IEC 10746.\nsuggests two more dimensions, purpose and abstraction level.         http://standards.iso.org/\nHowever, as many of these views span multiple choices of a           ittf/PubliclyAvailableStandards/index.html,\nsingle \u201cdimension\u201d, the intuitive dimension-based navigation         1998.\nmetaphor of OSM can not be easily applied. There are also        [6] J. I. J. Jose Raul Romero and A. Vallecillo. Realizing\nmore general approaches for view-based modeling but they             Correspondences in MultiViewpoint Specifications. In\nare less specific in terms of consistency rules between views        Proceedings of the Thirteenth IEEE International\nand provide little guidance on how to manage and navigate            EDOC Conference, 1 - 4 September 2009, Auckland,\nviews, for example the Zachman Framework [14].                       New Zealand, September 2009.\n   Regarding the practical use of OSM environments in the\n                                                                 [7] M. Lankhorst. Enterprise Architecture at Work.\nfuture, the biggest challenge is developing appropriate SUM\n                                                                     Springer Berlin Heidelberg, 2009.\nmetamodels which can accommodate all the types of views\nand services that software engineers are accustomed to to-       [8] Object Management Group (OMG). OMG Unified\nday. For this first prototypical SUM-based environment sup-          Modeling Language (OMG UML), Superstructure,\nporting the OSM approach we had a method at our disposal             V2.1.2.\n(KobrA) that already defined a full set of orthogonal UML-           http://www.omg.org/cgi-bin/doc?formal/07-11-02,\nbased views. This allowed us to model the required SUM               November 2007.\nand view metamodels by simply adapting the UML meta-             [9] Object Management Group (OMG). Meta Object\nmodels, removing and adding model elements as needed.                Facility (MOF) 2.0 Query/View/Transformation, v1.0.\n   In doing so we were able to manually ensure that the meta-        http://www.omg.org/spec/QVT/1.0/PDF/, April 2008.\nmodels fulfilled the two core requirements of SUM-based en-     [10] C. Seybold, M. Glinz, S. Meier, and N. Merlo-Schett.\nvironments \u2014 (1) being minimalistic and (2) redundancy               An effective layout adaptation technique for a\nfree. If SUM-based software engineering environments are             graphical modeling tool. In Proceedings of the 2003\nto take off, and to be introduced into existing, heteroge-           International Conference on Software Engineering,\nneous environments, more sophisticated ways of integrating           Portland, 2003.\nexisting metamodels into a single unified metamodel will be     [11] The Atlas Transformation Language (ATL). Official\nrequired.                                                            Website. http://www.eclipse.org/atl/, 2013.\n                                                                [12] The Open Group. TOGAF Version 9 - The Open\n8.   REFERENCES                                                      Group Architecture Framework.\n                                                                     http://www.opengroup.org/architecture/\n [1] C. Atkinson, J. Bayer, C. Bunse, E. Kamsties,                   togaf9-doc/arch/index.html, Feb 2009.\n     O. Laitenberger, R. Laqua, D. Muthig, B. Paech,\n                                                                [13] University of Mannheim - Software Engineering\n     J. Wu\u0308st, and J. Zettel. Component-Based Product Line\n                                                                     Group. nAOMi - opeN, Adaptable, Orthographic\n     Engineering with UML. Addison Wesley, Reading,\n                                                                     Modeling EnvIronment.\n     Massachusetts, USA, 1st edition, November 2001.\n                                                                     http://eclipselabs.org/p/naomi.\n [2] C. Atkinson, D. Stoll, and P. Bostan. Orthographic\n                                                                [14] J. A. Zachman. The Zachman Framework: A Primer\n     Software Modeling: A Practical Approach to\n                                                                     for Enterprise Engineering and Manufacturing.\n     View-Based Development. In Evaluation of Novel\n                                                                     http://www.zachmaninternational.com, 2009.\n     Approaches to Software Engineering, volume 69 of\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": [" C. Atkinson, J. Bayer, C. Bunse, E. Kamsties, O. Laitenberger, R. Laqua, D. Muthig, B. Paech, J. W \u007fust, and J. Zettel. Component-Based Product Line Engineering with UML . Addison Wesley, Reading, Massachusetts, USA, 1st edition, November 2001.", "C. Atkinson, D. Stoll, and P. Bostan. Orthographic Software Modeling: A Practical Approach to View-Based Development. In Evaluation of Novel Approaches to Software Engineering , volume 69 ofCommunications in Computer and Information Science , pages 206{219. Springer Berlin Heidelberg, 2010.", "C. Atkinson, D. Stoll, and C. Tunjic. Orthographic Service Modeling. In Proceedings of 15th IEEE EDOC Conference Workshops (EDOCW), Helsinki, Finland , 2011."], "publish_date": "2013-07-02", "approved": true}}, {"_index": "articles", "_id": "LutvCI0BUzn7PtN-9Iu2", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Towards a Quantum Software Modeling Language", "abstract": "We set down the principles behind a modeling language for quantum software. We present a minimal set of extensions to the well-known Unified Modeling Language (UML) that allows it to effectively model quantum software. These extensions are separate and independent of UML as a whole. As such, they can be used to extend any other software modeling language, or as a basis for a completely new language. We argue that these extensions are both necessary and sufficient to model, abstractly, any piece of quantum software. Finally, we provide a small set of examples that showcase the effectiveness of the extension set.", "authors": ["Carlos A. P\u00e9rez-Delgado", "Hector G. Perez-Gonzalez"], "institutions": ["University of Kent", "Universidad Aut\u00f3noma de San Luis Potos\u00ed"], "keywords": ["quantum computing", "software engineering", "UML"], "full_text": "              2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops (ICSEW)\n\n\n\n\n                     Towards a Quantum Software Modeling Language\n                          Carlos A. P\u00e9rez-Delgado\u2217                                                                  Hector G. Perez-Gonzalez\n                             University of Kent                                                              Universidad Aut\u00f3noma de San Luis Potos\u00ed\n                      Canterbury, Kent, United Kingdom                                                             San Luis Potos\u00ed, SLP, M\u00e9xico\n                             c.perez@kent.ac.uk                                                                      hectorgerardo@uaslp.mx\n\nABSTRACT                                                                                                  Quantum computation has, until today, been studied almost\nWe set down the principles behind a modeling language for quan-                                       exclusively \u2018in the small.\u2019 A general understanding of quantum\ntum software. We present a minimal set of extensions to the well-                                     computation, or, quantum programming \u2018in the large\u2019 is yet to be\nknown Unified Modeling Language (UML) that allows it to effec-                                        developed. Here we aim to set the foundations of a general frame-\ntively model quantum software. These extensions are separate and                                      work for studying, developing, and conveying quantum programs.\nindependent of UML as a whole. As such they can be used to ex-                                        We aim to do so by developing a universal modeling language\ntend any other software modeling language, or as a basis for a                                        for quantum software. Rather than develop such a language from\ncompletely new language. We argue that these extensions are both                                      scratch, we have decided to start from the well-known Unified\nnecessary and sufficient to model, abstractly, any piece of quantum                                   Modeling Language (UML)[3], and introduce a minimum set of\nsoftware. Finally, we provide a small set of examples that showcase                                   extensions that allow it to effectively model quantum software.\nthe effectiveness of the extension set.                                                                   Assuming UML to be a shared common-language upon which\n                                                                                                      we can build, allows us to convey our original extensions much\nCCS CONCEPTS                                                                                          more succinctly. Our extension set can, however, be applied with\n                                                                                                      little or no modification to any other modeling language.\n\u2022 General and reference \u2192 General conference proceedings;\nDesign; \u2022 Software and its engineering \u2192 System descrip-\ntion languages; Unified Modeling Language (UML); Software\n                                                                                                      2   Q-UML\ndesign engineering; \u2022 Theory of computation \u2192 Quantum                                                 Before discussing in depth the extensions we are introducing, we\ncomputation theory; Quantum information theory.                                                       make a few fundamental observations on which we base the guiding\n                                                                                                      principles for our extension set.\nKEYWORDS                                                                                                 Our first observation is about the nature of quantum computa-\nquantum computing, software engineering, UML                                                          tion. The central difference between quantum and classical com-\n                                                                                                      putation is in how it achieves its goals. Quantum computers have\nACM Reference Format:                                                                                 access to quantum algorithms[7], and quantum data-structures[8],\nCarlos A. P\u00e9rez-Delgado and Hector G. Perez-Gonzalez. 2020. Towards a\n                                                                                                      that are unavailable to classical computers\u2014hence their perfor-\nQuantum Software Modeling Language. In IEEE/ACM 42nd International\n                                                                                                      mance advantage. Algorithms and data-structures are, however,\nConference on Software Engineering Workshops (ICSEW\u201920), May 23\u201329, 2020,\nSeoul, Republic of Korea. ACM, New York, NY, USA, 3 pages. https://doi.org/                           implementation details. Algorithms are an essential design choice\n10.1145/3387940.3392183                                                                               while programming in the small. However, they are more often\n                                                                                                      than not completely ignored in large-scale software architectural\n1     INTRODUCTION                                                                                    design. For instance, UML diagrams seldom portray algorithms and\n                                                                                                      data-structures beyond a very high-level design perspective.\nQuantum computation rose to prominence after the discovery of\n                                                                                                         It would seem then that quantum computation introduces noth-\nquantum algorithms[5, 7] that can efficiently perform tasks that\n                                                                                                      ing to computation that needs to be captured in a software design\nare intractable classically. These discoveries propelled research and\n                                                                                                      diagram. This is not the case, and the reason for this is our second\ninterest in quantum computation. Today, there exists prototype\n                                                                                                      observation. Quantum computation changes the very nature of in-\nquantum hardware with computational capabilities beyond that of\n                                                                                                      formation itself. Quantum information is much richer than classical\nany classical machine[1]. Further applications of quantum theory\n                                                                                                      information. It is also much more challenging to store, transmit,\nto computation have also been made in several areas of theory of\n                                                                                                      and receive. If a module (class, object, etc.) needs to store, transmit\ncomputing, such as models of computation[6], data structures[8],\n                                                                                                      or receive quantum information, then this is an important design\nand cryptography[2].\n                                                                                                      consideration\u2014which needs to be included in any effective software\n\u2217 Both authors contributed equally to this research.                                                  design.\n                                                                                                         A third observation here is that the classical vs. quantum nature\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed                 of the information used by a module is an important consideration\nfor profit or commercial advantage and that copies bear this notice and the full citation             both when discussing its internal implementation and its interface.\non the first page. Copyrights for components of this work owned by others than the                    Furthermore, these two are separate and independent considera-\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission         tions.\nand/or a fee. Request permissions from permissions@acm.org.                                              A classical module, implementing some classical behavior, would\nICSEW\u201920, May 23\u201329, 2020, Seoul, Republic of Korea                                                   have no need, or capability, to communicate quantum data. A quan-\n\u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-7963-2/20/05. . . $15.00                                                          tum module may or may not have to; i.e. a module\u2019s quantum\nhttps://doi.org/10.1145/3387940.3392183                                                               behavior may be completely part of its internal implementation\n\n\n                                                                                                442\n\fand not appear as part of its interface. For instance, take a module                 (5) (Quantum Communication): Quantum and classical mod-\nimplementing Shor\u2019s algorithm. Shor\u2019s algorithm uses quantum                             ules can communicate with each other as long as their inter-\neffects to efficiently factor a large integer into its prime factors.                    faces are compatible, i.e. the quantum module has classical\nThe implementation of this module must necessarily be quantum.                           inputs and/or outputs that can interface with the classical\nBoth the input (the large integer) and the output (the prime factors),                   module.\nconsist of classical information. And hence, the interface of such a                 We will argue in Sec. 2.3 how these extensions are not only nec-\nmodule can be strictly classical.                                                 essary, but also sufficient in order to design and represent quantum\n    More generally, we can conceive of quantum software modules                   software. First, in the following two sections we put these principles\nthat have all classical inputs and outputs (like the above example),              into practice as a set of concrete extensions to UML.\nall quantum inputs and outputs, or a mix of both. A quantum soft-\nware design must address, for each individual interface element,                  2.1    Class Diagram Extensions\nwhether it is classical input/output, or if it is quantum. In short,\n                                                                                  UML is a very graphical language, meant to convey a lot of meaning\nwhether a module communicates classically or via quantum infor-\n                                                                                  in a very small amount of space. As such, it makes sense to use a\nmation, and whether its internal implementation requires quantum\n                                                                                  graphical way to represent quantum software elements. We chose to\nhardware are important considerations that need to be captured in\n                                                                                  do this by use of bold text to denote quantum elements, and double\na design document.\n                                                                                  lines to denote a quantum relationship or quantum communication.\n    The importance of such labelling should be clear. Quantum data\ncan only be stored and transmitted with special hardware designed\nto do so. More importantly, from an abstract, device-independent,\nstrictly software perspective: quantum and classical information\nare not interchangeable. Classical information is clone-able and\nadmits fanout operations, while quantum information (in general)\ndoes not. On the other hand, quantum information has a much\nlarger state-space.\n    Finally, it is true that quantum information is strictly a super-set\nof classical information\u2014and hence a quantum module can commu-\nnicate any classical information it desires using a quantum interface\nelement. We argue, however, that using a quantum interface ele-\nment and messaging when classical would suffice is bad quantum\nsoftware design, for the reasons stated above.\n    In summary, the guiding principles behind any quantum software\nmodeling language must include the following:\n\n   (1) (Quantum Classes): Whenever a software module makes\n        use of quantum information, either as part of its internal\n        state/implementation, or as part of its interface, this must be\n        clearly established in a design document.\n   (2) (Quantum Elements): Each module interface element (e.g.                    Figure 1: Q-UML class diagram of Shor\u2019s Algorithm. Quan-\n        public functions/methods, public variables) and internal state            tum classes and interface elements are presented in bold\n        variables can be either classical or quantum, and must be                 text, and quantum relationships use double-lines.\n        labelled accordingly.\n      (a) (Quantum Variables): Each variable should be labelled                       For attributes, the name will be bold if it is represented using\n           as classical or quantum. If the model represents data types,           quantum information. For methods, we use the following conven-\n           the variables should also specify the classical (e.g. integer,         tion. If any of the inputs are quantum, these are bold. If the output\n           string) or quantum (e.g. qubit, qubit array, quantum graph             or datatype of the method is quantum, then the datatype should also\n           state) data type,                                                      be bold. For backwards compatibility with regular UML, whenever\n     (b) (Quantum Operations): For each operation, both the in-                   the input or output datatypes of a method are omitted, these will be\n           put and output should be clearly labelled as either classical          assumed to be classical in nature. If a class/object has any quantum\n           or quantum. Whether the operation internally operates                  attributes or methods then it itself is considered quantum, and its\n           quantumly should also be labelled.                                     name shall also be bold.\n   (3) (Quantum Supremacy): A module that has at least one                            Relationships between classes will use double-lines whenever the\n        quantum element is to be considered a quantum software                    relationship is quantum in nature. For inheritance, if the superclass\n        module, otherwise it is a classical module. Quantum and                   is quantum then the subclass, and the inheritance relationship, will\n        classical modules should be clearly labelled as such.                     also be quantum. (the converse is not necessarily true however).\n   (4) (Quantum Aggregation): Any module that is composed of                      In the case of aggregation and composition, if a class/object being\n        one or more quantum modules will itself be considered a                   aggregated/composed is quantum, then the class/object to which\n        quantum module, and must be labelled as such.                             it is aggregated/composed into, as well as that relationship will\n\n\n\n\n                                                                            443\n\falso be quantum. Association relationships do not have any special            can (and therefore should) be communicated classically. This dia-\nrules, beyond the need of a quantum class/object to have a classical          gram showcases the level of granularity available to us using these\ninterface if it is to associate with classical classes/objects.               diagrams with the proposed extensions.\n   Fig. 1 showcases a Q-UML diagram that exemplifies the above\nrules.                                                                        2.3     Discussion\n                                                                              We have proposed a minimal series of extensions to existing soft-\n2.2    Sequence Diagram Extensions                                            ware modeling languages. We exemplify our additions in UML,\nSequence diagrams in UML allow us to portray the dynamic rela-                but these extensions are easily applicable to any other modeling\ntionship between modules in a software program. As we did before              language, or be used as the basis for a new modeling language.\nfor static relationships, we extend the existing language in order to            We\u2019ve argued the necessity of each of the extensions in previous\nallow us to differentiate between classical and quantum messages.             sections. We can argue as well, that these extensions are not only\nAs previously discussed, this is essential information. Quantum               necessary, but also sufficient to fully model quantum software.\ninformation behaves differently from classical information; it can            To make this argument, we appeal to the fact that all quantum\nstore/portray different data; it admits different operations; and, it         computation is simulable using classical computation albeit with\nrequires different hardware to store, send, and receive.                      an efficiency loss. Other than their use of quantum information and\n                                                                              algorithms, quantum computers are indistinct from classical ones.\n                                                                              Hence, from a high-level design perspective, the only information\n                                                                              element that needs to be considered when developing quantum\n                                                                              software is when quantum (rather than classical) information is\n                                                                              being used.\n                                                                                 The one remaining information element we have not discussed\n                                                                              is algorithm efficiency. If quantum computation is to be used, it\n                                                                              will most likely be due to the efficient algorithms at its disposal.\n                                                                              That said, algorithm efficiency is not a solely quantum consider-\n                                                                              ation. UML itself does not inherently have language elements for\n                                                                              algorithm efficiency (beyond user-defined notes). It does, however,\n                                                                              have several extensions used and proposed for this purpose(see\n                                                                              e.g.[4]). Other modeling languages may also have definite algorithm\n                                                                              efficiency elements. We argue that it is best to use existing language\n                                                                              elements when they are available.\n\n                                                                              ACKNOWLEDGMENTS\n                                                                              CP-D would like to acknowledge funding through the EPSRC Quan-\n                                                                              tum Communications Hub (EP/T001011/1). The authors would also\nFigure 2: Q-UML sequence diagram of Shor\u2019s Algorithm.                         like to thank Joanna I. Ziembicka for useful comments during the\nQuantum classes are presented in bold text, and quantum                       preparation on this manuscript.\nmessages use double-lines.\n                                                                              REFERENCES\n                                                                              [1] Frank Arute et. al. 2019. Quantum supremacy using a programmable supercon-\n   Like before, we make use of bold text to markup quantum mod-                   ducting processor. Nature 574, 7779 (2019), 505\u2013510. https://doi.org/10.1038/\n                                                                                  s41586-019-1666-5\nules, and double lines to portray quantum messages. Fig. 2 shows a            [2] Charles H Bennett and Gilles Brassard. 2014. Quantum cryptography: public key\nQ-UML sequence diagram. Note how even though the relationship                     distribution and coin tossing. Theor. Comput. Sci. 560, 12 (2014), 7\u201311.\nbetween Shorfactor and ShorOrder is quantum, the messaging                    [3] Grady Booch, James Rumbaugh, and Ivar Jacobson. 2005. Unified Modeling Lan-\n                                                                                  guage User Guide, The (2nd Edition) (Addison-Wesley Object Technology Series).\nbetween them is not. This illustrates an important point. A module                Addison-Wesley Professional.\nis marked as quantum if it uses quantum resources in any form,                [4] C. Canevet, S. Gilmore, J. Hillston, M. Prowse, and P. Stevens. 2003. Performance\neither directly as part of its internal implementation or as part of              modelling with the Unified Modelling Language and stochastic process algebras.\n                                                                                  IEE Proceedings - Computers and Digital Techniques 150, 2 (March 2003), 107\u2013120.\nan aggregated module. If a sub-module (in UML a composed class                    https://doi.org/10.1049/ip-cdt:20030084\nor object) is quantum, then the encompassing module must also be              [5] Lov K. Grover. 1996. A Fast Quantum Mechanical Algorithm for Database\n                                                                                  Search. In Proceedings of the Twenty-eighth Annual ACM Symposium on The-\nmarked as quantum. In a static (e.g. class) diagram, the quantum                  ory of Computing (STOC \u201996). ACM, New York, NY, USA, 212\u2013219. https:\ncomposition relationships inform us\u2014especially in the case of a                   //doi.org/10.1145/237814.237866\nseemingly classical module that does not in itself use quantum                [6] Carlos A. P\u00e9rez-Delgado and Donny Cheung. 2007. Local unitary quantum cellular\n                                                                                  automata. Phys. Rev. A 76 (Sep 2007), 032320. Issue 3. https://doi.org/10.1103/\nresources\u2014which composed modules are using quantum resources.                     PhysRevA.76.032320\n   Also, note the communication between the objects ShorOrder                 [7] Peter W Shor. 1994. Algorithms for quantum computation: Discrete logarithms\nand QFT_n. The module QFT_n operates on a quantum state.                          and factoring. In Proceedings 35th annual symposium on foundations of computer\n                                                                                  science. Ieee, 124\u2013134.\nHence, both \u2018set\u2019 messages are quantum. Likewise, the return mes-             [8] Liming Zhao, Carlos A. P\u00e9rez-Delgado, and Joseph F. Fitzsimons. 2016. Fast graph\nsages \u03c1 and \u03c1 \u0002 are quantum states. However, the request to perform               operations in quantum computation. Phys. Rev. A 93 (Mar 2016), 032314. Issue 3.\n                                                                                  https://doi.org/10.1103/PhysRevA.93.032314\na quantum Fourier transform (QFT) or a QFT inverse operation\n\n\n\n\n                                                                        444\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nFrank Arute et.al. 2019. Quantum supremacy using a programmable superconducting processor. Nature 574, 7779 (2019), 505\u2013510. https://doi.org/10.1038/s41586-019-1666-5 ", " Charles H Bennett and Gilles Brassard. 2014. Quantum cryptography: public key distribution and coin tossing. Theor. Comput. Sci. 560, 12 (2014), 7\u201311. ", " Grady Booch, James Rumbaugh, and Ivar Jacobson. 2005. Unified Modeling Language User Guide, The (2nd Edition) (Addison-Wesley Object Technology Series). Addison-Wesley Professional."], "publish_date": "2020-05-23", "approved": true}}, {"_index": "articles", "_id": "L-twCI0BUzn7PtN-PItD", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "How to Teach Software Modeling", "abstract": "To enhance motivation of students to study software engineering, some way of \ufb01nding balance between the scienti\ufb01c aspect and the practical aspect of software engineering is required. In this paper, we claim that teaching multiple software modeling techniques from a uni\ufb01ed viewpoint is a good way of obtaining the balance and attracting the students\u2019 interest as well.", "authors": ["Tetsuo Tamai"], "institutions": ["Graduate School of Arts and Sciences", "The University of Tokyo"], "keywords": ["software modeling", "software engineering education", "UML"], "full_text": "                                How to Teach Software Modeling\n\n                                                             Tetsuo Tamai\n                                                Graduate School of Arts and Sciences\n                                                      The University of Tokyo\n                                                     3-8-1 Komaba, Meguro-ku\n                                                      Tokyo 153-8902, Japan\n                                                           tamai@acm.org\n\nABSTRACT                                                                     technology and may generate anti-intellectualism. A good balance\nTo enhance motivation of students to study software engineering,             between the scientific aspect and the practical aspect of software\nsome way of finding balance between the scientific aspect and the            engineering should always be pursued.\npractical aspect of software engineering is required. In this paper,            In our view, teaching various software modeling techniques is a\nwe claim that teaching multiple software modeling techniques from            good way to achieve balanced software engineering education. It\na unified viewpoint is a good way of obtaining the balance and               is needless to say that model is a key concept and modeling is an\nattracting the students\u2019 interest as well.                                   essential skill in software engineering. There are a variety of mod-\n                                                                             eling techniques; some are intuitive and quite accessible to novices,\n                                                                             while some are highly sophisticated and attract theory oriented stu-\nCategories and Subject Descriptors                                           dents and researchers.\nK.3.2 [Computers and Education]: Computer and Information                       In this paper, we would like to show that it is effective to teach\nScience Education\u2014computer science education; D.2.1 [Software                multiple modeling techniques from a unified viewpoint. It is based\nEngineering]: Requirements/Specification\u2014modeling                            on our experience of teaching software engineering courses at sev-\n                                                                             eral universities in Japan. Recently, the author published a textbook\nGeneral Terms                                                                on software engineering, specifically focused on software model-\n                                                                             ing (unfortunately, it is written in Japanese)[1]. The book covers\nDesign                                                                       the whole area of software engineering, including design, testing\n                                                                             and evolution but the modeling part has a role of attracting inter-\nKeywords                                                                     ests of intelligent students, who may not have much experience in\n                                                                             developing real scale software systems. It also gives a consistent\nsoftware modeling, software engineering education, UML\n                                                                             viewpoint penetrating through various techniques employed in dif-\n                                                                             ferent stages of software engineering.\n1.    INTRODUCTION\n   Software engineering education at universities faces a common             2.     MODELING TECHNIQUES\nproblem; that is regular students do not usually have experience of             In software engineering, models are used for various purposes,\ndeveloping software for practical use and thus are not motivated for         e.g. life cycle model, process model, project model, product model,\nsoftware engineering aiming at high quality software production              quality model, domain model, requirements model, design model,\nby a project team or a persistent organization. Software projects            object model, data model, etc. In the following, we basically focus\nconducted by students simulating real scale software development             on requirements and design models but most of the discussions will\nmay help enhance students\u2019 motivation, although it requires a lot of         hold for other kinds of models.\nefforts to prepare such projects and manage them.                               Teaching modeling is almost equal to teaching abstraction. Mod-\n   Another way of solving this problem is to teach those who al-             els are constructed through capturing the crucial properties and\nready have real experience in industry. In our case, there are cur-          structure of the target, abstracting away irrelevant details. Thus,\nrently five Ph. D. students under the author\u2019s supervision who are           learning how to model is a good training for mastering abstraction.\nworking at companies as well as doing research in our lab. As\na by-product, interactions between the part-time students and the            2.1      Graph Representation of Models\nother regular students stimulate each other, particularly enlighten-            Many software models are represented with diagrams. Wide ac-\ning the regular students to practical software issues. However, too          ceptance of UML symbolizes the trend that diagrams are often pre-\nmuch emphasis on practicality may bring negligence to science and            ferred to textual languages. Among many types of diagrams, graph\n                                                                             structured diagrams are by far the most widely used. The reasons\n                                                                             may be as follows.\n                                                                                  1. A most fundamental way for human mind to understand the\n                                                                                     world is by regarding it as consisting of a set of conceptual\n                                                                                     units and a set of relations between them. Conceptual units\n                                                                                     can be naturally illustrated with boxes or circles or whatever\n                                                                                     closed figures and relations can be illustrated with lines or ar-\nCopyright is held by the author/owner.\nICSE\u201905, May 15\u201321, 2005, St. Louis, Missouri, USA.                                  rows connecting such figures, corresponding to vertices and\nACM 1-58113-963-2/05/0005.                                                           edges of graphs, respectively.\n\n\n\n                                                                       609\n\f   2. It is easy to draw graph structured diagrams by hand or with                 Static models and dynamic models may not be easily confused\n      drawing tools.                                                            but confusion between different dynamic models are often observed,\n                                                                                e.g. data flow and control flow or state transition and activity tran-\n   3. Concepts and algorithms of the graph theory are available                 sition. Since graphs are intuitively understandable, their semantics\n      and often useful in analyzing models represented by graphs.               are apt to be understood ambiguously or misunderstood.\n      A typical example is reasoning on transitive relations by trac-\n      ing along paths of graphs. Also, the concept of subgraph is\n      highly useful in decomposing higher-level models or cluster-\n                                                                                3.    UML\n      ing lower-level models.                                                     UML diagrams can also be viewed in terms of graph structures.\n                                                                                Table 2 shows graph structures of five UML diagrams.\n  Accordingly, a number of models share the same structure of\ngraphs. Table 1 shows graph structures of some typical models.\n                                                                                          Table 2: Graph structures of UML diagrams\n\n          Table 1: Graph structures of typical models                                diagram                    vertex          edge\n                                                                                     class diagram              class           generalization,\n   model               vertex              edge                                                                                 composition,\n   Data flow           process             data flow                                                                            association\n   ER                  entity              relationship                              state machine              state           transition\n   State transition    state               transition                                activity diagram           activity        control flow\n   JSD                 process             data stream connection                    collaboration diagram      object          message flow\n                                           state vector connection                   sequence diagram           message         message flow\n   Activity            activity            control flow                                                         anchor point\n   Petri net           place, transition   fire and token flow\n\n                                                                                   It is usually not desirable to teach UML per se. UML is a col-\n                                                                                lection of miscellaneous diagrams and its specification is continu-\n2.2     Commonality and Difference between                                      ously changing. For the pedagogical purpose, UML had better be\n        Models                                                                  regarded as a catalogue of analysis and design know-how collected\n    It is pedagogical to let students notice the common structure               around diagrammatic representations. Diagrams should be selected\nshared by a number of models. However, the apparent resemblance                 according to the policy of how to teach modeling methods.\noften causes confusion. Such confusion can be observed not only                    Each UML diagram contains overly rich constructs, which some-\nin software modeling graphs but in many diagrams found in daily                 times blur the essential property of the model. For example, the\nnewspapers, magazines, reports, proposals and other documents. It               activity diagram is essentially a control flow diagram but it also in-\nis often the case that one vertex denotes a type of things and an-              cludes a notation for data flow description. From the stance of em-\nother denotes quite a different type on the same diagram or one                 phasizing differences between various models, it is not appropriate\ntype of edges co-exist with edges with different meaning. Thus,                 to include such ad hoc constructs. By the same token, the collab-\nit is important to make students consciously aware the differences              oration diagram (, renamed to \u201ccommunication diagram\u201d in UML\nbetween different models. We often experience that when we let                  2) is explained to have the equivalent semantics as the sequence\nstudents draw data flow diagrams who appear to have understood                  diagram. But if that is the case, significance of the collaboration\nthe data flow model perfectly, the diagrams turn out to be some-                diagram is considerably limited. The author prefers to regard it as\nthing like control flow graphs.                                                 showing collaboration relations between objects, integrating a set\n    To show the difference, it is instructive to categorize models rep-         of different sequence diagrams.\nresented by graphs. Basically, there are two categories.\n                                                                                4.    CONCLUSION\n   1. Static models:\n                                                                                   Software modeling is important by itself but teaching modeling\n      An edge connecting vertex A and vertex B represents a rela-               in the software engineering course has at least two additional mean-\n      tion between A and B. When the edge is undirected, it means               ings. One is to give a bird\u2019s-eye view to the whole software engi-\n      \u201cA and B are in some relation\u201d and when directed, it means                neering through the standpoint of modeling technology. The other\n      \u201cA has a relation with B\u201d. Typical examples include entity                is to attract interest of good students who may not have much expe-\n      relationship model, class diagram and semantic network.                   rience in developing a real-scale software but possess intelligence\n   2. Dynamic models:                                                           and will to attack complexity of modern software construction.\n\n      An edge from vertex A to B denotes a move from A to B.\n      The edge in this case is always directed. There are two sub-\n                                                                                5.    REFERENCES\n      categories:                                                               [1] T. Tamai. Foundations of Software Engineering. Iwanami\n                                                                                    Shoten, Tokyo, Japan, 2004. in Japanese.\n        (a) The case where a view of control moves from A to B.\n            Examples are control flow model and state transition\n            model.\n        (b) The case where data or objects flow from A to B. Exam-\n            ples are data flow model, work flow model, and trans-\n            portation flow model.\n\n\n\n                                                                          610\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": [" T. Tamai. Foundations of Software Engineering. Iwanami Shoten, Tokyo, Japan, 2004. in Japanese. ", "\n\nT. Tamai. Foundations of Software Engineering. Iwanami Shoten, Tokyo, Japan, 2004. in Japanese. ", ""], "publish_date": "2004-01-01", "approved": true}}, {"_index": "articles", "_id": "K-ttCI0BUzn7PtN-dYui", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Framing the News: From Human Perception to Large Language Model Inferences", "abstract": "Identifying the frames of news is important to understand the articles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.", "authors": ["David Alonso del Barrio", "Daniel Gatica-Perez"], "institutions": ["Idiap Research Institute", "EPFL"], "keywords": ["Covid-19 no-vax", "news framing", "GPT-3", "prompt-engineering", "transformers", "large language models"], "full_text": "                      Framing the News:\n    From Human Perception to Large Language Model Inferences\n                           David Alonso del Barrio                                                                      Daniel Gatica-Perez\n                                  ddbarrio@idiap.ch                                                                        gatica@idiap.ch\n                               Idiap Research Institute                                                           Idiap Research Institute and EPFL\n                                     Switzerland                                                                             Switzerland\nABSTRACT                                                                                              1   INTRODUCTION\nIdentifying the frames of news is important to understand the arti-                                   In recent years, there has been a proliferation in the use of concepts\ncles\u2019 vision, intention, message to be conveyed, and which aspects                                    such as data journalism, computational journalism, and computer-\nof the news are emphasized. Framing is a widely studied concept                                       assisted reporting [15] [29], which all share the vision of bridging\nin journalism, and has emerged as a new topic in computing, with                                      journalism and technology. The progress made in NLP has been\nthe potential to automate processes and facilitate the work of jour-                                  gradually integrated into the journalistic field [5][8][54]. More\nnalism professionals. In this paper, we study this issue with articles                                specifically, machine learning models based on transformers have\nrelated to the Covid-19 anti-vaccine movement. First, to under-                                       been integrated in the media sector in different tasks [41] such as\nstand the perspectives used to treat this theme, we developed a                                       the creation of headlines with generative languages models [17],\nprotocol for human labeling of frames for 1786 headlines of No-                                       summarization of news articles [28][27], false news detection [49],\nVax movement articles of European newspapers from 5 countries.                                        and topic modeling and sentiment analysis [25]. The development of\nHeadlines are key units in the written press, and worth of analysis                                   large language models such as GPT-3 [9], BLOOM [51] or ChatGPT\nas many people only read headlines (or use them to guide their                                        show a clear trend towards human-machine interaction becoming\ndecision for further reading.) Second, considering advances in Nat-                                   easier and more intuitive, opening up a wide range of research\nural Language Processing (NLP) with large language models, we                                         possibilities. At the same time, the use of these models is also\ninvestigated two approaches for frame inference of news headlines:                                    associated with a lack of transparency regarding how these models\nfirst with a GPT-3.5 fine-tuning approach, and second with GPT-                                       work, but efforts are being made to bring some transparency to\n3.5 prompt-engineering. Our work contributes to the study and                                         these models, and to analyze use cases where they can be useful and\nanalysis of the performance that these models have to facilitate                                      where they cannot [35]. Based on the premises that these models\njournalistic tasks like classification of frames, while understanding                                 open up a wide range of research directions [7], and that at the same\nwhether the models are able to replicate human perception in the                                      time (and needless to say) they are not the solution to all problems,\nidentification of these frames.                                                                       we are interested in identifying use cases and tasks where they\n                                                                                                      can be potentially useful, while acknowledging and systematically\nCCS CONCEPTS                                                                                          documenting their limitations [56]. More specifically, the aim of\n                                                                                                      this work is to analyze the performance of GPT-3.5 for a specific\n\u2022 Computing methodologies \u2192 Information extraction; \u2022\n                                                                                                      use case, namely the analysis of frames in news, from an empirical\nHuman-centered computing \u2192 Text input.\n                                                                                                      point of view, with the objective of shedding light on a potential\n                                                                                                      use of generative models in journalistic tasks.\nKEYWORDS                                                                                                 Frame analysis is a concept from journalism, which consists of\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering, trans-                                      studying the way in which news stories are presented on an issue,\nformers, large language models                                                                        and what aspects are emphasized: Is a merely informative vision\n                                                                                                      given in an article? Or is it intended to leave a moral lesson? Is\nACM Reference Format:                                                                                 a news article being presented from an economic point of view?\nDavid Alonso del Barrio and Daniel Gatica-Perez. 2023. Framing the News:                              Or from a more human, emotional angle? The examples above\nFrom Human Perception to Large Language Model Inferences . In Inter-                                  correspond to different frames with which an article can be written.\nnational Conference on Multimedia Retrieval (ICMR \u201923), June 12\u201315, 2023,\n                                                                                                         The concept of news framing has been studied in computing as\nThessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3591106.3592278\n                                                                                                      a step beyond topic modeling and sentiment analysis, and for this\n                                                                                                      purpose, in recent years, pre-trained language models have been\n                                                                                                      used for fine-tuning the classification process of these frames [60]\nPermission to make digital or hard copies of all or part of this work for personal or                 [10], but the emergence of generative models opens the possibility\nclassroom use is granted without fee provided that copies are not made or distributed                 of doing prompt-engineering of these classification tasks, instead\nfor profit or commercial advantage and that copies bear this notice and the full citation             of the fine-tuning approach investigated so far.\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or                   Our work aims to address this research gap by posing the fol-\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission         lowing research questions:\nand/or a fee. Request permissions from permissions@acm.org.\n                                                                                                         RQ1: What are the main frames in the news headlines about\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.                     the anti-vaccine movement, as reported in newspapers across 5\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00                                                          European countries?\nhttps://doi.org/10.1145/3591106.3592278\n\n\n\n\n                                                                                                627\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                        David Alonso del Barrio and Daniel Gatica-Perez\n\n\n    RQ2: Can prompt engineering be used for classification of head-         message? Does the story make reference to morality, God, and other\nlines according to frames?                                                  religious tenets? Does the story offer specific social prescriptions\n    By addressing the above research questions, our work makes the          about how to behave?\", and so on for each of the frame types. This\nfollowing contributions:                                                    categorization of frames has been used in various topics such as\n    Contribution 1. We implemented a process to do human an-                climate change [18] [19], vaccine hesitance [13], or immigration\nnotation of the main frame of 1786 headlines of articles about the          [34].\nCovid-19 no-vax movement, as reported in 19 newspapers from 5                   We now compare the two approaches on a common topic, such\nEuropean countries (France, Italy, Spain, Switzerland and United            as Covid-19. Ebrahim et al. [21] followed an inductive approach\nKingdom.) At the headline level, we found that the predominant              in which the frames were not predefined but emerged from the\nframe was human interest, where this frame corresponds to a per-            text (e.g., deadly spread, stay home, what if, the cost of Covid-19)\nsonification of an event, either through a statement by a person,           using headlines as the unit of analysis. In contrast, the deductive\nor the explanation of a specific event that happened to a person.           approach has studied very different labels. El-Behary et al. [22]\nFurthermore, we found a large number of headlines annotated as              followed the method of yes/no questions, but in addition to the\ncontaining no frame, as they simply present information without             5 generic frames presented before, they also used blame frame\nentering into evaluations. We also found that for all the countries         and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5\ninvolved, the distribution of frame types was very similar, i.e., hu-       generic frames with yes/no questions, while Catal\u00e1n-Matamoros et\nman interest and no frame are the two predominant frames. Finally,          al. [14] used the 5 frames and read the headline and subheadline\nthe generated annotations allowed to subsequently study the per-            to decide the main frame. Table 1 summarizes some of the the\nformance of a large language model.                                         existing approaches. This previous work showed how frame labels\n    Contribution 2. We studied the performance of GPT-3.5 on                can be different, and also that frame analysis has been done at both\nthe task of frame classification of headlines. In addition to using         headline and article levels. These two approaches (inductive and\nthe fine-tuning approach from previous literature, we propose an            deductive) that originated in journalism have since been replicated\nalternative approach for frame classification that requires no labeled      in the computing literature.\ndata for training, namely prompt-engineering using GPT-3.5. The                 We decided to follow the deductive approach because a prede-\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy            fined list of frames allows to compare among topics, countries,\n(slightly higher than other smaller models), and that the prompt-           previous literature, and also because they represent a fixed list of\nengineering approach results in lower performance (49% accuracy.)           labels for machine classification models. Furthermore, the induc-\nOur analysis also shows that the subjectivity of the human labeling         tive approach tends to be more specific to a topic, and from the\ntask has an effect on the obtained accufracy.                               computing viewpoint, past work has tried to justify topic modeling\n    The paper is organized as follows. In Section 2, we discuss related     as a technique to extract frames from articles.\nwork. In Section 3, we describe the news dataset. In Section 4, we              Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex-\ndescribe the methodology for both human labeling and machine                traction technique. They argued that topics can be interpreted as\nclassification of news frames. We present and discuss results for           frames if three requirements are met: frames are operationalized as\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide          connections between concepts; subject-specific data is selected; and\nconclusions in Section 7.                                                   topics are adequately validated as frames, for which they suggested\n                                                                            a practical procedure. This approach was based on the choice of a\n                                                                            specific topic (e.g., climate change) and the use of Latent Dirichlet\n2    RELATED WORK                                                           Allocation (LDA) as a technique to extract a number of subtopics.\nFraming has been a concept widely studied in journalism, with a             In a second phase, a qualitative study of the top 10 words of each\ndefinition that is rooted in the study of this domain [23]: \u201cTo frame       subtopic was performed, and the different subtopics were elimi-\nis to select some aspects of a perceived reality and make them more         nated or grouped, reducing the number and establishing a tentative\nsalient in a communicating text, in such a way as to promote a par-         description. In a third phase, the top 10 articles belonging to that\nticular problem definition, causal interpretation, moral evaluation,        frame/topic were taken, and if the description of the topic fitted\nand/or treatment recommendation for the item described.\u201d                    at least 8 of the 10 articles, that topic/frame remained. The frames\n    For frame recognition, there are two main approaches: the induc-        found in this article were: green growth, emission cuts, negotiations\ntive approach [16], where one can extract the frames after reading          and treaties, environmental risk, cost of carbon emissions, Chinese\nthe article, and the deductive approach [38], where a predefined            emissions, economics of energy production, climate change, en-\nlist of frames exists and the goal is to interpret if any of them ap-       vironmental activism, North-South burden sharing, state leaders\npears in the article. In the deductive case, there are generic frames       negotiating, and citizen participation.\nand subject-specific frames, and the way to detect them typically               From Entman\u2019s definition of frame [23], it seems that the deduc-\ninvolves reading and identifying one frame at a time, or through            tive approach is more refined than the inductive approach (which\nanswers to yes/no questions that represent the frames. Semetko et           seems to resemble the detection of sub-themes.) For example, with\nal. [52] used 5 types of generic frames (attribution of responsibility,     regard to climate change, there are stories on how people have been\nhuman interest, conflict, morality, and economic consequences)              affected by climate change from an emotional point of view, thus\nbased on previous literature, and they defined a list of 20 yes/no          personalizing the problem. In this case, we could categorize the\nquestions to detect frames in articles. For instance, the questions         corresponding frame as human interest, as the writer of the article\nabout morality are the following: \"Does the story contain any moral         is selecting \"some aspects of a perceived reality and make them\n\n\n\n\n                                                                      628\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                            ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\nmore salient\". The language subtleties with which news articles are            Table 1: Summary of deductive approaches for frame analysis\npresented cannot be captured with basic topic modeling.\n   Isoaho et al.[30] held the position that while the benefits of                  Ref Frames                                         Goal                       Technique          Number\n                                                                                                                                                                                    samples\n                                                                                                                                                                                                of\n\nscale and scope in topic modeling were clear, there were also a                    [12] 15 generic frames: \"Economic\", \"Capac-        To label frames of full    Reading the\n                                                                                                                                                                                    20000 articles\nnumber of problems, namely that topic outputs do not correspond                         ity and resources\", \"Morality\", \"Fair-        articles                   full    article,\n                                                                                        ness and equality\", \"Legality, constitu-                                 the annotator\nto the methodological definition of frames, and thus topic modeling                     tionality and jurisprudence\", \"Policy                                    defines     the\n                                                                                        prescription and evaluation\", \"Crime                                     main frame\nremained an incomplete method for frame analysis. Topic modeling,                       and punishment\", \"Security and de-\n                                                                                        fense\", \"Health and safety\", \"Quality of\nin the practice of journalistic research, is a useful technique to deal                 life\", \"Cultural identity\", \"Public opin-\nwith the large datasets that are available, yet is often not enough to                  ion\", \"Political\", \"External regulation\n                                                                                        and reputation\", \"Other\".\ndo more thorough analyses [31]. In our work, we clearly notice that                [33] 15 generic frames                             Classification             BERT based\n                                                                                                                                                                                    12000 articles\n                                                                                                                                                                 models\nframe analysis is not topic modeling. For example, two documents                   [52] 5 generic frames: \"human interest\",           To label frames of full    Yes/No ques-       2600 articles\n                                                                                        \"conflict\", \"morality\", \"attribution of       articles                   tions.             and 1522 tv\ncould be about the same topic, say Covid-19 vaccination, but one                        responsibility\", and \"economic conse-                                                       news stories\narticle could emphasize the number of deaths after vaccination,                         quences\".\n                                                                                   [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic         To label frames of full    Reading the        2990 headlines\nwhile the other emphasized the role of the vaccine as a solution to                     opinion\u201d, \u201cSociety/Culture\u201d, and              articles/ Classification   full    article,\n                                                                                        \u201cEconomic consequences\u201d , \u201c2nd                                           the annotator\nthe epidemic.                                                                           Amendment\u201d (Gun Rights), \u201cGun                                            defines     the\n                                                                                        control/regulation\u201d, \u201cMental health\u201d,                                    main frame.\n   We also consider that the larger the number of possible frame                        \u201cSchool/Public space safety\u201d, and                                        BERT based\n                                                                                        \u201cRace/Ethnicity\u201d.                                                        models\ntypes, the more likely it is to end up doing topic modeling instead of             [22] 5 generic frames + blame frame and            To label frames of full    Yes/No ques-       1170 articles\nframe analysis. Using a deductive approach, Dallas et al. [12] created                  fear frame\n                                                                                   [1] 5 generic frames\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        6713 articles\na dataset with articles about polemic topics such as immigration,                                                                     articles                   full    article,\n                                                                                                                                                                 the annotator\nsame sex marriage, or smoking, and they defined 15 types of frames:                                                                                              defines     the\n                                                                                                                                                                 main frame.\n\"economic, capacity and resources, morality, fairness and equality,                [50] 5 generic frames + pandemic frames            To label frames of full    Yes/No ques-       2742 articles\nlegality, constitutionality and jurisprudence, policy prescription and             [14] 5 generic frames, journalistic role and\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        131 headlines +\nevaluation, crime and punishment, security and defense, health and                      pandemic frames                               articles                   headline and       subheadlines\n                                                                                                                                                                 subheadline,\nsafety, quality of life, cultural identity, political, external regulation                                                                                       the annotator\n                                                                                                                                                                 defines     the\nand reputation, other\". In this case, they authors did not use a list                                                                                            main frame.\nof questions. Instead, for each article, annotators were asked to\nidentify any of the 15 framing dimensions present in the article\nand to label text blurbs that cued them (based on the definitions of\n                                                                               the potential applications, and the challenges to face when using\neach of the frame dimensions) and decide the main frame of each\n                                                                               this approach. Prompt engineering applications include knowledge\narticle. In our case, we followed the idea of detecting the main frame\n                                                                               probing [46], information extraction [53], NLP reasoning [57], ques-\nby reading the text instead of answering questions, but instead of\n                                                                               tion answering [32], text generation [20], multi-modal learning [58],\nusing the 15 frames proposed in [12] , we used the 5 generic frames\n                                                                               and text classification [24], the latter being the prompt-engineering\nproposed in [52].\n                                                                               use case in our work. Puri et al.[45] presented a very interesting\n   A final decision in our work was the type of text to analyze,\n                                                                               idea that we apply to our classification task. This consists of pro-\nwhether headlines or whole article. For this decision, the chosen\n                                                                               viding the language model with natural language descriptions of\nclassification method was also going to be important. For example,\n                                                                               classification tasks as input, and training it to generate the correct\nKhanehzar et al. [33] used traditional approaches such as SVMs as\n                                                                               answer in natural language via a language modeling objective. It is\nbaseline, and demonstrated the improvement in frame classifica-\n                                                                               a zero-shot learning approach, in which no examples are used to\ntion with the use of pre-trained languages models such as BERT,\n                                                                               explain the task to the model. Radford et al. [48] demonstrated that\nRoBERTa and XLNet, following a fine-tuning approach, setting\n                                                                               language models can learn tasks without any explicit supervision.\nas input text a maximum of 256 tokens (although the maximum\n                                                                               We have followed this approach to find an alternative way to do\nnumber of input tokens in these models is 512 tokens.) Liu et al.\n                                                                               frame analysis.\n[37] classified news headlines about the gun problem in the United\n                                                                                   As mentioned before, the emergence of giant models like GPT-3,\nStates, arguing for the choice of headlines as a unit of analysis\n                                                                               BLOOM, and ChatGPT are a very active research topic. To the best\nbased on previous journalism literature [6], [44], that advocated\n                                                                               of our knowledge, on one hand our work extends the computational\nfor the importance and influence of headlines on readers and the\n                                                                               analysis of news related to the covid-19 no-vax movement, which\nsubsequent perception of articles. From a computational viewpoint,\n                                                                               illustrates the influence of the press on the ways societies think\nusing headlines is also an advantage, since you avoid the 512 token\n                                                                               about relevant issues [40], [59], and on the other hand it adds to\nlimitation in BERT-based models. Therefore, we decided to work\n                                                                               the literature of human-machine interaction, regarding the design\nwith headlines about a controversial issue, namely the Covid-19\n                                                                               of GPT-3 prompts for classification tasks [39], [2].\nno-vax movement.\n   Continuing with the question of the methods used for classi-\nfication, much work has been developed in prompt engineering,\n                                                                               3        DATA: EUROPEAN COVID-19 NEWS\nespecially since the release of GPT-3. Liu et al.[36] presented a good                  DATASET\noverview of the work done on this new NLP paradigm, not only                   We used part of the European Covid-19 News dataset collected in\nexplaining the concept of prompt engineering, but also the differ-             our recent work [3]. This dataset contains 51320 articles on Covid-\nent strategies that can be followed both in the design of prompts,             19 vaccination from 19 newspapers from 5 different countries: Italy,\n\n\n\n\n                                                                         629\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                                                   David Alonso del Barrio and Daniel Gatica-Perez\n\n\nFrance, Spain, Switzerland and UK. The articles cover a time period                                                   [52] with 5 generic frames (attribution of responsibility, human\nof 22 months, from January 2020 to October 2021. All content was                                                      interest, conflict, morality, and economic consequences) plus one\ntranslated into English to be able to work in a common language.                                                      additional \u2019no-frame\u2019 category. Two researchers were engaged to\nThe dataset was used for various analyses, such as name entity                                                        annotate a sample of the collected newspaper articles following a\nrecognition, sentiment analysis, and subtopic modeling, to under-                                                     three-phase training procedure.\nstand how Covid-19 vaccination was reported in Europe through                                                            In the first phase, annotators had to read the codebook and get\nthe print media (in digital format.) The subtopic modeling analysis                                                   familiar with the task. In the second phase, they were asked to\nrevealed a subsample of articles on the no-vax movement, which is                                                     identify the main frame in the same subset of 50 headlines. At the\nthe one we have used in this paper. We took the headlines of the                                                      end of the second phase, the intercoder reliability (ICR) was 0.58\narticles associated with the no-vax movement, selecting all articles                                                  between the 2 annotators. We analyzed those cases where there\ncontaining any of the keywords in Table 2 in the headline or in the                                                   were discrepancies, and observed that in some cases, there was not a\nmain text. This corresponds to a total of 1786 headlines.                                                             unique main frame, because both annotators had valid arguments to\n                                                                                                                      select one of the frames. In other cases, the discrepancies were due\n      Table 2: Keywords used to identify no-vax articles                                                              to slight misunderstanding of the definitions. In the third phase, the\n                                                                                                                      annotators coded again 50 headlines, and the ICR increased to was\n                                                         Keywords                                                     0.66. We realized that the possibility of having two frames remained.\n NO VAX TOPIC    \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"         They discussed the cases in which they had disagreed, and if the\n                                                                                                                      other person\u2019s arguments were considered valid, it could be said that\n   In Table 3, we show the number of headlines per country and                                                        there were two frames. After this three-phase training procedure,\nnewspaper. France is the country with the most no-vax articles in                                                     annotators were ready to annotate the dataset independently. We\nthe corpus, with 523 articles, followed by Italy with 508. However,                                                   divided the dataset into two equal parts, and each person annotated\nnote that there are 6 newspapers from France, while only 2 from                                                       893 headlines.\nItaly. Corriere della Sera is the newspaper that dealt most frequently\nwith the subject (429 articles), while The Telegraph is the second                                                    4.2    Fine-tuning GPT-3.5 and BERT-based\none (206 articles). The total number of articles normalized by the                                                           models\nnumber of newspapers per country is also shown in the last column                                                     With the annotated dataset, we investigated two NLP approaches:\nof the Table. Using these normalized values, the ranking is Italy,                                                    the first one involves fine-tuning a pre-trained model; the second\nUK, France, Switzerland, and Spain.                                                                                   one is prompt engineering. Pre-trained language models have been\n\n Table 3: Number of headlines by newspaper and country\n\n      COUNTRY               NEWSPAPER                HEADLINES             TOTAL (NORM. TOTAL)\n      FRANCE                    La Croix                     94                       523 (87.1)\n                               Le Monde                     125\n                               Les Echos                     49\n                              Liberation                     97\n                            Lyon Capitale                     8\n                            Ouest France                    150\n        ITALY             Corriere della Sera               429                      508 (254.0)\n                            Il Sole 24 Ore                   79\n        SPAIN                 20 minutos                     27                       303 (50.5)\n                                  ABC                        50\n                               El Diario                     32                                                                    Figure 1: Pre-train, fine-tune, prompt\n                               El Mundo                      77\n                              El Espa\u00f1ol                     22\n                           La Vanguardia                     95\n                               24 heures                     97                       230 (76.6)\n   SWITZERLAND\n                              La Libert\u00e9                     22\n                                                                                                                      trained with large text strings based on two unsupervised tasks,\n                               Le Temps                     111                                                       next sentence prediction and masked language model. Figure 1\n                           The Irish News                    16                      222 (111.0)\n UNITED KINGDOM\n                           The Telegraph                    206                                                       summarizes these techniques.\n                                                                                         1786                            In the first approach, a model with a fixed architecture is pre-\n                                                                                                                      trained as a language model (LM), predicting the likelihood of the\n                                                                                                                      observed textual data. This can be done due to the availability of\n4 METHODOLOGY                                                                                                         large, raw text data needed to train LMs. This learning process can\n                                                                                                                      produce general purpose features of the modeled language. The\n4.1 Human labeling of news frames                                                                                     learning process produces robust, general-purpose features of the\nTo carry out the labeling of the frames in our corpus of headlines, we                                                language being modeled. The above pre-trained LM is then adapted\nfirst designed a codebook, which contained the definitions of each                                                    to different downstream tasks, by introducing additional parameters\nof the frame types and a couple of examples of each type, as well                                                     and adjusting them using task-specific objective functions. In this\nas a definition of the corpus subject matter and definitions of the                                                   approach, the focus was primarily on goal engineering, designing\nconcept of frame analysis, so that the annotators could understand                                                    the training targets used in both the pre-training and the fine-tuning\nthe task to be performed. The codebook follows the proposed by                                                        stages [36].\n\n\n\n\n                                                                                                                630\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                    ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n    We present an example to illustrate the idea. Imagine that the        adjective was matched with a sentiment. In a similar fashion, we\ntask is sentiment analysis, and we have a dataset with sentences          decided to build a thesaurus of adjectives that define each of the\nand their associated sentiment, and a pre-trained model, which is a       frames. For instance, the human interest frame could be \u2019interest-\nsaved neural network trained with a much larger dataset. For that         ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be:\npre-trained model to address the target task, we unfreeze a few of        \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou-\nthe top layers of the saved model base and jointly train both the         blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined,\nnewly-added classifier layers and the last layers of the base model.      we needed to define the prompt in order to get, as an answer, one\nThis allows to \"fine-tune\" the higher-order feature representations       of the adjectives in our thesaurus to match them with the frame.\nin the base model to make them more relevant for the sentiment            We used the GPT-3 playground using the headline as input and\nanalysis task. In this way, instead of having to obtain a very large      asking for the frame as output, but the strategy did not work. In\ndataset with target labels to train a model, we can reuse the pre-        our final experiment, instead of giving the headline as input, we\ntrained model and use a much smaller train dataset. We use a part         gave the definitions of each type of frame plus the headline, and we\nof our dataset as examples for the model to learn the task, while         asked the model to choose between the different types of frames\nthe other part of the dataset is used to evaluate model performance.      as output. In this way, the output of the model was directly one of\n    Previous works related to frame classification in the computing       the frames, and we avoided the step of matching adjectives with\nliterature have used fine-tuning, BERT-based models. In our work,         frames. An example is shown in Figure 2.\nwe have done the same as a baseline, but we aimed to go one step\nfurther and also produce results using fine-tuning of GPT-3.5.\n\n\n4.3    Prompt-engineering with GPT-3.5\nModel fine-tuning has been widely used, but with the emergence\nof generative models such as GPT-3, another way to approach\nclassification tasks has appeared. The idea is to use the pre-trained\nmodel directly and convert the task to be performed into a format\nas close as possible to the tasks for which it has been pre-trained.\nThat is, if the model has been pre-trained from next word prediction          Figure 2: GPT-3.5 for frame inference: input and output\nas in the case of GPT-3, classification can be done by defining a\nprompt, where the input to the model is an incomplete sentence,               For the GPT-3 configuration 1 , there are 3 main concepts:\nand the model must complete it with a word or several words, just\n                                                                                \u2022 TEMPERATURE [0-1]. This parameter controls randomness,\nas it has been trained. This avoids having to use part of the already\n                                                                                  lowering it results in less random completions.\nlabeled dataset to teach the task to be performed to the model, and\n                                                                                \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus\na previous labeling is not needed [36].\n                                                                                  sampling.\n    In this approach, instead of adapting pre-trained LMs to down-\n                                                                                \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-\nstream tasks via objective engineering, downstream tasks are re-\n                                                                                  mum number of tokens to generate,\nformulated to look more like those solved during the original LM\n                                                                                \u2022 MODEL. GPT-3 offer four main models with different levels\ntraining with the help of a textual prompt. For example, when recog-\n                                                                                  of power, suitable for different tasks. Davinci is the most\nnizing the emotion of a social media post, \u201cI missed the bus today.\u201d,\n                                                                                  capable model, and Ada is the fastest.\nwe may continue with a prompt \u201cI felt so _\u201d, and ask the LM to\nfill the blank with an emotion-bearing word. Or if we choose the          After testing with the GPT-3 playground and varying different\nprompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may           hyper-parameters to assess performance, we set the temperature to\nbe able to fill in the blank with a French translation. In this way,      0, since the higher the temperature the more random the response.\nby selecting the appropriate prompts, we can influence the model          Furthermore, the Top-p parameter was set to 1, as it would likely\nbehavior so that the pre-trained LM itself can be used to predict the     get a set of the most likely words for the model to choose from. The\ndesired output, even without any additional task-specific training        maximum number of tokens was set to 2; in this way, the model\n[36].                                                                     is asked to choose between one of the responses. As a model, we\n    We use this emerging NLP approach to classify frames at headline      used the one with the best performance at the time of experimental\nlevel. We are not aware of previous uses of this strategy to classify     design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\nframes as we propose here. The idea is the following. Prompt engi-\nneering consists of giving a prompt to the model, and understands         5     RESULTS: HUMAN LABELING OF FRAMES\nthat prompt as an incomplete sentence. To do prompt engineer-                   IN NO-VAX NEWS HEADLINES (RQ1)\ning with our dataset, we needed to define an appropriate prompt           In this section, we present and discuss the results of the analysis\nthat would produce the headline frames as output. We defined sev-         related to our first RQ.\neral experiments with the Playground of GPT-3, in order to find              Figure 3 shows the distribution of frames per country at headline\nthe best prompt for our task. In our initial experiments, we fol-         level, with human interest and no-frame being the predominant\nlowed existing approaches in prompt engineering to do sentiment\nanalysis, where the individual answer was an adjective, and this          1 https://beta.openai.com/docs/introduction\n\n\n\n\n                                                                    631\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                       David Alonso del Barrio and Daniel Gatica-Perez\n\n\nones. Attribution of responsibility is the third one except in Switzer-\nland, where the corresponding frame is conflict. Finally, morality\nand economic are the least represented in the dataset for every\ncountry.\n\n\n\n\n                                                                            Figure 4: Non-normalized monthly distribution of frames.\n\n\n\n                                                                            detail the negative and positive sentiment of each frame category,\n                                                                            we observed a few trends:\n                                                                                \u2022 Attribution of responsibility: Negative sentiment represents\n                                                                                  30-40% of the cases, while positive tone is only found in\n                                                                                  residual form in Italy, Switzerland, and the United Kingdom.\n                                                                                \u2022 Conflict: Negative sentiment represents 20-35% of the cases.\nFigure 3: Non-normalized distribution of frames per country                     \u2022 Economic: Predominantly neutral, with only negative tone\n                                                                                  in Italy and UK (in the latter case, all headlines with this\n    The monthly distribution of frames aggregated for all countries               frame were considered negative.)\nis shown in Fig. 4. We can see two big peaks, the first one in January          \u2022 Human interest: Negative sentiment represents 30-40% of\n2021 and the second one in August 2021. In all countries, the vac-                the cases, while positive tone is only found in residual form\ncination process started at the end of December 2020, so it makes                 in Italy, Spain, and Switzerland.\nsense that the no-vax movement started to be more predominant in                \u2022 Morality: Predominantly neutral, with negative tone in Italy,\nthe news in January 2021. Human interest is the most predominant                  Switzerland, and the United Kingdom,\nframe. Manual inspection shows that this is because the headlines               \u2022 No frame: 20-30% of negative content.\nare about personal cases of people who are pro- or anti- vaccine.\nAttribution of responsibility is also present. Manual inspection in-\ndicates that local politicians and health authorities had to make\ndecisions about who could be vaccinated at the beginning of the\nprocess. The second peak at the end of summer 2021 coincided\nwith the health pass (also called Covid passport in some countries),\nand we can observe a peak in the curve corresponding to the con-\nflict frame, reflecting the demonstrations against the measure of\nmandatory health passes taken by country governments.\n    In Figure 5, we compare the sentiment per frame and per country,\nto understand if there were any major differences. The sentiment\nanalysis labels were obtained using BERT-sent from the Hugging\nFace package [47], used in our previous work (please refer to our\noriginal analysis in [3] for details.) We normalized the results be-\ntween 0 and 1 to compare frames between countries. We see that the\nsentiment is predominantly neutral (in blue). Examining in more              Figure 5: Sentiment of headline by frame and by country\n\n\n\n\n                                                                      632\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                   ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n   Regarding the results of the annotation process, the fact that the             On the other hand, BERT is open-source, while GPT-3 has an\ndistribution of the 6 frame types is relatively similar between coun-         economic cost as the use of the model is not free, which monetarily\ntries suggests that the anti-vaccine movement issue was treated               limits the number of experiments that can be performed with it,\nin a similar way in these countries. The fact that human interest             as well as the different configurations one can explore to improve\nis the most dominant frame indicates that this issue was treated              performance. This is important because much of the improvement\nfrom a more human and emotional approach, with headlines about                in performance requires empirical explorations of model parameters\npersonal experiences, celebrities giving their opinion about vacci-           More specifically, the cost of an experiment for each of the folds has\nnation, and politicians defending vaccine policies. Moreover, the             a cost of 4 dollars (at the time of writing this paper.) This represents\nreason for many headlines being classified as no-frame is partly              a limitation in practice.\ndue to how data was selected. We chose articles that contained                    Furthermore, GPT-3 has a significant carbon footprint. Similarly,\nwords related to no-vax, either in the headline or in the article. This       for prompt engineering (discussed in the next subsection), choosing\nresulted in many headlines not containing anything specific related           the right prompt (i.e., the words that best define the task so that the\nto no-vax, while the no-vax content was actually included in the              model is able to perform adequately) is also based on trial and error.\nmain text of the corresponding articles.                                      This also has an impact on carbon footprint. In connection with\n   It is worth mentioning that prior to obtaining the results, we had         this topic, Strubell et al.[55] argue that improvements in the accu-\nexpected that attribution of responsibility would be among the most           racy of models depend on the availability of large computational\nprominent frames, since governments took many measures such as                resources, which involve large economic and environmental costs.\nmandatory health pass requirements to access certain sites; we had            A criticism has been made as \u2019the rich get richer\u2019, in the sense that\nalso expected that the conflict frame would be prominent, since               not all research groups have sufficient infrastructure resources and\nthere were many demonstrations in Europe. In reality, however,                access to funding needed to use these models and improve their\nthese frames categories were not reflected as frequently at the               performance. Also in relation to this analysis, the work of Bender\nheadline level.                                                               et al. [4] evaluates the costs and risks of the use of large language\n   Regarding the analysis at the temporal level, it is clear that certain     models, stating that researchers should be aware of the impact that\nevents were captured by the press, such as the start of vaccination           these models have on the environment, and assess whether the\nor the mandatory vaccination passport.                                        benefits outweigh the risks. The work in [4] provides a very telling\n   Finally, the sentiment analysis of the different frames shows that         example, where people living in the Maldives or Sudan are affected\nthe predominant tone in all of them is neutral or negative, with very         by floods and pay the environmental price of training English LLMs,\nsimilar trends between countries. This association between senti-             when similar models have not been produced for languages like\nment analysis and frames has been discussed in previous literature            Dhivehi or Sudanese Arab. In short, there is a need to establish\n[11] [43].                                                                    ways to use this technological development responsibly, and it all\n                                                                              starts with being aware of the risks it presents.\n6     RESULTS: GPT-3.5 FOR FRAME\n      CLASSIFICATION OF HEADLINES (RQ2)                                       6.2    Prompt-engineering with GPT-3.5\nHere, we present and discuss the results related to our second RQ.            For each headline, we got the frame that the model considered the\n                                                                              most likely, and we compared these GPT-3.5 inferences with the\n6.1    Fine-tuning GPT-3.5                                                    frames labeled by the annotators. The agreement between model\nTable 4 shows the results of the 6-class classification task using            and annotator was of 49%. Analyzing the results, and specifically\n5-cross validation. Three models were used: GPT-3.5 and two BERT-             looking at the cases where the annotator and GPT-3.5 disagreed,\nbased models. We observe that, on average, GPT-3.5 performs better            we discovered that according to the frame definitions, the model\nthan the BERT-based models. This is somehow expected as GPT-                  in some cases proposed a frame that indeed made sense. This ob-\n3.5 is a much larger model. Overall, in the case of fine-tuning, the          servation, together with our previous experience in the annotation\nbest performance for the six-class frame classification task is 72%           process, where headlines could have more than one valid frame,\naccuracy, which is promising, with an improvement over previous               led us to design a second post-hoc experiment. We took all the\nmodels based on BERT. Yet, it should be noted that the performance            headlines where each of the two annotators had disagreed with\ndifferences are modest (2% improvement between GPT-3.5 and                    GPT-3.5, and we asked the annotators to state whether they would\nRoBERTa).                                                                     agree (or not) with each GPT-inferred label for a given headline.\n                                                                              It is important to emphasize that the annotators did not know the\n                                                                              origin of that label, i.e., they did not know if it was the label they\nTable 4: Classification results for six-class frame classifica-               had originally assigned, or if it was a random one. In this way, we\ntion and 5-fold cross validation                                              could quantify how GPT-3.5 worked according to valid arguments\n                                                                              provided by the annotators. In this post-hoc experiment, the model\n                                                                              agreed in 76% of cases with the annotators.\n    ACCURACY          0       1       2       3       4     AVERAGE\n                                                                                  Looking at the results of the classification models, the 49% accu-\n    BERT            0.68     0.69    0.72    0.64    0.70     0.67            racy of the prompt-engineering approach can be considered low,\n    RoBERTa         0.70     0.72    0.72    0.67    0.71     0.70\n                                                                              yet we consider that it is a valid avenue for further investigation,\n    GPT3            0.75     0.70    0.72    0.71    0.71     0.72\n                                                                              as in the second post-hoc analysis, we found that the model agrees\n\n\n\n\n                                                                        633\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                David Alonso del Barrio and Daniel Gatica-Perez\n\n\nwith human annotators in 76% of the cases. Clearly, framing in-             and in an post-doc experiment, the human-machine agreement\nvolves aspects of subjectivity [42]. Much of what we do as people           reached 76%. These results have opened several new directions for\nhas a subjective component, influenced by how we feel or how we             future work.\nexpress opinions.\n   News reading is never fully objective, and the annotators en-            ACKNOWLEDGMENTS\ngaged in the frame classification task, influenced by their personal        This work was supported by the AI4Media project, funded by the\nstate of mind, experience, and culture, may perceive information            European Commission (Grant 951911) under the H2020 Programme\ndifferently. Monarch affirms that \"for simple tasks, like binary labels     ICT-48-2020. We also thank the newspapers for sharing their online\non objective tasks, the statistics are fairly straightforward to decide     articles. Finally, we thank our colleagues Haeeun Kim and Emma\nwhich is the \u2018correct\u2019 label when different annotators disagree. But        Bouton-Bessac for their support with annotations, and Victor Bros\nfor subjective tasks, or even objective tasks with continuous data,         and Oleksii Polegkyi for discussions.\nthere are no simple heuristics for deciding what the correct label\nshould be\" [42].                                                            REFERENCES\n   Subjectivity is involved in both the generation and perception            [1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online\nof information: the assumption that there is only one frame is com-              media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.\n                                                                                 Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.\nplicated by the point of view of the reader. In the case of news, the        [2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess\ninformation sender (the journalist) has an intention, but the receiver           Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021.\n(the reader) plays a role and is influenced by it. In psychology, this           RAFT: A real-world few-shot text classification benchmark. arXiv preprint\n                                                                                 arXiv:2109.14076 (2021).\nis known as the lens model of interpersonal communication, where             [3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press\nthe sender has certain objectives, but the receiver can interpret                Cover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35\u201343.\n                                                                                 https://doi.org/10.1145/3512732.3533588\nor re-interpret what the sender wants to say, with more or less              [4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\naccuracy [26].                                                                   Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\n   Following this discussion on subjectivity, the question arose as to           Be Too Big? (2021), 610\u2013623.\n                                                                             [5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in\nwhat would happen if, instead of headlines, we used the complete                 journalism: A boon or bane? In Optimization in machine learning and applications.\narticle as a source of analysis. We wondered if longer text could                Springer, 155\u2013167.\nmake the frame labeling task clearer than when using headlines.              [6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015.\n                                                                                 Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013\nYet another possible hypothesis is that having to read longer texts              2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.\ncould lead to the same subject being presented from different angles.        [7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.\n                                                                                 https://doi.org/10.48550/ARXIV.2212.14402\nPlease recall that in the existing literature discussed in Section 2,        [8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe,\nboth headlines and full articles have been used from frame analysis              Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour-\n(see Table 1.) This remains as an issue for future work.                         nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695.\n                                                                             [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                 Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                 Askell, et al. 2020. Language models are few-shot learners. Advances in neural\n7    CONCLUSIONS                                                                 information processing systems 33 (2020), 1877\u20131901.\n                                                                            [10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H\nIn this paper, we first presented an analysis of human-generated                 De Vreese. 2014. Teaching the computer to code frames in news: Comparing\nnews frames on the covid-19 no-vax movement in Europe, and                       two supervised machine learning approaches to frame analysis. Communication\n                                                                                 Methods and Measures 8, 3 (2014), 190\u2013206.\nthen studied different approaches using large language models for           [11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond\nautomatic inference of frames. We conclude by answering the two                  words: Applying cluster and sentiment analysis to news coverage of the nuclear\nresearch questions we posed:                                                     power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.\n                                                                            [12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015.\n   RQ1: What are the main frames in the news headlines about the                 The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015),\ncovid-19 anti-vaccine movement in 5 European countries? After                    438\u2013444. https://doi.org/10.3115/v1/P15-2072\nannotating the headlines, we found that of the 1786 headlines,              [13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age\n                                                                                 of coronavirus and fake news: analysis of journalistic sources in the Spanish\nthe predominant frame is human interest (45.3% of cases), which                  quality press. International Journal of Environmental Research and Public Health\npresents a news item with an emotional angle, putting a face to a                17, 21 (2020), 8136.\n                                                                            [14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust\nproblem or situation. We also found that a substantial proportion                of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n\nof headlines were annotated as not presenting any frame (40.2% of                social 74 (2019), 786\u2013802.\ncases). Finally, the other frame types are found more infrequently.         [15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology\n                                                                                 for evaluating data journalism, computational journalism, and computer-assisted\n   RQ2: Can prompt engineering be used for classification of head-               reporting. Digital journalism 3, 3 (2015), 331\u2013348.\nlines according to frames? We first used fine-tuning of a number of         [16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News\nlanguage models, and found that GPT-3.5 produced classification ac-              Framing Analysis. Routledge, 151\u2013172.\n                                                                            [17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,\ncuracy of 72% on a six-frame classification task. This represented a             1 (2021), 113\u2013118.\nmodest 2% improvement over BERT-based models, at a significantly            [18] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A de-\n                                                                                 ductive frame-analysis of Dutch and French climate change coverage dur-\nlarger environmental cost. We then presented a new way of classi-                ing the annual UN Conferences of the Parties. Public Understanding of\nfying frames using prompts. At the headline level, inferences made               Science 19, 6 (2010), 732\u2013742.         https://doi.org/10.1177/0963662509352044\nwith GPT-3.5 reached 49% of agreement with human-generated                       arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.\n                                                                            [19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame-\nframe labels. In many cases, the GPT-3.5 model inferred frame                    analysis of Dutch and French climate change coverage during the annual UN\ntypes that were considered as valid choices by human annotators,                 Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.\n\n\n\n\n                                                                      634\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                        ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig.             [46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with\n     2020. Gsum: A general framework for guided neural abstractive summarization.                 mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).\n     arXiv preprint arXiv:2010.08014 (2020).                                                 [47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent.\n[21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news                https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.\n     headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA            [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\n     Gesondheid (Online) 27 (2022), 1\u20138.                                                          et al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n[22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative                      1, 8 (2019), 9.\n     Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).                 [49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022.\n[23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.               Fake News Classification using transformer based enhanced LSTM and BERT.\n     McQuail\u2019s reader in mass communication theory 390 (1993), 397.                               International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105.\n[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language                    https://doi.org/10.1016/j.ijcce.2022.03.003\n     models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).                [50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational\n[25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across                    predictors. Cuadernos. info 50 (2021), 91\u2013112.\n     four nations: a topic modeling and sentiment analysis approach. Ieee Access 9           [51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\n     (2021), 36645\u201336656.                                                                         Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\n[26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En-                     Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language\n     coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior.                     model. arXiv preprint arXiv:2211.05100 (2022).\n     Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https:              [52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content\n     //doi.org/10.1037//0022-3514.66.2.398                                                        Analysis of Press and Television News. Journal of Communication 50 (06 2000),\n[27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based                 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x\n     transformer architectures for long document summarization. In Proceedings of            [53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em-\n     the 16th Conference of the European Chapter of the Association for Computational             manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin\n     Linguistics: Main Volume. 1792\u20131810.                                                         Van Durme. 2021. Constrained language models yield few-shot semantic parsers.\n[28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news                      arXiv preprint arXiv:2104.08768 (2021).\n     summarization using transformers. In Sustainable Advanced Computing. Springer,          [54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab-\n     249\u2013259.                                                                                     orative Work trends on Media Organizations: Mixing Qualitative and Quan-\n[29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar-                titative Approaches. Studies in Media and Communication 5 (04 2017), 63.\n     chy of hybridity in data and computational journalism. Digital Journalism 5, 2               https://doi.org/10.11114/smc.v5i1.2279\n     (2017), 159\u2013176.                                                                        [55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy\n[30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and                 considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n     text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021),     [56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\n     300\u2013324.                                                                                     the capabilities, limitations, and societal impact of large language models. arXiv\n[31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative                  preprint arXiv:2102.02503 (2021).\n     analysis of large amounts of journalistic texts using topic modelling. Digital          [57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.\n     journalism 4, 1 (2016), 89\u2013106.                                                              arXiv preprint arXiv:1806.02847 (2018).\n[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we              [58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and\n     know what language models know? Transactions of the Association for Computa-                 Felix Hill. 2021. Multimodal few-shot learning with frozen language models.\n     tional Linguistics 8 (2020), 423\u2013438.                                                        Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.\n[33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling                   [59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of\n     Political Framing Across Policy Issues and Contexts. In ALTA.                                technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.\n[34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate             [60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling\n     during election years: Focus on generic frames. The Communication Review 21, 2               for frame analysis: A study of media debates on climate change in India and USA.\n     (2018), 89\u2013115.                                                                              Global Media and Communication 18, 1 (2022), 91\u2013112.\n[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\n     hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n     2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n     (2022).\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\n     Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of\n     prompting methods in natural language processing. (2021). https://doi.org/10.\n     48550/ARXIV.2107.13586\n[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019.\n     Detecting frames in news headlines and its application to analyzing news framing\n     trends surrounding US gun violence. In Proceedings of the 23rd conference on\n     computational natural language learning (CoNLL).\n[38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames:\n     Toward Improving Reliability and Validity. Journal of Communication 58 (06\n     2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x\n[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and\n     David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3\n     User Simulation in Conversational AI. In Proceedings of the 4th Conference on\n     Conversational User Interfaces. 1\u20136.\n[40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social\n     computing for verifying social media content in breaking news. IEEE Internet\n     Computing 22, 2 (2018), 83\u201389.\n[41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021. \u2018Our task is to demystify fears\u2019:\n     Analysing newsroom management of automation in journalism. Journalism 22,\n     9 (2021), 2203\u20132221.\n[42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and\n     Annotation for Human-centered AI. Manning. https://books.google.ch/books?\n     id=LCh0zQEACAAJ\n[43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of\n     media frames: Strengths, weaknesses, and opportunities. Political Communication\n     38, 1-2 (2021), 159\u2013181.\n[44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to\n     news discourse. Political communication 10, 1 (1993), 55\u201375.\n[45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative\n     language models. arXiv preprint arXiv:1912.10165 (2019).\n\n\n\n\n                                                                                       635\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020). ", " Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138. ", " Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021)."], "publish_date": "2023-06-12", "approved": true}}, {"_index": "articles", "_id": "LOtvCI0BUzn7PtN-E4uJ", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "Large Language Model Augmented Narrative Driven Recommendations", "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.", "authors": ["Sheshera Mysore", "Andrew McCallum", "Hamed Zamani"], "institutions": ["University of Massachusetts Amherst"], "keywords": ["narrative-driven recommendation", "information access", "user-item interaction", "data augmentation", "retrieval models"], "full_text": "                 Large Language Model Augmented Narrative Driven\n                                Recommendations\n                  Sheshera Mysore                                              Andrew McCallum                                              Hamed Zamani\n          smysore@cs.umass.edu                                           mccallum@cs.umass.edu                                       hzamani@cs.umass.edu\n    University of Massachusetts Amherst                             University of Massachusetts Amherst                        University of Massachusetts Amherst\n                    USA                                                             USA                                                        USA\nABSTRACT                                                                                              interactions are effective, users soliciting recommendations often\nNarrative-driven recommendation (NDR) presents an information                                         start with a vague idea about their desired target items or may\naccess problem where users solicit recommendations with verbose                                       desire recommendations depending on the context of use, often\ndescriptions of their preferences and context, for example, travelers                                 missing in historical interaction data (Figure 1). In these scenarios,\nsoliciting recommendations for points of interest while describ-                                      it is common for users to solicit recommendations through long-\ning their likes/dislikes and travel circumstances. These requests                                     form narrative queries describing their broad interests and context.\nare increasingly important with the rise of natural language-based                                    Information access tasks like these have been studied as narrative-\nconversational interfaces for search and recommendation systems.                                      driven recommendations (NDR) for items ranging from books [5]\nHowever, NDR lacks abundant training data for models, and current                                     and movies [18], to points of interest [1]. Bogers and Koolen [5]\nplatforms commonly do not support these requests. Fortunately,                                        note these narrative requests to be common on discussion forums\nclassical user-item interaction datasets contain rich textual data,                                   and several subreddits1 , but, there is a lack of support for these\ne.g., reviews, which often describe user preferences and context                                      complex natural language queries in current recommenders.\n\u2013 this may be used to bootstrap training for NDR models. In this                                          However, with the emergence of conversational interfaces for\nwork, we explore using large language models (LLMs) for data                                          information access tasks, support for complex NDR tasks is likely\naugmentation to train NDR models. We use LLMs for authoring                                           to become necessary. In this context, recent work has noted an\nsynthetic narrative queries from user-item interactions with few-                                     increase in complex and subjective natural language requests com-\nshot prompting and train retrieval models for NDR on synthetic                                        pared to more conventional search interfaces [13, 34]. Furthermore,\nqueries and user-item interaction data. Our experiments demon-                                        the emergence of large language models (LLM) with strong lan-\nstrate that this is an effective strategy for training small-parameter                                guage understanding capabilities presents the potential for fulfilling\nretrieval models that outperform other retrieval and LLM baselines                                    such complex requests [9, 33]. This work explores the potential for\nfor narrative-driven recommendation.                                                                  re-purposing historical user-item recommendation datasets, tra-\n                                                                                                      ditionally used for training collaborative filtering recommenders,\nCCS CONCEPTS                                                                                          with LLMs to support NDR.\n                                                                                                          Specifically, given a user\u2019s interactions, \ud835\udc37\ud835\udc62 , with items and\n\u2022 Information systems \u2192 Recommender systems; Users and inter-\n                                                                                                      their accompanying text documents (e.g., reviews, descriptions)\nactive retrieval; \u2022 Computing methodologies \u2192 Natural language                                                   \ud835\udc41\ud835\udc62\ngeneration.                                                                                           \ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56 }\ud835\udc56=1  , selected from a user-item interaction dataset I, we\n                                                                                                      prompt InstructGPT, a 175B parameter LLM, to author a synthetic\nACM Reference Format:                                                                                 narrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the\nSheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large\n                                                                                                      query \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user\nLanguage Model Augmented Narrative Driven Recommendations. In Sev-\nenteenth ACM Conference on Recommender Systems (RecSys \u201923), Septem-\n                                                                                                      reviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based\nber 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages.                               on a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user doc-\nhttps://doi.org/10.1145/3604915.3608829                                                               ument, \ud835\udc51\ud835\udc56 . Then, a pre-trained LM based retrieval model (110M\n                                                                                                      parameters) is fine-tuned for retrieval on the synthetic queries and\n1     INTRODUCTION                                                                                    filtered reviews.\n                                                                                                          Our approach, which we refer to as Mint2 , follows from the\nRecommender systems personalized to users are an important com-\n                                                                                                      observation that while narrative queries and suggestions are often\nponent of several industry-scale platforms [16, 17, 46]. These sys-\n                                                                                                      made in online discussion forums, and could serve as training data,\ntems function by inferring users\u2019 interests from their prior inter-\n                                                                                                      the number of these posts and the diversity of domains for which\nactions on the platform and making recommendations based on\n                                                                                                      they are available is significantly smaller than the size and diversity\nthese inferred interests. While recommendations based on historical\n                                                                                                      of passively gathered user-item interaction datasets. E.g. while\nPermission to make digital or hard copies of all or part of this work for personal or                 Bogers and Koolen [5] note nearly 25,000 narrative requests for\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation             books on the LibraryThing discussion forum, a publicly available\non the first page. Copyrights for components of this work owned by others than the                    user-item interaction dataset for Goodreads contains interactions\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\n                                                                                                      with nearly 2.2M books by 460k users [43] .\nand/or a fee. Request permissions from permissions@acm.org.                                               We empirically evaluate Mint in a publicly available test collec-\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                               tion for point of interest recommendation: pointrec [1]. To train\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n                                                                                                      1 r/MovieSuggestions, r/booksuggestions, r/Animesuggest\nACM ISBN 979-8-4007-0241-9/23/09. . . $15.00\nhttps://doi.org/10.1145/3604915.3608829                                                               2 Mint: Data augMentation with INteraction narraTives.\n\n\n\n\n                                                                                                777\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                  Mysore, McCallum, Zamani\n\n\n\n\nFigure 1: An example narrative query soliciting point of                      Figure 2: The format of the prompt used in Mint for\ninterest recommendations. The query describes the users                       generating synthetic narrative queries from user-item\npreferences and the context of their request.                                 interaction with a large language model.\n\n\nour NDR models, we generate synthetic training data based on                  Besides creating queries for ad-hoc retrieval tasks, concurrent\nuser-item interaction datasets from Yelp. Models (110M parameters)         work of Leszczynski et al. [25] has also explored the creation of syn-\ntrained with Mint significantly outperform several baseline models         thetic conversational search datasets from music recommendation\nand match the performance of significantly larger LLM baselines            datasets with LLMs. The synthetic queries and user documents are\nautoregressively generating recommendations. Code and synthetic            then used to train bi-encoder retrieval models for conversational\ndatasets are available:3                                                   search. Our work resembles this in creating synthetic queries from\n                                                                           sets of user items found in recommendation interaction datasets.\n2    RELATED WORK                                                          However, it differs in the task of focus, creating long-form narra-\nData Augmentation for Information Access. A line of recent                 tive queries for NDR. Finally, our work also builds on the recent\nwork has explored using language models to generate synthetic              perspective of Radlinski et al. [36] who make a case for natural\nqueries for data augmentation to train models for information re-          language user profiles driving recommenders \u2013 narrative requests\ntrieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of     tie closely to natural language user profiles. Our work presents a\ninterest, a pre-trained language model is used to create synthetic         step toward these systems.\nqueries for the document collection. An optional filtering step ex-           Finally, while our work explores data augmentation from user-\ncludes noisy queries, and finally, a bi-encoder or a cross-encoder is      item interactions for a retrieval-oriented NDR task, prior work has\ntrained for the retrieval task. While earlier work of Ma et al. [31]       also explored data augmentation of the user-item graph for training\ntrain a custom query generation model on web-text datasets, more           collaborative filtering models. This work has often explored aug-\nrecent work has leveraged large language models for zero/few-shot          mentation to improve recommendation performance for minority\nquestion generation [7, 8, 15, 23]. In generating synthetic queries,       [12, 47] or cold-start users [11, 28, 45]. And has leveraged genera-\nthis work indicates the effectiveness of smaller parameter LLMs            tive models [11, 45] and text similarity models [28] for augmenting\n(up to 6B parameters) for generating synthetic queries in simpler          the user-item graph.\ninformation-retrieval tasks [7, 8, 23], and finds larger models (100B         Complex Queries in Information Access. With the advent\nparameters and above) to be necessary for harder tasks such as             of performant models for text understanding, focus on complex\nargument retrieval [15, 23]. Similar to this work, we explore the          and interactive information access tasks has seen a resurgence\ngeneration of synthetic queries with LLMs for a retrieval task. Un-        [2, 29, 32, 48]. NDR presents an example of this \u2013 NDR was first\nlike this work, we demonstrate a data augmentation method for              formalized in Bogers and Koolen [5] for the case of book recommen-\ncreating effective training data from sets of user documents found in      dation and subsequently studied in other domains [3, 4, 6]. Bogers\nrecommendation datasets rather than individual documents. Other            and Koolen [5] systematically examined narrative requests posted\nwork in this space has also explored training more efficient multi-        by users on discussion forums. They defined NDR as a task requir-\nvector models from synthetic queries instead of more expensive             ing item recommendation based on a long-form narrative query\ncross-encoder models [39] and generating queries with a diverse            and prior-user item interactions. While this formulation resembles\nrange of intents than the ones available in implicit feedback datasets     personalized search [42] and query-driven recommendation [20],\nto enhance item retrievability [35].                                       the length and complexity of requests differentiate these from NDR.\n                                                                           Other work has also demonstrated the effectiveness of re-ranking\n                                                                           initial recommendations from collaborative filtering approaches\n3 https://github.com/iesl/narrative-driven-rec-mint/\n\n\n\n\n                                                                     778\n\fLarge Language Model Augmented Narrative Driven Recommendations                                          RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\n\n\nFigure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering\nmodels for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with\na large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic\nqueries and user items.\n\n\nbased on the narrative query [18]. More recent work of Afzali et al.            3.2.1 Narrative Queries from LLMs. To author a narrative query \ud835\udc5e\ud835\udc62\n[1] formulate the NDR task without access to the prior interactions             for a user in I, we make use of the 175B parameter InstructGPT4\nof a user while also noting the value of contextual cues contained              model as our query generation model QGen. We include the text\n                                                                                                         \ud835\udc41\ud835\udc62\nin the narrative request. In our work, we focus on this latter for-             of interacted items {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 in the prompt for QGen, and instruct it\nmulation of NDR, given the lack of focus on effectively using the               to author a narrative query (Figure 2). To improve the coherence\nrich narrative queries in most prior work. Further, we demonstrate              of generated queries and obtain correctly formatted outputs, we\nthe usefulness of data augmentation from LLMs and user-item                     manually author narrative queries for 3 topically diverse users\ninteraction datasets lacking narrative queries.                                 based on their interacted items and include it in the prompt for\n   Besides this, a range of work has explored more complex, long-               QGen. The same three few shot examples are used for the whole\nform, and interactive query formulations for information access;                dataset I, and the three users were chosen from I. Generating\nthese resemble queries in NDR. Arguello et al. [2] define the tip of            narrative queries based on user interactions may also be considered\ntongue retrieval task, a known-item search task where user queries              a form of multi-document summarization for generating a natural\ndescribe the rich context of items while being unable to recall item            language user profile [36].\nmetadata itself. Mysore et al. [32] formulate an aspect conditional\nquery-by example task where results must match specific aspects of              3.2.2 Filtering Items for Synthetic Queries. Since we expect user\na long natural language query. And finally, a vibrant body of work              items to capture multiple aspects of their interests and generated\nhas explored conversational critiquing of recommenders where nat-               queries to only capture a subset of these interests, we only retain\n                                                                                                                    \ud835\udc41\ud835\udc62\nural language feedback helps tune the recommendations received                  some of the items present in {\ud835\udc51\ud835\udc56 }\ud835\udc56=1   before using it for training re-\nby users [30, 44, 49].                                                          trieval models. For this, we use a pre-trained language model to com-\n                                                                                pute the likelihood of the query given each user item, \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ),\n3 METHOD                                                                        and only retain the top \ud835\udc40 highly scoring item for \ud835\udc5e\ud835\udc62 , this re-\n                                                                                sults in \ud835\udc40 training samples per user for our NDR retrieval models:\n3.1 Problem Setup                                                                          \ud835\udc40 }. In our experiments, we use FlanT5 with 3B parame-\n                                                                                {(\ud835\udc5e\ud835\udc62 , \ud835\udc51\ud835\udc56 )\ud835\udc56=1\nIn our work, we define narrative-driven recommendation (NDR) to                 ters [14] for computing and follow Sachan et al. [40] for computing\nbe a ranking task, where given a narrative query \ud835\udc5e made by a user               \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ). Note that our use of \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ) represents a query-\n\ud835\udc62, a ranking system \ud835\udc53 must generate a ranking \ud835\udc45 over a collection               likelihood model classically used for ad-hoc search and recently\nof items C. Further, we assume access to a user-item interaction                shown to be an effective unsupervised re-ranking method when\n                                                              \ud835\udc41\ud835\udc62\ndataset I consisting of user interactions with items (\ud835\udc62, {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 ). We         used with large pre-trained language models [40].\nassume the items \ud835\udc51\ud835\udc56 to be textual documents like reviews or item\ndescriptions. While we don\u2019t assume there to be any overlap in the              3.2.3 Training Retrieval Models. We train bi-encoder and cross-\nusers making narrative queries or the collection of items C and the             encoder models for NDR on the generated synthetic dataset \u2013 com-\nuser-items interaction dataset I, we assume them to be from the                 monly used models in search tasks. Bi-encoders are commonly used\nsame broad domain, e.g., books, movies, points-of-interest.                     as scalable first-stage rankers from a large collection of items. On the\n                                                                                other hand, cross-encoders allow a richer interaction between query\n3.2    Proposed Method                                                          and item and are used as second-stage re-ranking models. For both\n                                                                                models, we use a pre-trained transformer language model architec-\nOur proposed method, Mint, for NDR, re-purposes a dataset of                    ture with 110M parameters, MPnet, a model similar to Bert [41].\n                                                           \ud835\udc41\ud835\udc62\nabundantly available user-item interactions, I = {(\ud835\udc62, {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 )} into          Bi-encoder models embed the query and item independently into\ntraining data for retrieval models by using LLMs as query gener-                high dimensional vectors: q\ud835\udc62 = MPNet(\ud835\udc5e\ud835\udc62 ), d\ud835\udc56 = MPNet(\ud835\udc51\ud835\udc56 ) and\n                                                                \ud835\udc41\ud835\udc62\nation models to author narrative queries \ud835\udc5e\ud835\udc62 : D = {(\ud835\udc5e\ud835\udc62 , {\ud835\udc51\ud835\udc56 }\ud835\udc56=1   )}.         rank items for the user based on the minimum L2 distance between\nThen, retrieval models are trained on the synthetic dataset D (Fig-\nure 3).                                                                         4 https://platform.openai.com/docs/models/gpt-3, text-davinci-003\n\n\n\n\n                                                                          779\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                   Mysore, McCallum, Zamani\n\n\nq\ud835\udc62 and d\ud835\udc56 . Embeddings are obtained by averaging token embeddings           dislikes). It also retains the users whose interests are summarizable\nfrom the final layer of MPNet, and the same model is used for both          by QGen. In the Yelp dataset, this results in 45,193 retained users.\nqueries and items. Cross-encoder models input both the query and            Now, 10,000 randomly selected users are chosen for generating syn-\nitem and output a score to be used for ranking\u0010 \ud835\udc60 = \ud835\udc53Cr ([\ud835\udc5e\ud835\udc62\u0011; \ud835\udc51\ud835\udc56 ]),       thetic narrative queries. For these users, a single randomly selected\nwhere \ud835\udc53Cr is parameterized as w\ud835\udc47 dropout W\ud835\udc47 MPNet(\u00b7) . We                   sentence from 10 of their reviews is included in the prompt (Figure\n                                                                            2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some\ntrain our bi-encoder model with a margin ranking loss: L\ud835\udc35\ud835\udc56 =\n\u00cd \u00cd\ud835\udc40                                \u2032                                       items are filtered out (\u00a73.2.2). Here, we exclude 40% of the items\n  \ud835\udc62 \ud835\udc56=1 max[\ud835\udc3f2(q\ud835\udc62 , d\ud835\udc56 ) \u2212 \ud835\udc3f2(q\ud835\udc62 , d\ud835\udc56 ) + \ud835\udeff, 0] with randomly sam-          for a user. This results in about 60,000 training samples for training\n                \u2032\npled negatives \ud835\udc51 and \ud835\udeff = 1. Our cross-encoders are trained with             BiEnc-Mint and CrEnc-Mint. These decisions were made manu-\n                            \u00cd \u00cd\ud835\udc40                \ud835\udc60\na cross-entropy loss: L\ud835\udc36\ud835\udc5f = \ud835\udc62 \ud835\udc56=1       log( \u00cd \ud835\udc52 \ud835\udc60 \u2032 ). For training, 4     ally by examining the resulting datasets and the cost of authoring\n                                                      \ud835\udc51\u2032 \ud835\udc52\nnegative example items \ud835\udc51 \u2032 are randomly sampled from ranks 100-             queries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230.\n300 from our trained bi-encoder. At test time, we retrieve the top\n200 items with our trained bi-encoder and re-rank them with the             4.1.3 Baselines. We compare BiEnc-Mint and CrEnc-Mint mod-\ncross-encoder - we evaluate both these components in experiments            els against several standard and performant retrieval model base-\nand refer to them as BiEnc-Mint and CrEnc-Mint.                             lines. These span zero-shot/unsupervised rankers, supervised bi-\n                                                                            encoders, unsupervised cross-encoders, and LLM baselines. BM25:\n4     EXPERIMENTS AND RESULTS                                               A standard unsupervised sparse retrieval baseline based on term\n                                                                            overlap between query and document, with strong generalization\nNext, we evaluate Mint on a publicly available test collection for\n                                                                            performance across tasks and domains [38]. Contriver: A BERT-base\nNDR and present a series of ablations.\n                                                                            bi-encoder model pre-trained for zero-shot retrieval with weakly su-\n                                                                            pervised query-document pairs [22]. MPNet-1B: A strong Sentence-\n4.1     Experimental Setup                                                  Bert bi-encoder model initialized with MPNet-base and trained on\n4.1.1 Datasets. We perform evaluations on an NDR dataset for                1 billion supervised query-document pairs aggregated from numer-\npoint-of-interest (POI) recommendation Pointrec [1]. Pointrec               ous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned\ncontains 112 realistic narrative queries (130 words long) obtained          on supervised question-passage pairs from MSMarco. UPR: A two-\nfrom discussion forums on Reddit and items pooled from baseline             stage approach that retrieves items with a Contriver bi-encoder\nrankers. The items are annotated on a graded relevance scale by             and re-ranks the top 200 items with a query-likelihood model using\ncrowd-workers and/or discussion forum members and further vali-             a FlanT5 model with 3B parameters [14, 40]. This may be seen\ndated by the dataset authors. The item collection C in Pointrec             as an unsupervised \u201ccross-encoder\u201d model. Grounded LLM: A re-\ncontains 700k POIs with metadata (category, city) and noisy text            cently proposed two-stage approach which autoregressively gener-\nsnippets describing the POI obtained from the Bing search engine.           ates ten pseudo-relevant items using an LLM (175B InstructGPT)\nFor test time ranking, we only rank the candidate items in the city         prompted with the narrative query and generates recommenda-\nand request category (e.g., \u201cRestaurants\u201d) of the query available in        tions grounded in C by retrieving the nearest neighbors for each\nPointrec - this follows prior practice to exclude clearly irrelevant        generated item using a bi-encoder [19]. We include one few-shot\nitems [1, 26]. We use user-item interaction datasets from Yelp to           example of a narrative query and recommended items in the prompt\ngenerate synthetic queries for training.5 Note also that we limit our       to the LLM. We run this baseline three times and report average\nevaluations to Pointrec since it presents the only publicly avail-          performance across runs. We report NDCG at 5 and 10, MAP, MRR,\nable, manually annotated, and candidate pooled test collection for          and Recall at 100 and 200. Finally, our reported results should be\nNDR, to our knowledge. Other datasets for NDR use document col-             considered lower bounds on realistic performance due to the un-\nlections that are no longer publicly accessible [24], contain sparse        judged documents (about 70% at \ud835\udc58 = 10) in our test collections\nand noisy relevance judgments due to them being determined with             [10].\nautomatic rules applied to discussion threads [18, 24], lack pooling\nto gather candidates for judging relevance [18, 24], or lack realistic      4.2    Results\nnarrative queries [21]. We leave the development of more robust\n                                                                            Table 1 presents the performance of the proposed method compared\ntest collections and evaluation methods for NDR to future work.\n                                                                            against baselines. Here, bold numbers indicate the best-performing\n4.1.2 Implementation Details. Next, we describe important details           model, and superscripts indicate statistical significance computed\nfor Mint and leave finer details of the model and training to our           with two-sided t-tests at \ud835\udc5d < 0.05.\ncode release. To sample user interactions for generating synthetic             Here, we first note the performance of baseline approaches. We\nqueries from the Yelp dataset, we exclude POIs and users with               see BM25 outperformed by Contriver, a transformer bi-encoder\nfewer than ten reviews to ensure that users were regular users of           model trained for zero-shot retrieval; this mirrors prior work [22].\nthe site with well represented interests. This follows common prior         Next, we see supervised bi-encoder models trained on similar pas-\npractice in preparing user-item interaction datasets for use [27].          sage (MPNet-1B) and question-answer (BERT-MSM) pairs outper-\nThen we retain users who deliver an average rating greater than             form a weakly supervised model (Contriver) by smaller margins.\n3/5 and with 10-30 above-average reviews. This desirably biases             Finally, the Grounded LLM outperforms all bi-encoder baselines, in-\nour data to users who commonly describe their likings (rather than          dicating strong few-shot generalization and mirroring prior results\n                                                                            [19]. Examining the Mint models, we first note that the BiEnc-\n5 https://www.yelp.com/dataset                                              Mint sees statistically significant improvement compared to BM25\n\n\n\n\n                                                                      780\n\fLarge Language Model Augmented Narrative Driven Recommendations                                       RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\nTable 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts\ndenote statistically significant improvements compared to specific baseline models.\n\n                                                                                        Pointrec\n                              Model     Parameters      NDCG@5     NDCG@10        MAP        MRR          Recall@100      Recall@200\n                             1 BM25          -          0.2682     0.2464         0.1182     0.2685       0.4194          0.5429\n                         2 Contriver       110M         0.2924     0.2776         0.1660     0.3355       0.4455          0.5552\n                        3 MPNet-1B         110M         0.3038     0.2842         0.1621     0.3566       0.4439          0.5657\n                      4 BERT-MSM           110M         0.3117     0.2886         0.1528     0.3320       0.4679          0.5816\n                  5 Grounded LLM        175B+110M       0.3558     0.3251         0.1808     0.3861       0.4797          0.5797\n                               6 UPR     110M+3B        0.3586     0.3242         0.1712     0.4013       0.4489          0.5552\n                       BiEnc-Mint          110M         0.34891    0.32631        0.18901    0.39821      0.49141         0.6221\n                       CrEnc-Mint         2\u00d7110M        0.372512   0.348912       0.219214   0.43171      0.5448123       0.6221\n\n\nand outperforms the best bi-encoder baselines by 11-13% on preci-                6B LLM for Item Queries. We find a smaller 6B LLM to result\nsion measures and 5-7% on recall measures. Specifically, we see a             in poor quality data when used to generate narrative queries con-\n                                                                                                \ud835\udc41\ud835\udc62\nmodel trained for question-answering (BERT-MSM) underperform                  ditioned on {\ud835\udc51\ud835\udc56 }\ud835\udc56=1  . Here we simplify the text generation task \u2013\nBiEnc-Mint, indicating the challenge of the NDR task. Further,                using a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56 . This\nBiEnc-Mint, trained on 5 orders of magnitude lesser data than                 experiment also mirrors the setup for generating synthetic queries\nMPNet-1B, sees improved performance \u2013 indicating the quality of               for search tasks [7, 15]. Here, we use 3-few shot examples and sam-\ndata obtained from Mint. Furthermore, BiEnc-Mint also performs                ple one item per user for generating \ud835\udc5e\ud835\udc62 . Given the lower cost of\nat par with a 175B LLM while offering the inference efficiency of a           using a smaller LLM, we use all 45,193 users in our Yelp dataset\nsmall-parameter bi-encoder. Next, we see CrEnc-Mint outperform                rather than a smaller random sample. From Table 2, we see that this\nthe baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM                   results in higher quality queries than using smaller LLMs for gen-\nby 4-21% on precision measures and 7-13% on recall measures \u2013                                                      \ud835\udc41\ud835\udc62\n                                                                              erating narrative queries from {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 . The resulting BiEnc model\ndemonstrating the value of Mint for training NDR models.                      underperforms the BiEnc-Mint, indicating the value of generating\n                                                                              complex queries conditioned on multiple items as in Mint for NDR.\n4.3    Ablations                                                              We see that CrEnc approaches the performance of CrEnc-Mint\u2013\nIn Table 2, we ablate various design choices in Mint. Different               note, however, that this approach uses the performant BiEnc-Mint\nchoices result in different training sets for the BiEnc and CrEnc             for sampling negatives and first stage ranking. We leave further\nmodels. Also, note that in reporting ablation performance for CrEnc,          exploration of using small parameter LLMs for data augmentation\nwe still use the performant BiEnc-Mint model for obtaining nega-              for NDR models to future work.\ntive examples for training and first-stage ranking. Without high-\nquality negative examples, we found CrEnc to result in much poorer            5    CONCLUSIONS\nperformance.                                                                  In this paper, we present Mint, a data augmentation method for the\n   No item filtering. Since synthetic queries are unlikely to rep-            narrative-driven recommendation (NDR) task. Mint re-purposes\n                                                                  \ud835\udc41\ud835\udc62\nresent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56 }\ud835\udc56=1             historical user-item interaction datasets for NDR by using a 175B pa-\nwhich have a low likelihood of being generated from the document              rameter large language model to author long-form narrative queries\n(\u00a73.2.2). Without this step, we expect the training set for training          while conditioning on the text of items liked by users. We evaluate\nretrieval models to be larger and noisier. In Table 2, we see that            bi-encoder and cross-encoder models trained on data from Mint on\nexcluding this step leads to a lower performance for BiEnc and                the publicly available Pointrec test collection for narrative-driven\nCrEnc, indicating that the quality of data obtained is important for          point of interest recommendation. We demonstrate that the result-\nperformance.                                                                  ing models outperform several strong baselines and ablated models\n   6B LLM for QGen. Mint relies on using an expensive 175B pa-                and match or outperform a 175B LLM directly used for NDR in a\nrameter InstructGPT model for QGen. Here, we investigate the                  1-shot setup.\n                                    \ud835\udc41\ud835\udc62\nefficacy for generating \ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 with a 6B parameter Instruct-            However, Mint also presents some limitations. Given our use of\nGPT model (text-curie-001). We use an identical setup to the                  historical interaction datasets for generating synthetic training data\n175B LLM for this. In Table 2, we see that training on the synthetic          and the prevalence of popular interests in these datasets longer,\nnarrative queries of the smaller LLM results in worse models \u2013 of-            tailed interests are unlikely to be present in the generated syn-\nten underperforming the baselines in Table 1. This indicates the              thetic datasets. In turn, causing retrieval models to likely see poorer\ninability of a smaller model to generate complex narrative queries            performance on these requests. Our use of LLMs to generate syn-\nwhile conditioning on a set of user items. This necessity of a larger         thetic queries also causes the queries to be repetitive in structure,\nLLM for generating queries in complex retrieval tasks has been                likely causing novel longer-tail queries to be poorly served. These\nobserved in prior work [15, 23].                                              limitations may be addressed in future work.\n\n\n\n\n                                                                      781\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                                               Mysore, McCallum, Zamani\n\n\n                                             Table 2: Mint ablated for different design choices on Pointrec.\n\n                                                                                                      Pointrec\n                            Ablation                                NDCG@5          NDCG@10      MAP         MRR        Recall@100        Recall@200\n                            BiEnc-Mint                              0.3489          0.3263       0.1890      0.3982     0.5263            0.6221\n                            \u2212 No item filtering                     0.2949          0.2766       0.1634      0.3505     0.4979            0.5951\n                            \u2212 6B LLM for QGen                       0.2336          0.2293       0.1125      0.2287     0.426             0.5435\n                            \u2212 6B LLM for Item Queries               0.3012          0.2875       0.1721      0.3384     0.4800            0.5909\n                            CrEnc-Mint                              0.3725          0.3489       0.2192      0.4317     0.5448            0.6221\n                            \u2212 No item filtering                     0.3570          0.3379       0.2071      0.4063     0.5366            0.6221\n                            \u2212 6B LLM for QGen                       0.2618          0.2421       0.1341      0.3118     0.4841            0.6221\n                            \u2212 6B LLM for Item Queries               0.3792          0.3451       0.2128      0.4098     0.5546            0.6221\n\n\n   Besides this, other avenues also present rich future work. While                                Society: 14th International Conference, iConference 2019, Washington, DC, USA,\nMint leverages a 175B LLM for generating synthetic queries, smaller                                March 31\u2013April 3, 2019, Proceedings 14. Springer, 503\u2013515.\n                                                                                               [5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven\nparameter LLMs may be explored for this purpose - perhaps by                                       Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender\ntraining dedicated QGen models. Mint may also be expanded to                                       Systems (Como, Italy) (RecSys \u201917). Association for Computing Machinery, New\n                                                                                                   York, NY, USA, 238\u2013242. https://doi.org/10.1145/3109859.3109893\nexplore more active strategies for sampling items and users for                                [6] Toine Bogers and Marijn Koolen. 2018. \u201cI\u2019m looking for something like. . . \u201d:\nwhom narrative queries are authored - this may allow more effi-                                    Combining Narratives and Example Items for Narrative-driven Book Recommen-\ncient use of large parameter LLMs while ensuring higher quality                                    dation. In Knowledge-aware and Conversational Recommender Systems Workshop.\n                                                                                                   CEUR Workshop Proceedings.\ntraining datasets. Next, the generation of synthetic queries from                              [7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nsets of documents may be explored for a broader range of retrieval                                 InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings\ntasks beyond NDR given its promise to generate larger training                                     of the 45th International ACM SIGIR Conference on Research and Development\n                                                                                                   in Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing\nsets \u2013 a currently underexplored direction. Finally, given the lack of                             Machinery, New York, NY, USA, 2387\u20132392. https://doi.org/10.1145/3477495.\nlarger-scale test collections for NDR and the effectiveness of LLMs                                3531863\n                                                                                               [8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,\nfor authoring narrative queries from user-item interaction, fruitful                               Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-\nfuture work may also explore the creation of larger-scale datasets                                 pervised Training of Efficient Rankers. arXiv:2301.02998\nin a mixed-initiative setup to robustly evaluate models for NDR.                               [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                                   Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                                   Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\n                                                                                                   Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nACKNOWLEDGMENTS                                                                                    Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nWe thank anonymous reviewers for their invaluable feedback. This                                   Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n                                                                                                   Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nwork was partly supported by the Center for Intelligent Informa-                                   In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-\ntion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of                                  zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nNaval Research contract number N000142212688, an Amazon Alexa                                      Inc., 1877\u20131901. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n                                                                                                   1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nPrize grant, and the Chan Zuckerberg Initiative under the project                             [10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nScientific Knowledge Base Construction. Any opinions, findings                                     Information. In Proceedings of the 27th Annual International ACM SIGIR Conference\n                                                                                                   on Research and Development in Information Retrieval (Sheffield, United Kingdom)\nand conclusions or recommendations expressed here are those of                                     (SIGIR \u201904). Association for Computing Machinery, New York, NY, USA, 25\u201332.\nthe authors and do not necessarily reflect those of the sponsors.                                  https://doi.org/10.1145/1008992.1009000\n                                                                                              [11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-\n                                                                                                   CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing\nREFERENCES                                                                                         Cold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con-\n                                                                                                   ference on Research and Development in Information Retrieval (Virtual Event,\n [1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:\n                                                                                                   China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA,\n     A Test Collection for Narrative-Driven Point of Interest Recommendation. In\n                                                                                                   1251\u20131260. https://doi.org/10.1145/3397271.3401038\n     Proceedings of the 44th International ACM SIGIR Conference on Research and\n                                                                                              [12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun\n     Development in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As-\n                                                                                                   Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data\n     sociation for Computing Machinery, New York, NY, USA, 2478\u20132484. https:\n                                                                                                   Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n     //doi.org/10.1145/3404835.3463243\n                                                                                                   USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n [2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and\n                                                                                                   1012\u20131020. https://doi.org/10.1145/3543507.3583341\n     Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in\n                                                                                              [13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study\n     Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference\n                                                                                                   for Understanding Users\u2019 Attitudes Towards a Conversational Agent for News\n     on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/\n                                                                                                   Recommendation. In Proceedings of the 4th Conference on Conversational User\n     doi/10.1145/3406522.3446021\n                                                                                                   Interfaces (Glasgow, United Kingdom) (CUI \u201922). Association for Computing\n [3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018.\n                                                                                                   Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/\n     \u201cWhat was this Movie About this Chick?\u201d A Comparative Study of Relevance\n                                                                                                   3543829.3544530\n     Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter-\n                                                                                              [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\n     national Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings\n                                                                                                   Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\n     13. Springer, 323\u2013334.\n                                                                                                   instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n [4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019.\n                                                                                              [15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\n     \u201cLooking for an amazing game I can relax and sink hours into...\u201d: A Study of\n                                                                                                   Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot\n     Relevance Aspects in Video Game Discovery. In Information in Contemporary\n\n\n\n\n                                                                                        782\n\fLarge Language Model Augmented Narrative Driven Recommendations                                                          RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\n     Dense Retrieval From 8 Examples. In The Eleventh International Conference on                  Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,\n     Learning Representations. https://openreview.net/forum?id=gmL46YMpu2J                         A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\n[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.                       Inc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/\n     Google News Personalization: Scalable Online Collaborative Filtering. In Pro-                 b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n     ceedings of the 16th International Conference on World Wide Web (Banff, Alberta,         [34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,\n     Canada) (WWW \u201907). Association for Computing Machinery, New York, NY, USA,                    and Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces\n     271\u2013280. https://doi.org/10.1145/1242572.1242610                                              That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on\n[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,                Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR\n     Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi                \u201921). Association for Computing Machinery, New York, NY, USA, 261\u2013265. https:\n     Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of                     //doi.org/10.1145/3406522.3446035\n     the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys              [35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues\n     \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https:                 Bouchard. 2023. Improving Content Retrievability in Search with Controllable\n     //doi.org/10.1145/1864708.1864770                                                             Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating                     USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n     Narrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th                  3182\u20133192. https://doi.org/10.1145/3543507.3583261\n     International Conference on Intelligent User Interfaces (Marina del Ray, California)     [36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin.\n     (IUI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https:               2022. On Natural Language User Profiles for Transparent and Scrutable Rec-\n     //doi.org/10.1145/3301275.3302287                                                             ommendation. In Proceedings of the 45th International ACM SIGIR Conference\n[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot                   on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR\n     Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).             \u201922). Association for Computing Machinery, New York, NY, USA, 2863\u20132874.\n[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context                   https://doi.org/10.1145/3477495.3531873\n     Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom-                 [37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\n     mender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing                     using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\n     Machinery, New York, NY, USA, 9\u201316. https://doi.org/10.1145/2507157.2507187                   pirical Methods in Natural Language Processing. Association for Computational\n[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M               Linguistics. https://arxiv.org/abs/1908.10084\n     Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In               [38] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\n     TREC.                                                                                         Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389.\n[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-                  https://doi.org/10.1561/1500000019\n     janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-              [39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin\n     mation Retrieval with Contrastive Learning. Transactions on Machine Learning                  Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.\n     Research (2022). https://openreview.net/forum?id=jKN1pXi7b0                                   UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation\n[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,                of Rerankers. arXiv:2303.00807 [cs.IR]\n     Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as            [40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\n     Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820                      Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\n[24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo                      with Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\n     Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016.                     Empirical Methods in Natural Language Processing. Association for Computational\n     Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul-               Linguistics, Abu Dhabi, United Arab Emirates, 3781\u20133797. https://aclanthology.\n     tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa            org/2022.emnlp-main.249\n     Gon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato,            [41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked\n     and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351\u2013370.                    and Permuted Pre-training for Language Understanding. In Advances in Neural\n[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski,                   Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_\n     Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data                  files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf\n     for Conversational Music Recommendation Using Random Walks and Language                  [42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via\n     Models. arXiv:2301.11489                                                                      Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual\n[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of-                International ACM SIGIR Conference on Research and Development in Information\n     Interest Recommendation by Mining Users\u2019 Preference Transition. In Proceedings                Retrieval (Salvador, Brazil) (SIGIR \u201905). Association for Computing Machinery,\n     of the 22nd ACM International Conference on Information & Knowledge Manage-                   New York, NY, USA, 449\u2013456. https://doi.org/10.1145/1076034.1076111\n     ment (San Francisco, California, USA) (CIKM \u201913). Association for Computing Ma-          [43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic\n     chinery, New York, NY, USA, 733\u2013738. https://doi.org/10.1145/2505515.2505639                  Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender\n[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An                           Systems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association for\n     Experimental Evaluation of Point-of-Interest Recommendation in Location-Based                 Computing Machinery, New York, NY, USA, 86\u201394. https://doi.org/10.1145/\n     Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010\u20131021. https://doi.                 3240323.3240369\n     org/10.14778/3115404.3115407                                                             [44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.\n[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and                Controllable Gradient Item Retrieval. In Web Conference.\n     Lucas Dixon. 2021. Augmenting the user-item graph with textual similarity                [45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\n     models. arXiv preprint arXiv:2109.09358 (2021).                                               and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug-\n[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue                        mentation. In Proceedings of the 25th ACM SIGKDD International Conference\n     Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In                on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). As-\n     Proceedings of the 17th Conference of the European Chapter of the Association for             sociation for Computing Machinery, New York, NY, USA, 548\u2013556. https:\n     Computational Linguistics. Association for Computational Linguistics, Dubrovnik,              //doi.org/10.1145/3292500.3330873\n     Croatia, 2799\u20132829. https://aclanthology.org/2023.eacl-main.206                          [46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized\n[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear                   Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM\n     Critiquing for Conversational Recommender Systems. In The Web Conference.                     Conference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association\n[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot             for Computing Machinery, New York, NY, USA, 502\u2013505. https://doi.org/10.\n     Neural Passage Retrieval via Domain-targeted Synthetic Question Generation.                   1145/3523227.3547394\n     In Proceedings of the 16th Conference of the European Chapter of the Associa-            [47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng.\n     tion for Computational Linguistics: Main Volume. Association for Computational                2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users\n     Linguistics, Online, 1075\u20131088. https://doi.org/10.18653/v1/2021.eacl-main.92                 in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n[32] Sheshera Mysore, Tim O\u2019Gorman, Andrew McCallum, and Hamed Zamani. 2021.                       USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n     CSFCube - A Test Collection of Computer Science Research Articles for Faceted                 1396\u20131404. https://doi.org/10.1145/3543507.3583538\n     Query by Example. In Thirty-fifth Conference on Neural Information Processing            [48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con-\n     Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv.              versational information seeking. arXiv preprint arXiv:2201.08808 (2022).\n     2103.12906                                                                               [49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela                  Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer-\n     Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John                        ence on Research and Development in Information Retrieval (Virtual Event, China)\n     Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda                     (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 881\u2013890.\n     Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.                    https://doi.org/10.1145/3397271.3401180\n     Training language models to follow instructions with human feedback. In\n\n\n\n\n                                                                                        783\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\nAbhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google News Personalization: Scalable Online Collaborative Filtering. In Proceedings of the 16th International Conference on World Wide Web (Banff, Alberta, Canada) (WWW \u201907) . Association for Computing Machinery, New York, NY, USA, 271\u2013280. https://doi.org/10.1145/1242572.1242610 ", " James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https://doi.org/10.1145/1864708.1864770 ", " Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating Narrative-Driven Movie Recommendations on Reddit. In Proceedings of"], "publish_date": "2023-09-18", "approved": true}}, {"_index": "articles", "_id": "MOtwCI0BUzn7PtN-iYvm", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "ModelGame: A Quality Model for Gamified Software Modeling Learning", "abstract": "Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the engagement of developers while creating UML models or writing code. Empirical studies report that UML models suffer from incompleteness and inconsistency problems. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and UML model quality in the context of software modeling learning. This article, therefore, proposes ModelGame , which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gamified activities and providing guidelines aware of quality issues. A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt the ModelGame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. The results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling.", "authors": ["Ed Wilson J\u00fanior", "Kleinner Farias"], "institutions": ["Universidade do Vale do Rio dos Sinos"], "keywords": ["Model design", "learning model", "Gamification"], "full_text": "  ModelGame: A Quality Model for Gamified Software Modeling\n                         Learning\n                                Ed Wilson J\u00fanior\u2217                                                                        Kleinner Farias\n                   Universidade do Vale do Rio dos Sinos                                                     Universidade do Vale do Rio dos Sinos\n                  S\u00e3o Leopoldo, Rio Grande do Sul, Brazil                                                   S\u00e3o Leopoldo, Rio Grande do Sul, Brazil\n                         edwjr7@edu.unisinos.br                                                                   kleinnerfarias@unisinos.br\nABSTRACT                                                                                            1   INTRODUCTION\nGamification has been adopted in software development tasks in                                      Gamification has been adopted in software development tasks in\nrecent years. This adoption seeks, for example, to improve the en-                                  recent years. This adoption seeks, for example, to improve the en-\ngagement of developers while creating UML models or writing                                         gagement of developers while creating UML models or writing\ncode. Empirical studies report that UML models suffer from incom-                                   code. Empirical studies [7, 9, 14] report that UML models suffer\npleteness and inconsistency problems. This study conjectures that                                   from incompleteness and inconsistency problems. Lange [14] rein-\ngamification mechanics can improve learner engagement while                                         forces that these defects bring potential risks that can cause mis-\nlearning software modeling, mitigating such problems concern-                                       interpretation and communication failure, representing a risk to\ning UML models. The current literature lacks studies that explore                                   software quality. Thus, finding formats that favor student learning\ngamification and UML model quality in the context of software                                       and consequently in generating increasingly effective UML mod-\nmodeling learning. This article, therefore, proposes ModelGame,                                     els can become one of the main challenges faced by instructors\nwhich is a quality model to support software modeling learning in a                                 that include UML (Unified Modeling Language) as part of software\ngamified way. It serves as a reference framework so that instructors                                modeling content.\ncan obtain a parameterized way to evaluate UML models created                                           Some studies [3, 12, 25] sought to understand how to apply gam-\nby learners. The quality of UML models can be improved by apply-                                    ification in software modeling teaching using some elements such\ning gamified activities and providing guidelines aware of quality                                   as points, emblems and levels. However, instructors and researchers\nissues. A qualitative questionnaire was answered by 19 instructors                                  still find limitations when applying, evaluating, and measuring the\nwho teach software modeling at higher education institutions. The                                   use of this tool in the learning of software modeling students and,\nresults show that (1) 94.7% recognize that the proposed model can                                   consequently, in the models developed by them, since in the current\nimprove the quality of UML models, indicating that they would                                       literature there is no \u201cframe of reference\u201d that guides them. This\nadopt the ModelGame in their learning practices; and (2) 47.4% do                                   study conjectures that gamification mechanics can improve learner\nnot use any gamification mechanics in their classes. The results are                                engagement while learning software modeling, mitigating such\nencouraging, showing the potential for applying and improving                                       problems concerning UML models. The current literature lacks\nthe teaching and learning of software modeling.                                                     studies that explore gamification and model quality in the context\n                                                                                                    of software modeling learning.\nCCS CONCEPTS                                                                                            This article, therefore, introduces ModelGame, which is a quality\n\u2022 Software and its engineering \u2192 Software design engineer-                                          model to support software modeling learning in a gamified way.\ning.                                                                                                It serves as a reference framework so that instructors can obtain\n                                                                                                    a parameterized way to evaluate UML models created by learners.\n                                                                                                    The quality of UML models can be improved by applying gami-\nKEYWORDS\n                                                                                                    fied activities and providing guidelines aware of quality issues. A\nModel design, learning model, Gamification                                                          reference framework would help to (1) establish parameters for eval-\nACM Reference Format:                                                                               uating UML models created by learners; (2) provide guidelines to\nEd Wilson J\u00fanior and Kleinner Farias. 2021. ModelGame: A Quality Model                              improve the quality of these artifacts; (3) to analyze which elements\nfor Gamified Software Modeling Learning. In 15th Brazilian Symposium                                of gamification could be included in each of the phases of modeling\non Software Components, Architectures, and Reuse (SBCARS \u201921), September                            using UML; (4) identify intrinsic and extrinsic aspects of students\n27-October 1, 2021, Joinville, Brazil. ACM, New York, NY, USA, 10 pages.                            during the modeling stages, to improve the models; (5) to compare\nhttps://doi.org/10.1145/3483899.3483910                                                             validated theories about the inclusion of gamification in software\n                                                                                                    modeling teaching, taking into account the types of learning and\nPermission to make digital or hard copies of all or part of this work for personal or               methodologies used; and (6) contributing to the identification of\nclassroom use is granted without fee provided that copies are not made or distributed               gamification use objectives in modeling activities.\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM                      A qualitative questionnaire was answered by 19 instructors who\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,             teach software modeling at higher education institutions. The re-\nto post on servers or to redistribute to lists, requires prior specific permission and/or a         sults show that (1) 94.7% recognize that the proposed model can\nfee. Request permissions from permissions@acm.org.\nSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil                                         improve the quality of UML models, indicating that they would\n\u00a9 2021 Association for Computing Machinery.                                                         adopt it in their learning practices; and (2) 47.4% do not use any\nACM ISBN 978-1-4503-8419-3/21/09. . . $15.00\nhttps://doi.org/10.1145/3483899.3483910\n\n\n\n\n                                                                                              100\n\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil                                                       Ed Wilson J\u00fanior and Kleinner Farias\n\n\ngamification mechanics in their classes. These results are encourag-        product. The principles of this activity establish a philosophy that\ning, showing the potential for applying and improving the teaching          guides the entire software development process.\nand learning of software modeling.                                             In this scenario, UML models play a crucial role in software\n   The remainder of the paper is organized as follows. Section 2            development tasks, for example, documenting project decisions,\npresents the main concepts discussed throughout the article. Sec-           understanding development details, promoting better communica-\ntion 3 discusses the related work, highlighting research opportu-           tion between teams, and generating greater efficiency in software\nnities. Section 4 introduces the proposed quality model. Section 5          development [19]. However, these models suffer problems of in-\npresents how the quality model was evaluated. Section 6 points out          consistency and incompleteness [10, 18], as well as end up being\nsome threats to validity. Finally, Section 7 presents some concluding       overlooked within the modeling process, as pointed out in some\nremarks and future work.                                                    empirical studies in the literature [14, 15]. Class and sequence dia-\n                                                                            grams, for example, present inconsistencies when sequence diagram\n2     BACKGROUND                                                            objects are not found in the class diagram, consequently developers\nThis section presents the essential concepts for understanding this         end up living with inconsistencies throughout the development\nwork, including gamification and software engineering teaching              process.\n(Section 2.1), and software modeling and model quality (Section                A research challenge still open is how to evaluate these diagrams,\n2.2).                                                                       both in industry and in the teaching process, in terms of quality,\n                                                                            such as syntactic and semantic, for example.\n2.1     Gamification and Software Engineering\n        Teaching                                                            3     RELATED WORK\nGamification aims to use game elements in the context of not game           The selection of related works was carried out following two steps:\n[5], bringing all positive aspects they provide as a way to encourage       (1) search in digital repositories, such as Google Scholar and Scopus\nand engage \u201cplayers,\u201d thereby broadening their motivations.                 (Elsevier) of articles related to gamification, quality modeling, and\n   Werbach [23] classifies gamification into three dimensions: Dy-          modeling learning; and (2) filter selected articles considering the\nnamics, Mechanics, and Components. Dynamicsinclude all game                 alignment of such works with the objective of the work (Section 4).\naspects related to the emotional responses of \u201cplayers\u201d (e.g., rela-        After selecting the works, they were analyzed (Section 3.1) and\ntionship, progression, and narrative).Mechanics offer elements that         compared (Section 3.2), seeking to identify research opportunities.\npromote the action of a game \u2014 usually elaborated via a rule-based\ndevelopment \u2014, so that the player can interact with such elements,          3.1    Analysis of Related Works\ne.g., challenges, feedback, and rewards. Components represent               Porto et al. (2021) [4]. This work performed a systematic map-\nthe aesthetic elements of gamification, whose goal is to present            ping with the objective of characterizing how gamification has\nvisual aspects with which players can perform the interaction, for          been adopted in noneducational contexts of software engineering\nexample, points, scores, and emblems (badges).                              activities. The main results of this study show that gamification\n   Knowing that the teaching of Software Engineering should in-             provided benefits for activities such as requirements specification,\nvolve students to experience the professional practices of the area         development, testing, project management, and support process.\nso that they can understand which practices and techniques are              In addition, he pointed out that the number of publications and\nuseful in several different situations [2]. The challenges of teaching      new research initiatives has increased over the years, many posi-\nnew software engineers are not limited to learning programming,             tive results have been achieved in software engineering activities.\nbut also include paying attention to detail, considering the quality        Nevertheless, the study reinforced that gamification can still be\nof created models, established schedule and defined budgets [1]. In         explored for other tasks in this area, as empirical evidence is very\naddition to understanding the technical challenges, these future            limited.\nprofessionals must be up to date with nontechnical issues, including           Marin (2021) [17]. It performed the application of gamification\nteamwork, communication and management.                                     on some topics of a software engineering course to engage students\n   To meet these new demands of the current context, the format             and increase their motivation and argued that, with due motiva-\nwith exhibition classes is no longer considered enough and may              tion, students can better exercise the topics and obtain more solid\neven become demotivating and ineffective in learning students. In           knowledge. There were five games related to risk management,\nthis sense, gamification has been increasingly used in the teach-           BPMN modeling, Scrum process, design and inspection of class\ning of software engineering as a way to promote behavioral and              diagrams, and cosmic functional size measurement to assist in the\npsychological changes [11] providing an environment that favors             learning process of the software engineering course. This study also\ncommunication, cooperation, feedback, reward, achievement and               presented the lessons learned about the application of gamification\nother recurring elements that are capable of improving perfor-              and serious games in software engineering, including limitations\nmance, efficiency and engagement in educational activities , and            or disadvantages.\ncan enhance, for example, the learning of software modeling.                   Jurgelaitis et al. (2018) [12]. This work conducted a research to\n                                                                            investigate how gamification could be inserted into an Information\n2.2     Software Modeling and Model Quality                                 Systems Modeling course, which covers a range of topics on UML.\nSoftware modeling encompasses the set of principles, concepts, and          As a result, an implementation of the gamified system modeling\npractices that lead to the development of a high-quality system or          course in the Moodle environment was presented, using additional\n\n\n\n\n                                                                      101\n\fModelGame: A Quality Model for Gamified Software Modeling Learning                                  SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil\n\n\nplugins for the use of the necessary gamified elements. The study                 \u2022 Applicability of Gamification in UML (CC03): Studies\nshowed good results and obtained a positive acceptance by the                       that evaluated how gamification can contribute to UML mod-\nparticipating students.                                                             els.\n   Rodrigues et al. (2018) [22]. They investigated the use of games               \u2022 Model creation (CC04): Studies that have developed a model\nand game elements in software engineering education, through a                      to improve factors that imply the non-adoption of UML.\nresearch that had the participation of 88 instructors of this disci-              \u2022 Instructor participation (CC05): Studies that collected\npline. The results showed that most instructors are aware of these                  qualitative data through the participation of software mod-\neducational approaches, however, the games were adopted by only                     eling instructors.\n21 participants and game elements were adopted only by 19. Games               Table 1 shows the comparison of the selected works, confronting\nare most often used to cover \u201cSoftware Process\u201d and \u201cProject Man-           this work. Some gaps and research opportunities are observed: (1)\nagement\u201d. The most commonly used game elements are points,                  only the proposed work was the only one to fully meet all compar-\nquizzes, and challenges. The results also show that the main rea-           ison criteria; (2) although most of them targeted the application\nsons for not adopting the resources are the lack of knowledge,              of gamification in software modeling teaching, they were not di-\ninformation about games relevant to the engineering of teaching             rected to the use of UML; (3) no study has developed a model to\nsoftware, and the lack of time to plan and include these approaches         evaluate the learning and improvement of UML models developed\nin the classroom.                                                           by students; and (4) most of them did not have the participation\n   Cosentino et al. (2017) [3]. They present a model-based ap-              of instructors to identify the difficulties and opportunities in the\nproach to learning modeling in a gamified way. The approach in-             application of gamification in the teaching of software modeling.\ncludes a new language to model the gamification process itself and          Thus, the next Section presents a quality model to explore these\nan environment where it can be incorporated into current mod-               identified opportunities.\neling tools to allow instructors and students to design and use a\ncomplete modeling framework, including gamification elements. In                                                         Comparison Criterion\n                                                                                        Related Work\naddition, the approach also had as a proposal to provide support to                                               CC1     CC2 CC3 CC4 CC5\ncollect and analyze gamification data, thus facilitating monitoring                Proposed Work\n                                                                                   Porto et al (2021) [4]           #        #             #       #\nactivities.                                                                        Marin (2021) [17]                         #      G\n                                                                                                                                    #      #\n                                                                                                                                           G       #\n   Yohannis (2016) [25]. This research presents an exploration of                  Jurgelaitis et al (2018) [12]             #                     #\ngame design as an approach to strengthening the student\u2019s mas-                     Rodrigues et al (2018) [22]                      G\n                                                                                                                                    #      #\n                                                                                   Cosentino et al (2017) [3]                #                      #\ntery in software modeling by developing their abstraction skills. It               Yohannis (2016) [25]                      #      #\n                                                                                                                                    G      #\n                                                                                                                                           G        #\nbrought together concepts of gamification development, such as the                 Pedreira et al (2015) [21]       #        #      G\n                                                                                                                                    #      #        #\nlens of atoms of intrinsic skill and principles of pedagogical design                  Completely Meets G       # Partially Meets   # Does not attend\nof various theories and models of learning. The research follows            Table 1: Comparative analysis of the selected related works\nthe Design Science Research Methodology and explores the best\npractices of Model Oriented Engineering. As a result, a modeling\ngame design framework and generation structure and a series of\nproduced games are presented.\n                                                                            4     PROPOSED QUALITY MODEL\n   Pedreira et al. (2015) [21]. They developed a systematic map-\nping of gamification in Software Engineering based on 29 studies.           This section presents the proposed quality model to support soft-\nThe mapping revealed that software implementation is the area               ware modeling learning in a gamified way. It serves as a frame\nin which most studies focus, followed by software requirements,             of reference so that instructors can evaluate the UML models cre-\nfew others in different areas, such as project planning and software        ated by students through gamified activities. Section 4.1 presents\ntesting, and even to a lesser extent in activities involving software       a proposal of a generic analytical framework. Section 4.2 details\nmodeling. However, the highlight of this work was to highlight that         the abstract syntax of the proposed quality model. Section 4.3 ex-\ngamification in software engineering is still at a very early stage         plains the quality notions related to the gamified software modeling\nand the evidence on its impact in this field remains inconclusive.          learning.\n\n                                                                            4.1     Generic Analytical Framework\n3.2     Comparative Analysis and Opportunities\n                                                                            Figure 1 presents the generic analytical framework for improving\nFive Comparison Criteria (CC) were defined selecting the most rele-\n                                                                            the quality of the models and serves as the basis for the creation of\nvant variables to assist in the process of identifying similarities and\n                                                                            an evaluation scheme. The arrows (\"links\"), labeled as Evaluation\ndifferences between the proposed work and the selected articles.\n                                                                            and Gamified Modeling, represent the questions that the evidence\nThis comparison is crucial to make the process of identifying re-\n                                                                            must answer; dotted lines represent associations; rectangles rep-\nsearch opportunities using objective rather than subjective criteria.\n                                                                            resent the Models (rounded corners) or the quality states (square\nThe criteria are described below:\n                                                                            corners) by which these bindings are measured. Ellipses represent\n      \u2022 Context (CC01): Works that explore the use of gamification          the adverse effects that can be generated from the evaluation and\n        in software modeling teaching/learning.                             use of gamification.\n      \u2022 Participant profile (CC02): Studies that collected data from           The numbers refer to the key questions and are connected with\n        participants for screening and profile characterization.            the concepts and relationships of the abstract syntax of the Quality\n\n\n\n\n                                                                      102\n\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil                                                     Ed Wilson J\u00fanior and Kleinner Farias\n\n\n\n\n                              Figure 1: Generic analytical framework for gamified software modeling learning.\n\n\nModel (presented in Section 4.2), as follows: (1) Are there tools          notions of quality that are discussed in Section 4.3. The following\nthat assist instructors in evaluating the models developed by stu-         are detailed each of these concepts and relationships.\ndents, thus reducing the poor quality and incompleteness of these             Domain. The first concept presented in this study is the domain,\nartifacts? (2) What is the prevalence of characteristics that cause        which corresponds to a specific context of the application to be\nmodels to be at risk? (3) Are there notions of quality to evaluate         developed to solve the problem. In this process, the design template\nthe models as a way to define parameters when performing their             represents the solution given to the domain.\ncorrection? (4) Applying the use of gamification in models that               Association\nneed intervention would be a way to identify factors that could                \u2022 contextualizes: Challenges[*]\ngenerate models with high quality levels? (5) Does the application\nof gamification improve the quality of the model? (5.a) How are the           Each contextualise refers to the domain that will serve as the\nmodels without gamification evaluated in relation to those with            basis for the challenges launched.\ngamification? (5.b) Are there reasons to expect that gamification             Challenges. This concept represents the phase in which the\nmodels can have better quality results than those that are gener-          problem is contextualized (domain-based), as well as what will be\nated without gamification? (6) Is the output model really effective        the missions, phases, scenarios, and other elements presented to\nwhen associated with reducing the poor quality of the model? (7)           the players, in this case the students, who must use the principles\nDoes the absence of evaluation result in adverse effects? (7.a) is the     of software engineering to perform the modeling and reach the\nevaluation acceptable for the model? (7.b) What are the potential          final goal.\nharms, and how often do they occur? (8) Does gamification result              Association\nin adverse effects on models?                                                  \u2022 influences: Design Model[*]\n   Fact is that it is not enough just to include this \"toolbox\" in the\nUML learning process, it is necessary to provide the instructor with          Each influence represents that the proposed challenge interfered\na model (guide) that can serve as a reference to evaluate the quality      in aspects of the design model, causing the user to seek to make a\nof diagrams elaborated through gamified activities. For example,           continuous improvement.\nthe instructor could create models predefining inconsistencies by             Modeling Language. Software modeling is an important step\nmaking use of these questions raised to evaluate the models created        for development to happen in a way that adheres to the require-\nby the students. The set of questions serves as the starting point for     ments established by the requester, for this, there is the modeling\nthis evaluation. Knowing that the adaptation of the gamification           language, which offers a standardized way to document and design\napproach requires a significant effort [20], in this study we present      software. Through the use of modeling languages, it is possible to\nThe ModelGame as a way to identify factors that contribute to              achieve a high level of understanding about the software in ques-\nthe quality of these artifacts and, consequently, to the students\u2019         tion, improving the communication between all those involved in\nlearning.                                                                  the process, thus avoiding implementation errors. It points out that\n                                                                           software engineers use these languages to communicate design\n                                                                           decisions and verify the feasibility of implementing the intended\n                                                                           design. The UML was consolidated as the Modeling Language in\n4.2     Abstract Syntax                                                    the paradigm of object orientation, in which it is possible through\nFollowing the specification pattern of the UML metamodel, Figure 2         visual notation generated from the diagrams- presented later in this\npresents the abstract syntax of the proposed Quality Model for             study as Design Models- to perform the representation of various\ngamified software modeling learning (ModelGame). It identifies             perspectives of the system.\nthe main concepts and relationships. The numbers represent the                Association\n\n\n\n\n                                                                     103\n\fModelGame: A Quality Model for Gamified Software Modeling Learning                              SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil\n\n\n\n\n                                                     Figure 2: Abstract Quality Model Syntax.\n\n\n     \u2022 expresses: Design Model[*]                                             Association\nPerforms the representation of the intended design templates, in                \u2022 Without a directed relationship.\nwhich the Modeling Language should be applicable to the domain\n                                                                               Modeling Tool. This concept represents the applications that\ntype.\n                                                                            are used to carry out the construction of design models. There are\n    User. This concept corresponds to the individual who performs\n                                                                            several tools available, online and desktop, and it is up to the user\nthe interpretation of the developed design models, whose objective\n                                                                            to choose the one that will best meet their needs and adapt to the\nis to be able to understand the domain in question. In the gamified\n                                                                            context in question, that is, they work in any domain that is being\ncontext, the user has the role of player and it is he who performs\n                                                                            considered.\nthe whole process, being able to perform the interpretation of ex-\n                                                                               Design Model. The design model refers to a visual notation\nisting models or even creating new ones. The user can also identify\n                                                                            (diagram) to represent static and dynamic aspects. These models are\nand resolve inconsistencies that arise from compositions between\n                                                                            built according to a specific objective or task and tend to facilitate\nmodels.\n                                                                            the logical interpretation of the software in several aspects. The\n    Association\n                                                                            most popular diagrams are Use Cases and Classes, the first being\n     \u2022 creates: Design Model[1..*]                                          static and representing a set of actions generated from functional\nRepresents the process in which the user creates a design template,         requirements (use cases) and presenting the interactions generated\nwhich can be one or more.                                                   with external users (actors). The second is a static diagram and\n     \u2022 interprets: Design Model[1..*]                                       makes the representation of the logical structure of the software\nIn this association, the user performs the interpretation of the design     involving the classes, their attributes, methods, and relationships\ntemplate. When interpreting the model, paths for the resolution of          between them [19].\ninconsistencies can be identified.                                             Association\n     \u2022 detects: Inconsistency [*]                                               \u2022 describes: Domain[1]\nRepresents the user\u2019s discovery of design model inconsistencies,               Each describes makes the representation of a specific domain\nfor example, those that are generated from identifying conflicts,           and means that every design model must describe it.\nwhether a class is abstract or not.                                            Inconsistency. It corresponds to the defects found in the models\n     \u2022 resolves: Inconsistency [*]                                          developed by users. They may occur because of the nonidentifi-\nEach resolves equates to the resolution representation of the incon-        cation and correction of possible conflicts and even an erroneous\nsistencies by the user that happens after he analyzes and determines        interpretation.\nthe best alternative to perform this action.                                   Association\n     \u2022 uses: Modeling Tools [*]                                                 \u2022 affects: Design Model[*]\nDetermines that the user can use modeling tools to generate/update          This association indicates that with each occurrence of the affect, a\ndesign models.                                                              problem is presented harming the quality of the design model.\n\n\n\n\n                                                                      104\n\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil                                                            Ed Wilson J\u00fanior and Kleinner Farias\n\n\n   Points. This concept represents one of the most used game me-                Quality of Use (6). To produce design templates, users can use\nchanics in software engineering and functions as a quantitative             unusual tools such as paper, whiteboard, and more. However, most\nreward for each action developed, in which it is possible to regulate       of the time they choose to use formal tools (CASES) and can be\nthe number of rewarded points of the player, defined here as user,          online or desktop. This notion corresponds to the level of ease and\nbased on the importance of each action. Through this concept, it            applicability of the models elaborated when making use of these\nis possible to stimulate competition, collaboration, and creativity         tools, it is also important to contribute to communication between\namong users, stimulating learning. Points appear as a derivation            users through collaboration-related functionalities.\nof the association affects, since when each inconsistency error is              Detection Quality (7). This notion is referenced to the process\nidentified or not, the user will receive a score and the association        of locating inconsistencies, since when users arise, they should\ndescribes, because the points will also be applied when making              perform traceability of them quickly. If the detection is complicated,\nconnections between the model and the domain.                               it could hinder the process of correcting the models.\n   Progress. The concept of progress emerges as a factor that makes             Resolution Quality (8). It corresponds to the level of quality\nthe user able to perceive its evolution in the process, in this case,       related to the effort that users take to look for alternatives to solve\nsoftware modeling. Progress emerges as a derivation of the associa-         the identified problem.\ntion interprets, making the user know when they have performed                  Motivational Quality (9). This notion refers to the motiva-\na correct interpretation of the proposed design model or what still         tional factors involved during the learning and development of\nneeds to be improved.                                                       design models, which can be intrinsic and extrinsic. Elements of\n   Feedback. Feedback has the role of making the user realize that          gamification such as points, feedback and progress bring the user a\nthe proposed goal can be achieved and follow its evolution, includ-         degree of satisfaction in continuing their discovery and transfor-\ning analyzing how to change or creating new strategies to achieve           mations throughout the process.\nthe goal. This concept emerges as a derivation between the associa-             Quality of Engagement (10). The user in tracking their progress\ntions it creates, causing the user to receive a return to the model         can feel committed to the objective in question, and this notion\ncreation process.                                                           represents the measurement of the level of commitment of them\n                                                                            during the development of design models.\n\n4.3     Quality Notions                                                     5     EVALUATION\nAs discussed in Section 2, gamification can bring important ele-\n                                                                            This section describes the methodology followed to evaluate the\nments for learning software modeling and, therefore, the objective\n                                                                            proposed quality model. This methodology follows well-established\nof this section is to produce the notions of quality of the model\n                                                                            empirical guidelines [24]. Section 5.1 details the objective and re-\nof this study. The ModelGame is composed of ten counts, four of\n                                                                            search questions (RQ). Section 5.2 presents the questionnaire formu-\nwhich are proposed in this study - scope, use, motivational and en-\n                                                                            lated to evaluate the proposed quality model. Section 5.3 explains\ngagement - extracted from the main benefits that the gamification\n                                                                            the context and selection of participants. Section 5.4 describes the\nelements presented in Figure 2 can bring to the models. The others\n                                                                            presentation of the Model. Section 5.5 presents the analysis of the\nare adaptations of previous works [6, 14, 15], they are, syntactic,\n                                                                            collected data.\nsemantic, social, effort, detection and resolution.\n   Scope Quality (1). It seeks to determine how much the proposed\nchallenge is contextualized with the design model, as well as the def-      5.1     Objective and Research Questions\ninition of the domain, problem, competencies, concepts, behaviors           The objective (O) of this study is twofold: (O1) Introduce Model-\nand attitudes that will be developed throughout the process.                Game as a tool for teaching Software Modeling; and (O2) Analyze\n   Syntactic Quality (2). This notion makes the representation of           the applicability of the quality model regarding the improvement\nthe process of correction of the design models that are produced            of UML models.\nby the modeling language, because if it is not used correctly, incon-          To analyze the different facets of the objectives, two Research\nsistencies will arise. It is important to insert this notion of quality     Questions (RQ) have been formulated:\ninto our study, since during the process of developing the models,              \u2022 RQ1: How do instructors evaluate the use of gamification\nusers may come across the composition of two class diagrams, for                   in software modeling?\nexample.                                                                        \u2022 RQ2: What is the acceptance of ModelGame by software\n   Semantic Quality (3). It is necessary to verify that the design                 modeling instructors?\nmodel and the problem domain match, so this notion performs\nthis type of analysis. Communication problems may occur between             5.2     Questionnaire\nusers if the semantic elements of the model are affected.                   Data was collected through an online questionnaire created through\n   Social Quality (4). Design models are used to communicate                Google Forms1 following well-established guidelines described in\nbetween members of a team to inform all established decisions               [24]. This strategy was chosen because the questionnaire could\nabout software development [8]. If divergent interpretations occur,         be applied quickly and easily collect data from individuals in geo-\nthis communication will be greatly impaired.                                graphically diverse locations. The questions of the questionnaire\n   Quality of Effort (5). This notion refers to the production chal-\nlenges of the model that will be generated, including factors such\nas time and cost.                                                           1 Questionnaire: https://forms.gle/qjaFDpErEtGdLuWw6\n\n\n\n\n                                                                      105\n\fModelGame: A Quality Model for Gamified Software Modeling Learning                              SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil\n\n\nwere concerned with examining the research gaps of previous stud-          The second activity Apply TAM questionnaire (input). Participants\nies and apprehending the structures of the previously developed            received a list of questions about the perception of ease of use, per-\nquestionnaire.                                                             ceived utility, attitudes, and intention of behavior, in relation to the\n   Part 1: Participant profile. The first part of the questionnaire        ModelGame. Qualitative data (output) were generated, regarding\nconsisted of collecting data that are related to the characteristics       the usability and acceptance of the Model under the perspective\nand opinions of the participants. The creation of the participant          of professionals who teach software modeling. This questionnaire\nprofile through this data is important to make the selection of            followed the guidelines of the TAM [16].\npossible users of ModelGame. Without this profile, participants                Phase 3: Analysis and result report. It has two activities. The\nwith an inadequate profile may generate inconsistent assessments.          first, Analyze data sought to perform a thorough analysis of the data\nParticipants were asked to provide more general information, such          collected through the questionnaire and the researcher\u2019s perception\nas age, education level, academic background. Information about            regarding the participants\u2019 doubts during the presentation stage.\nthe time of experience in teaching was also considered, including          For this, the collected data were analyzed separately, as well as con-\nteaching software modeling and level of knowledge about UML                fronted, aiming to perform a triangulation of them. Subsequently,\nmodels.                                                                    there was an Evaluation data, as a way to understand in a more\n   Part 2: TAM questionnaire. The second part addressed ques-              depth the context, the perceptions of the participants in relation to\ntions about the usability and acceptance of the technique, aiming          the proposed model as well as its applicability.\nto explore q3. To this end, this part of our questionnaire is based on\nthe technology acceptance model (TAM) [16]. This part contained\nnine questions, which were answered through the Likert Scale, in-\ncluding Totally Agree, Partially Agree, Neutral, Partially Disagree,\nand Totally Disagree. The questions formulated (Q) dealt with sev-\neral topics, including perceived ease of use (Q1-3), perceived utility\n(Q4-7), attitude towards use (Q8), and behavioral intention to use\n(Q9).\n\n5.3    Selection of participants\nThe participants were selected based on the following criteria: in-\nstructors and/or professionals working in the teaching of software\nmodeling in higher education institutions in Brazil. Using this cri-\nterion, we sought to select participants with academic training\nand practical experience in teaching. This finite set of all possible\nparticipants represents the target population [13]. This popula-\ntion represents those people who are in a position to answer the\nquestions formulated and to whom the results of the survey apply\n[13]. In all, 19 people (n) answered the questionnaire. The partici-\npants were invited via e-mail to participate in the study and each\nof them previously received the explanation/training about the\nmodel proposed through the researcher and there was no doubt,\nthey could leave for the next step that consisted of completing the\nTAM questionnaire. We discussed the experimental process in the                          Figure 3: The experimental process.\nnext section.\n\n5.4    Experimental Process                                                5.5    Result Analysis\nFigure 3 presents the experimental process used in this study, which       5.5.1 Profile data of the participants. Table 3 describes the profile\nis composed of three phases discussed below:                               data, reporting the characteristics and opinions of the participants.\n   Phase 1: Presentation. It has an activity, presentation, in which       These data were collected from May 18 to June 5, 2021. In total,\nthe researcher explained to the participants through a video detail        we had 19 participants. Our participants are between 20 and 49\nabout the quality model. This process took place individually and          years old, most of them have a degree in Computer Science (52.6%),\nin a standard way, where space was also made available for par-            Information Systems (26.3%) or Systems Analysis (21.1%) and are\nticipants to answer possible doubts about the proposed study and           specialists (36.8%), masters (36.8%) and doctors (15.8%). About the\nmodel, lasting an average of 20 minutes.                                   working time in teaching, the majority (42.1%) they have been teach-\n   Phase 2: Application of the TAM questionnaire. It has two               ing for more than 8 years and teach disciplines related to software\nactivities, the first being Collect demographic data. The participants     modeling, including software engineering, systems analysis and\nanswered a list of questions (input) so that we could collect their        software projects. A total of 47.4% have a full level of knowledge\ncharacteristics and opinions about the ModelGame. The demo-                about UML and almost half of them (47.4%) has not yet used gamifi-\ngraphic data collected (output) became the result of this activity.        cation in the teaching of software modeling. Therefore, we consider\n\n\n\n\n                                                                     106\n\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil                                                                                             Ed Wilson J\u00fanior and Kleinner Farias\n\n\n                                                                                                            Totally agree   Partially agree   Neutral   Partially disagree   Totally disagree\n  Perceived ease of use\n  I found the quality model easy to use                                                                          8                 9             2              0                   0\n  I found the quality model easy to learn                                                                        10                9             0              0                   0\n  I found the quality model easy to master                                                                       6                12             0              1                   0\n  Perceived usefulness\n  The model would make it easier to understand which elements of gamification can be used in modeling .          12               5              2              0                   0\n  Using the quality model would help increase productivity.                                                      9                8              2              0                   0\n  The model would provide an understanding of how to mitigate the incompleteness of UML diagrams.                5                8              5              1                   0\n  The model would help compare theories about gamification in software modeling teaching.                        13               4              2\n  Attitude towards use\n  Using the Quality Model for Gamified Software Modeling Learning is a good idea.                                13               5              1              0                   0\n  Behavioral intention to use\n  I would use the quality model in software modeling classes.                                                    10               7              2              0                   0\n\n                                                         Table 2: Collected data related to TAM questionnaire.\n\n\n\nthat although small, our sample is adequate to carry out an initial                                       elements (scores, challenge, emblem, among others) in their classes,\nevaluation of the proposed approach.                                                                      most (52.6%) totally agree and (42.1%) partially agree that the use\n                                                                                                          of these can contribute to the quality of the models developed by\n      Characteristic and Opinion (n=19)                   Answer            #        %\n                                                                                                          the students.\n      Age                                                 < 20 years         0       0.0%\n                                                                                                             We consider the percentage of instructors who have not yet\n                                                         20-29 years         4      21.1%                 used gamification in their classes to be high and this may be tied\n                                                         30-39 years         8      42.1%\n                                                         40-49 years         5      26.3%                 to factors such as lack of knowledge, information about the tool,\n                                                          > 49 years         2      10.5%                 and even time to plan and include these approaches [22]. Although\n      Education                                        Undergraduate*        0       0.0%                 they were based on software modeling teaching context, previous\n                                                       Specialization*       7      36.8%\n                                                          Master*            7      36.8%                 studies [3, 4, 12, 17, 25] they did not count on the participation of\n                                                           PhD*              3      15.8%                 instructors and we understand that this participation is fundamental\n                                                          Others             2      10.6%\n                                                                                                          to understand the perceptions of these professionals since they will\n      Undergraduate course                          Information Systems     5       26.3%\n                                                     Computer Science       10      52.6%                 be at the forefront of the use of gamification.\n                                                   Computer Engineering     0        0.0%                    The ModelGame proposed in this study could help them insert\n                                                      System Analysis       4       21,1%\n                                                           Others           0        0.0%                 gamification into their classes, according to the software modeling\n      Time of experience in teaching                      < 2 years          4      21.1%                 learning design [25], based on the assumption that for this, it is\n                                                          2-4 years\n                                                          5-6 years\n                                                                             2\n                                                                             3\n                                                                                    10.5%\n                                                                                    15.8%\n                                                                                                          necessary to develop a better understanding of the tasks, activities,\n                                                          7-8 years          2      10.5%                 skills and operations that the different elements of gamification can\n                                                          > 8 years          8      42.1%\n                                                                                                          offer and how they can correspond to the desired learning outcomes\n      Experience in teaching                              < 2 years          3      15.8%\n      software modeling                                   2-4 years          5      26.3%\n                                                                                                          by developing a more concrete and motivating presentation that\n                                                          5-6 years          3      15.8%                 can involve students and facilitate deep learning with UML.\n                                                          7-8 years          2      10.5%\n                                                          > 8 years          6      31,6%\n      Level of knowledge                                  Beginner           2      10.5%\n      about UML models                                     Junior            5      26.3%\n                                                            Full             9      47.4%\n                                                           Senior            3      15.8%\n      Used gamification in teaching                         Yes              9      47.4%\n                                                            No               9      47,4%\n                                                           Maybe             1       5.3%\n      Gamification can contribute to the quality         Totally agree      10      52.6%\n      of the models of UML diagrams generated           Partially agree      8      42.1%\n      by students                                          Neutral           1      5.3%\n                                                      Partially disagree     0      0.0%\n                                                       Totally disagree     0       0.0%\n\n            Table 3: The profile data of the participants.\n                                                                                                          5.5.3 RQ2: What is the acceptance of the ModelGame by software\n                                                                                                          modeling instructors? Using the TAM questionnaire, we tried to\n                                                                                                          evaluate the ease of use, perceived usefulness, attitude, and behav-\n5.5.2 RQ1: How do instructors evaluate the use of gamification in                                         ioral intention to use the Quality Model. Table 2 shows the data\nsoftware modeling? Table 3 presents the collected data related to                                         obtained. Our data obtained show that no one disagreed that the\nthe RQ formulated. First, we begin the analysis by verifying how                                          ModelGame is easy to use, learn, and master. On the contrary, al-\ninstructors visualize gamification in software modeling teaching.                                         most 90% of participants find the model easy to use (42.1% totally\nAlthough most of them (47.4%) have not yet used gamification                                              agree and 47.4% partially agrees and 10.5% neutral), learn (52.6%\n\n\n\n\n                                                                                              107\n\fModelGame: A Quality Model for Gamified Software Modeling Learning                                   SBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil\n\n\nfully agree and 47.4% partially agree) and master (31.6% fully agree,       7    CONCLUSIONS AND FUTURE WORK\n63.2% partially agree and 5.3% partially disagree).                         This study proposed an initial quality model (ModelGame) that\n   The results are also favorable considering the perception of util-       serves as a reference framework for instructors for qualitative eval-\nity. Most participants realized that the ModelGame would make it            uations of UML models developed from gamified activities, the\neasier to understand which elements of gamification can be used in          application of an empirical study with 19 participants was carried\neach of the phases of modeling using UML(63.3% totally agree, 26.3%         out to understand their vision in relation to gamification and the\npartially agree and 10.5% neutral), increase productivity (47.4% fully      acceptance of the proposed Model. It was identified that most have\nagree, 42.1% partially agree and 10.5% neutral), and the use of the         not yet used gamification in their classes, but agree that their use\nquality model would provide an understanding of how to mitigate             can contribute to the quality of the models developed by the stu-\nthe incompleteness of UML diagrams (26.3% agree totalmen 42.1%              dents and were open to using the model. Our findings can enhance\npartially agree, 26.3% neutral and 5.3% partially disagree). Still in       the adoption of new teaching practices through gamification, result-\nthe useful aspect, we tried to know if the quality model would help         ing in the improvement of software modeling learning using UML,\nto compare validated theories about the inclusion of gamification           and consequently the creation of models developed by students.\nin software modeling teaching (68.4% totally agree, 21.1% partially         These approaches can stimulate students\u2019 immersion in the design\nagree and 10.5% neutral).                                                   of systems as future professionals during learning.\n   Considering the attitude towards use, participants believe that             Finally, we hope to carry out in the future a series of experimental\nusing the ModelGame is a good idea (68.4% totally agree, 26.3%              studies to analyze each stage of application of the ModelGame\npartially agree and 5.3% neutral), just as they are confident and           and that this work represents a first step to better support the\nwould use the Model in software modeling classes (52.6% totally             application of empirical studies on models of evaluation of the\nagree, 36.8% partially agree and 10.5% neutral). These findings show        use of gamification in software modeling. We also hope that the\nthe potential for acceptance by people with profiles similar to those       questions described throughout the article will encourage other\nof participants. The results are encouraging and show the potential         researchers to extend our study to different modeling languages\nto use the proposed approach in the educational scenario.                   and teaching methodologies.\n\n\n                                                                            REFERENCES\n                                                                             [1] Rick Adcock, Edward Alef, Bruce Amato, Mark Ardis, Larry Bernstein, Barry\n                                                                                 Boehm, Pierre Bourque, John Brackett, Murray Cantor, Lillian Cassel, et al. 2009.\n                                                                                 Curriculum guidelines for graduate degree programs in software engineering. ACM.\n                                                                             [2] Mark Ardis, David Budgen, Gregory W Hislop, Jeff Offutt, Mark Sebern, and\n                                                                                 Willem Visser. 2015. SE 2014: Curriculum guidelines for undergraduate degree\n                                                                                 programs in software engineering. Computer 48, 11 (2015), 106\u2013109.\n                                                                             [3] Valerio Cosentino, S\u00e9bastien G\u00e9rard, and Jordi Cabot Sagrera. 2017. A model-\n                                                                                 based approach to gamify the learning of modeling. CEUR Workshop Proceed-\n6    THREATS TO VALIDITY                                                         ings.\nThis section discusses the possible threats to the validity of the           [4] Daniel de Paula Porto, Gabriela Martins de Jesus, Fabiano Cutigi Ferrari, and\n                                                                                 Sandra Camargo Pinto Ferraz Fabbri. 2021. Initiatives and challenges of using\nstudy.                                                                           gamification in software engineering: A Systematic Mapping. Journal of Systems\n   Internal validity. The main point affecting the internal validity             and Software 173 (2021), 110870.\n                                                                             [5] Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton O\u2019Hara, and Dan\nof our study concerns the total time used for the exploratory phase.             Dixon. 2011. Gamification. using game-design elements in non-gaming contexts.\nTo mitigate this threat, we performed the video recording of a pilot             In CHI\u201911 extended abstracts on human factors in computing systems. 2425\u20132428.\nexplaining the operating details and objectives of the ModelGame.            [6] Ana Fern\u00e1ndez-Saez et al. 2012. A systematic literature review on the quality of\n                                                                                 UML models. J. Data. Manage 22, 3 (2012), 46\u201370.\nIn relation to the methods used, the threats related to internal             [7] Kleinner Farias et al. 2012. Evaluating the impact of aspects on inconsistency\nvalidity relate to how we extract the perceptions of the discussions             detection effort: a controlled experiment. In International Conference on Model\nand whether they represent the perceptions of teachers about the                 Driven Engineering Languages and Systems. Springer, 219\u2013234.\n                                                                             [8] Kleinner Frias et al. 2014. Towards a quality model for model composition effort.\nuse of the Model. We try to reduce this threat by applying the TAM               In 29th Annual ACM Symposium on Applied Computing. 1181\u20131183.\nquestionnaire.                                                               [9] Kleinner Farias et al. 2015. Evaluating the effort of composing design models: a\n                                                                                 controlled experiment. Software & Systems Modeling 14, 4 (2015), 1349\u20131365.\n   External validity. We identified threats related to external va-         [10] Kleinner Farias et al. 2019. UML2Merge: a UML extension for model merging.\nlidity, such as the number of participants who never applied the use             IET Software 13, 6 (2019), 575\u2013586.\nof gamification. This study was limited to 19 participants (teachers)       [11] Juho Hamari, Jonna Koivisto, and Harri Sarsa. 2014. Does gamification work?\u2013\n                                                                                 a literature review of empirical studies on gamification. In 2014 47th Hawaii\nfrom various educational institutions, of which 9 (47.4%) never used             international conference on system sciences. Ieee, 3025\u20133034.\nany element of gamification in their classes, this factor can interfere     [12] Mantas Jurgelaitis, Vaidotas Drungilas, and Lina \u010ceponiene\u0307. 2018. Gamified\nin the data, since the model intends to evaluate the quality of UML              Moodle course for teaching UML. Baltic journal of modern computing 6, 2 (2018),\n                                                                                 119\u2013127.\ndiagrams from gamified activities.                                          [13] Barbara A Kitchenham and Shari L Pfleeger. 2008. Personal opinion surveys. In\n   Conclusion validity. Threats related to the validity of the con-              Guide to advanced empirical software engineering. Springer, 63\u201392.\n                                                                            [14] Christian Franz Josef Lange. 2007. Assessing and Improving the Quality of\nclusion are related to treatment and outcome. We try to make the                 Modeling: A series of Empirical Studies about the UML. (2007).\nreduction by combining quantitative and qualitative data through            [15] Odd Ivar Lindland, Guttorm Sindre, and Arne Solvberg. 1994. Understanding\ndifferent resources. These data were obtained through audio and                  quality in conceptual modeling. IEEE software 11, 2 (1994), 42\u201349.\n                                                                            [16] Nikola Maranguni\u0107 and Andrina Grani\u0107. 2015. Technology acceptance model: a\nquestionnaires. We analyze this data to answer the research ques-                literature review from 1986 to 2013. Universal access in the information society\ntions.                                                                           14, 1 (2015), 81\u201395.\n\n\n\n\n                                                                      108\n\fSBCARS \u201921, September 27-October 1, 2021, Joinville, Brazil                                                                                 Ed Wilson J\u00fanior and Kleinner Farias\n\n\n[17] Beatriz Mar\u00edn. 2021. Lessons Learned About Gamification in Software Engineer-                technology 57 (2015), 157\u2013168.\n     ing Education. In Latin American Women and Research Contributions to the IT             [22] Pedro Rodrigues, Mauricio Souza, and Eduardo Figueiredo. 2018. Games and\n     Field. IGI Global, 174\u2013197.                                                                  gamification in software engineering education: A survey with educators. In 2018\n[18] Kleinner Oliveira, Alessandro Garcia, and Jon Whittle. 2008. On the quantitative             IEEE Frontiers in Education Conference (FIE). IEEE, 1\u20139.\n     assessment of class model compositions: An exploratory study. 1th ESMDE at              [23] Kevin Werbach and Dan Hunter. 2012. For the win: How game thinking can\n     MODELS (2008).                                                                               revolutionize your business. Wharton digital press.\n[19] OMG.         2017.                   UML:       Infrastructure       specification.     [24] Claes Wohlin, Per Runeson, Martin H\u00f6st, Magnus C Ohlsson, Bj\u00f6rn Regnell, and\n     https://www.omg.org/spec/UML/2.5.1/PDF.                                                      Anders Wessl\u00e9n. 2012. Experimentation in software engineering. Springer Science\n[20] Sofia Ouhbi and Nuno Pombo. 2020. Software Engineering Education: Challenges                 & Business Media.\n     and Perspectives. In IEEE Global Engineering Education Conference. 202\u2013209.             [25] Alfa Yohannis. 2016. Gamification of Software Modelling Learning.. In DS@\n[21] Oscar Pedreira, F\u00e9lix Garc\u00eda, Nieves Brisaboa, and Mario Piattini. 2015. Gamifica-           MoDELS.\n     tion in software engineering\u2013A systematic mapping. Information and software\n\n\n\n\n                                                                                       109\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nEd Wilson J\u00fanior and Kleinner Farias ", " Beatriz Mar\u00edn. 2021. Lessons Learned About Gamification in Software Engineer-ing Education. In Latin American Women and Research Contributions to the IT Field . IGI Global, 174\u2013197. ", " Kleinner Oliveira, Alessandro Garcia, and Jon Whittle. 2008. On the quantitative assessment of class model compositions: An exploratory study. 1th ESMDE at MODELS (2008)."], "publish_date": "2021-09-27", "approved": true}}, {"_index": "articles", "_id": "KuttCI0BUzn7PtN-C4tJ", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Generating Diverse Code Explanations using the GPT-3 Large Language Model", "abstract": "Good explanations are essential to efficiently learning introductory programming concepts. Large language models (LLMs) are also changing how students interact with code. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet.", "authors": ["Stephen MacNeil", "Andrew Tran", "Dan Mogil", "Seth Bernstein", "Erin Ross", "Ziheng Huang"], "institutions": ["Temple University", "University of California\u2014San Diego"], "keywords": ["large language models", "natural language processing", "code explanations", "computer science education"], "full_text": "                                    Generating Diverse Code Explanations\n                                    using the GPT-3 Large Language Model\n                 Stephen MacNeil                                                 Andrew Tran                                         Dan Mogil\n          stephen.macneil@temple.edu                                    andrew.tran10@temple.edu                             daniel.mogil@temple.edu\n               Temple University                                            Temple University                                   Temple University\n              Philadelphia, PA, USA                                       Philadelphia, PA, USA                               Philadelphia, PA, USA\n\n                   Seth Bernstein                                                   Erin Ross                                     Ziheng Huang\n            seth.bernstein@temple.edu                                       erinross@temple.edu                                z8huang@ucsd.edu\n                Temple University                                            Temple University                         University of California\u2014San Diego\n               Philadelphia, PA, USA                                        Philadelphia, PA, USA                               La Jolla, CA, USA\n\nKEYWORDS                                                                                         explanation types to illustrate the explanatory power of GPT-3. The\nlarge language models, natural language processing, code explana-                                additional types include: 1) tracing the execution of code, 2) fixing\ntions, computer science education                                                                bugs and explaining how they were fixed, 3) generating analogies\n                                                                                                 to real world settings, 4) listing relevant programming concepts,\nACM Reference Format:\n                                                                                                 and 5) predicting the console output.\nStephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross,\nand Ziheng Huang. 2022. Generating Diverse Code Explanations using the\nGPT-3 Large Language Model. In Proceedings of the 2022 ACM Conference\non International Computing Education Research V.2 (ICER 2022), August 7\u201311,\n2022, Lugano and Virtual Event, Switzerland. ACM, New York, NY, USA,\n3 pages. https://doi.org/10.1145/3501709.3544280\n\n1     ABSTRACT\nGood explanations are essential to efficiently learning introductory\nprogramming concepts [10]. To provide high-quality explanations\nat scale, numerous systems automate the process by tracing the\nexecution of code [8, 12], defining terms [9], giving hints [16],\nand providing error-specific feedback [10, 16]. However, these ap-\nproaches often require manual effort to configure and only explain\na single aspect of a given code segment. Large language models\n(LLMs) are also changing how students interact with code [7]. For\nexample, Github\u2019s Copilot can generate code for programmers [4],\nleading researchers to raise concerns about cheating [7]. Instead,\nour work focuses on LLMs\u2019 potential to support learning by explain-                                 Figure 1: A prompt and explanation based on analogy.\ning numerous aspects of a given code snippet. This poster features\na systematic analysis of the diverse natural language explanations\nthat GPT-3 can generate automatically for a given code snippet. We\npresent a subset of three use cases from our evolving design space                               2.1    Analyzing and explaining time complexity\nof AI Explanations of Code.                                                                      Instructors rate time complexity as the most difficult programming\n                                                                                                 topic [17]. However, understanding time complexity is important [6,\n2     USE CASES                                                                                  13] because it facilitates decision-making so students choose an\nTo understand the types of explanations GPT-3 [2] can generate,                                  appropriate algorithm for a given problem. This use case shows\nwe issued over 700 prompts across numerous code snippets. An                                     GPT-3 can identify and explain time complexity.\nexample prompt and resulting explanation is shown in Figure 1.\nWe discovered eight explanation types and Figure 2 includes three                                2.2    Identifying common mistakes made by\nPermission to make digital or hard copies of part or all of this work for personal or                   beginner programmers\nclassroom use is granted without fee provided that copies are not made or distributed            Commonality exists in how students solve programming prob-\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.           lems [15] and the mistakes they make [1, 11]. Pedagogical tech-\nFor all other uses, contact the owner/author(s).                                                 niques, such as the \u2018muddiest point\u2019 highlight these common and\nICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland                              most confusing concepts [3, 14]. GPT-3 can automatically create\n\u00a9 2022 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-9195-5/22/08.                                                                a checklist of common mistakes students might make regarding a\nhttps://doi.org/10.1145/3501709.3544280                                                          given code snippet.\n\n\n\n\n                                                                                            37\n\fICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland                                                                                                MacNeil et al.\n\n\n\n\n    Figure 2: Three example explanations automatically generated by GPT-3 for an \u2018anonymized\u2019 Binary Search code snippet.\n\n\n2.3     Summarizing code at multiple levels of                                               [5] Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor O\u2019Rourke. 2022. Bring-\n                                                                                                 ing \"High-Level\" Down to Earth: Gaining Clarity in Conversational Program-\n        abstraction                                                                              mer Learning Goals. In Proceedings of the 53rd ACM Technical Symposium\nBefore understanding how a code snippet executes, it is often useful                             on Computer Science Education V. 1 (Providence, RI, USA) (SIGCSE 2022). As-\n                                                                                                 sociation for Computing Machinery, New York, NY, USA, 551\u2013557. https:\nto understand the purpose of the code [5]. The summary gener-                                    //doi.org/10.1145/3478431.3499370\nated by GPT-3 and shown in Figure 2 defines the goal, traces the                             [6] Elvina Elvina and Oscar Karnalim. 2017. Complexitor: An educational tool for\n                                                                                                 learning algorithm time complexity in practical manner. ComTech: Computer,\nexecution, and highlights relevant CS concepts such as arrays.                                   Mathematics and Engineering Applications 8, 1 (2017), 21\u201327.\n                                                                                             [7] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\n3     DISCUSSION                                                                                 James Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope-\n                                                                                                 nAI Codex on Introductory Programming. In Australasian Computing Education\nOur three use cases demonstrate the potential for GPT-3 to explain                               Conference (Virtual Event, Australia) (ACE \u201922). ACM, New York, NY, USA, 10\u201319.\ncode for intro CS students. Our poster presentation will feature all                             https://doi.org/10.1145/3511861.3511863\n                                                                                             [8] Philip J Guo. 2013. Online python tutor: embeddable web-based program visual-\neight explanation types as a design space of explanations to convey                              ization for cs education. In Proceeding of the 44th ACM technical symposium on\nthe diversity of explanations that can be generated by LLMs. We will                             Computer science education. 579\u2013584.\nhighlight best practices for generating effective explanations and                           [9] Andrew Head, Codanda Appachu, Marti A Hearst, and Bj\u00f6rn Hartmann. 2015.\n                                                                                                 Tutorons: Generating context-relevant, on-demand explanations and demonstra-\npitfalls that lead to less effective explanations. We are evaluating                             tions of online code. In 2015 IEEE Symposium on Visual Languages and Human-\nthe usefulness of these explanations in a series of summer classes.                              Centric Computing (VL/HCC). IEEE, 3\u201312.\n                                                                                            [10] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020.\n                                                                                                 Adaptive Immediate Feedback Can Improve Novice Programming Engagement\nREFERENCES                                                                                       and Intention to Persist in Computer Science. In Proceedings of the 2020 ACM\n [1] Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating              Conference on International Computing Education Research (Virtual Event, New\n     novice programming mistakes in large-scale student data. In Proceedings of the              Zealand) (ICER \u201920). Association for Computing Machinery, New York, NY, USA,\n     46th ACM Technical Symposium on Computer Science Education. 522\u2013527.                        194\u2013203. https://doi.org/10.1145/3372782.3406264\n [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,                 [11] Davin McCall and Michael K\u00f6lling. 2014. Meaningful categorisation of novice pro-\n     Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda                  grammer errors. In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings.\n     Askell, et al. 2020. Language models are few-shot learners. Advances in Neural              IEEE, 1\u20138.\n     Information Processing Systems 33 (2020), 1877\u20131901.                                   [12] Greg L Nelson, Benjamin Xie, and Amy J Ko. 2017. Comprehension first: eval-\n [3] Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013.                      uating a novel pedagogy and tutoring system for program tracing in CS1. In\n     \u201cUnmuddying\u201d course content using muddiest point reflections. In 2013 IEEE                  Proceedings of the 2017 ACM conference on international computing education\n     Frontiers in Education Conference (FIE). IEEE, 937\u2013942.                                     research. 2\u201311.\n [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira           [13] Miranda Parker and Colleen Lewis. 2014. What makes big-O analysis difficult:\n     Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,             understanding how students understand runtime analysis. Journal of Computing\n     et al. 2021. Evaluating large language models trained on code. arXiv preprint               Sciences in Colleges 29, 4 (2014), 164\u2013174.\n     arXiv:2107.03374 (2021).                                                               [14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany Vinci-Cannava, Laird\n                                                                                                 Kramer, and Maria Charters. 2020. WIP: An exploration into the muddiest points\n\n\n\n\n                                                                                       38\n\fGenerating Diverse Explanations with Large Language Models                                                  ICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland\n\n\n     and self-efficacy of students in introductory computer science courses. In 2020          [16] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards\n     IEEE Frontiers in Education Conference (FIE). IEEE, 1\u20135.                                      intelligent tutoring in novice programming environments. In Proceedings of the\n[15] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au-                    2017 ACM SIGCSE Technical Symposium on computer science education. 483\u2013488.\n     tonomously generating hints by inferring problem solving policies. In Proceedings        [17] Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory\n     of the second (2015) acm conference on learning@ scale. 195\u2013204.                              programming?. In Proceedings of the second international workshop on Computing\n                                                                                                   education research. 17\u201328.\n\n\n\n\n                                                                                         39\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nChris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Autonomously generating hints by inferring problem solving policies. In Proceedings of the second (2015) acm conference on learning@ scale . 195\u2013204.", "Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards intelligent tutoring in novice programming environments. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on computer science education . 483\u2013488.", "Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory programming?. In Proceedings of the second international workshop on Computing education research . 17\u201328."], "publish_date": "2022-08-07", "approved": true}}, {"_index": "articles", "_id": "cfLva40B_jNEtOrdiJq_", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Framing the News: From Human Perception to Large Language Model Inferencesqs", "abstract": "Identifying the frames of news is important to understand the articles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.", "authors": ["David Alonso del Barrio", "Daniel Gatica-Perez"], "institutions": ["Idiap Research Institute", "EPFL"], "keywords": ["Covid-19 no-vax", "news framing", "GPT-3", "prompt-engineering", "transformers", "large language models"], "full_text": "                      Framing the News:\n    From Human Perception to Large Language Model Inferences\n                           David Alonso del Barrio                                                                      Daniel Gatica-Perez\n                                  ddbarrio@idiap.ch                                                                        gatica@idiap.ch\n                               Idiap Research Institute                                                           Idiap Research Institute and EPFL\n                                     Switzerland                                                                             Switzerland\nABSTRACT                                                                                              1   INTRODUCTION\nIdentifying the frames of news is important to understand the arti-                                   In recent years, there has been a proliferation in the use of concepts\ncles\u2019 vision, intention, message to be conveyed, and which aspects                                    such as data journalism, computational journalism, and computer-\nof the news are emphasized. Framing is a widely studied concept                                       assisted reporting [15] [29], which all share the vision of bridging\nin journalism, and has emerged as a new topic in computing, with                                      journalism and technology. The progress made in NLP has been\nthe potential to automate processes and facilitate the work of jour-                                  gradually integrated into the journalistic field [5][8][54]. More\nnalism professionals. In this paper, we study this issue with articles                                specifically, machine learning models based on transformers have\nrelated to the Covid-19 anti-vaccine movement. First, to under-                                       been integrated in the media sector in different tasks [41] such as\nstand the perspectives used to treat this theme, we developed a                                       the creation of headlines with generative languages models [17],\nprotocol for human labeling of frames for 1786 headlines of No-                                       summarization of news articles [28][27], false news detection [49],\nVax movement articles of European newspapers from 5 countries.                                        and topic modeling and sentiment analysis [25]. The development of\nHeadlines are key units in the written press, and worth of analysis                                   large language models such as GPT-3 [9], BLOOM [51] or ChatGPT\nas many people only read headlines (or use them to guide their                                        show a clear trend towards human-machine interaction becoming\ndecision for further reading.) Second, considering advances in Nat-                                   easier and more intuitive, opening up a wide range of research\nural Language Processing (NLP) with large language models, we                                         possibilities. At the same time, the use of these models is also\ninvestigated two approaches for frame inference of news headlines:                                    associated with a lack of transparency regarding how these models\nfirst with a GPT-3.5 fine-tuning approach, and second with GPT-                                       work, but efforts are being made to bring some transparency to\n3.5 prompt-engineering. Our work contributes to the study and                                         these models, and to analyze use cases where they can be useful and\nanalysis of the performance that these models have to facilitate                                      where they cannot [35]. Based on the premises that these models\njournalistic tasks like classification of frames, while understanding                                 open up a wide range of research directions [7], and that at the same\nwhether the models are able to replicate human perception in the                                      time (and needless to say) they are not the solution to all problems,\nidentification of these frames.                                                                       we are interested in identifying use cases and tasks where they\n                                                                                                      can be potentially useful, while acknowledging and systematically\nCCS CONCEPTS                                                                                          documenting their limitations [56]. More specifically, the aim of\n                                                                                                      this work is to analyze the performance of GPT-3.5 for a specific\n\u2022 Computing methodologies \u2192 Information extraction; \u2022\n                                                                                                      use case, namely the analysis of frames in news, from an empirical\nHuman-centered computing \u2192 Text input.\n                                                                                                      point of view, with the objective of shedding light on a potential\n                                                                                                      use of generative models in journalistic tasks.\nKEYWORDS                                                                                                 Frame analysis is a concept from journalism, which consists of\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering, trans-                                      studying the way in which news stories are presented on an issue,\nformers, large language models                                                                        and what aspects are emphasized: Is a merely informative vision\n                                                                                                      given in an article? Or is it intended to leave a moral lesson? Is\nACM Reference Format:                                                                                 a news article being presented from an economic point of view?\nDavid Alonso del Barrio and Daniel Gatica-Perez. 2023. Framing the News:                              Or from a more human, emotional angle? The examples above\nFrom Human Perception to Large Language Model Inferences . In Inter-                                  correspond to different frames with which an article can be written.\nnational Conference on Multimedia Retrieval (ICMR \u201923), June 12\u201315, 2023,\n                                                                                                         The concept of news framing has been studied in computing as\nThessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3591106.3592278\n                                                                                                      a step beyond topic modeling and sentiment analysis, and for this\n                                                                                                      purpose, in recent years, pre-trained language models have been\n                                                                                                      used for fine-tuning the classification process of these frames [60]\nPermission to make digital or hard copies of all or part of this work for personal or                 [10], but the emergence of generative models opens the possibility\nclassroom use is granted without fee provided that copies are not made or distributed                 of doing prompt-engineering of these classification tasks, instead\nfor profit or commercial advantage and that copies bear this notice and the full citation             of the fine-tuning approach investigated so far.\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or                   Our work aims to address this research gap by posing the fol-\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission         lowing research questions:\nand/or a fee. Request permissions from permissions@acm.org.\n                                                                                                         RQ1: What are the main frames in the news headlines about\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.                     the anti-vaccine movement, as reported in newspapers across 5\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00                                                          European countries?\nhttps://doi.org/10.1145/3591106.3592278\n\n\n\n\n                                                                                                627\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                        David Alonso del Barrio and Daniel Gatica-Perez\n\n\n    RQ2: Can prompt engineering be used for classification of head-         message? Does the story make reference to morality, God, and other\nlines according to frames?                                                  religious tenets? Does the story offer specific social prescriptions\n    By addressing the above research questions, our work makes the          about how to behave?\", and so on for each of the frame types. This\nfollowing contributions:                                                    categorization of frames has been used in various topics such as\n    Contribution 1. We implemented a process to do human an-                climate change [18] [19], vaccine hesitance [13], or immigration\nnotation of the main frame of 1786 headlines of articles about the          [34].\nCovid-19 no-vax movement, as reported in 19 newspapers from 5                   We now compare the two approaches on a common topic, such\nEuropean countries (France, Italy, Spain, Switzerland and United            as Covid-19. Ebrahim et al. [21] followed an inductive approach\nKingdom.) At the headline level, we found that the predominant              in which the frames were not predefined but emerged from the\nframe was human interest, where this frame corresponds to a per-            text (e.g., deadly spread, stay home, what if, the cost of Covid-19)\nsonification of an event, either through a statement by a person,           using headlines as the unit of analysis. In contrast, the deductive\nor the explanation of a specific event that happened to a person.           approach has studied very different labels. El-Behary et al. [22]\nFurthermore, we found a large number of headlines annotated as              followed the method of yes/no questions, but in addition to the\ncontaining no frame, as they simply present information without             5 generic frames presented before, they also used blame frame\nentering into evaluations. We also found that for all the countries         and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5\ninvolved, the distribution of frame types was very similar, i.e., hu-       generic frames with yes/no questions, while Catal\u00e1n-Matamoros et\nman interest and no frame are the two predominant frames. Finally,          al. [14] used the 5 frames and read the headline and subheadline\nthe generated annotations allowed to subsequently study the per-            to decide the main frame. Table 1 summarizes some of the the\nformance of a large language model.                                         existing approaches. This previous work showed how frame labels\n    Contribution 2. We studied the performance of GPT-3.5 on                can be different, and also that frame analysis has been done at both\nthe task of frame classification of headlines. In addition to using         headline and article levels. These two approaches (inductive and\nthe fine-tuning approach from previous literature, we propose an            deductive) that originated in journalism have since been replicated\nalternative approach for frame classification that requires no labeled      in the computing literature.\ndata for training, namely prompt-engineering using GPT-3.5. The                 We decided to follow the deductive approach because a prede-\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy            fined list of frames allows to compare among topics, countries,\n(slightly higher than other smaller models), and that the prompt-           previous literature, and also because they represent a fixed list of\nengineering approach results in lower performance (49% accuracy.)           labels for machine classification models. Furthermore, the induc-\nOur analysis also shows that the subjectivity of the human labeling         tive approach tends to be more specific to a topic, and from the\ntask has an effect on the obtained accufracy.                               computing viewpoint, past work has tried to justify topic modeling\n    The paper is organized as follows. In Section 2, we discuss related     as a technique to extract frames from articles.\nwork. In Section 3, we describe the news dataset. In Section 4, we              Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex-\ndescribe the methodology for both human labeling and machine                traction technique. They argued that topics can be interpreted as\nclassification of news frames. We present and discuss results for           frames if three requirements are met: frames are operationalized as\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide          connections between concepts; subject-specific data is selected; and\nconclusions in Section 7.                                                   topics are adequately validated as frames, for which they suggested\n                                                                            a practical procedure. This approach was based on the choice of a\n                                                                            specific topic (e.g., climate change) and the use of Latent Dirichlet\n2    RELATED WORK                                                           Allocation (LDA) as a technique to extract a number of subtopics.\nFraming has been a concept widely studied in journalism, with a             In a second phase, a qualitative study of the top 10 words of each\ndefinition that is rooted in the study of this domain [23]: \u201cTo frame       subtopic was performed, and the different subtopics were elimi-\nis to select some aspects of a perceived reality and make them more         nated or grouped, reducing the number and establishing a tentative\nsalient in a communicating text, in such a way as to promote a par-         description. In a third phase, the top 10 articles belonging to that\nticular problem definition, causal interpretation, moral evaluation,        frame/topic were taken, and if the description of the topic fitted\nand/or treatment recommendation for the item described.\u201d                    at least 8 of the 10 articles, that topic/frame remained. The frames\n    For frame recognition, there are two main approaches: the induc-        found in this article were: green growth, emission cuts, negotiations\ntive approach [16], where one can extract the frames after reading          and treaties, environmental risk, cost of carbon emissions, Chinese\nthe article, and the deductive approach [38], where a predefined            emissions, economics of energy production, climate change, en-\nlist of frames exists and the goal is to interpret if any of them ap-       vironmental activism, North-South burden sharing, state leaders\npears in the article. In the deductive case, there are generic frames       negotiating, and citizen participation.\nand subject-specific frames, and the way to detect them typically               From Entman\u2019s definition of frame [23], it seems that the deduc-\ninvolves reading and identifying one frame at a time, or through            tive approach is more refined than the inductive approach (which\nanswers to yes/no questions that represent the frames. Semetko et           seems to resemble the detection of sub-themes.) For example, with\nal. [52] used 5 types of generic frames (attribution of responsibility,     regard to climate change, there are stories on how people have been\nhuman interest, conflict, morality, and economic consequences)              affected by climate change from an emotional point of view, thus\nbased on previous literature, and they defined a list of 20 yes/no          personalizing the problem. In this case, we could categorize the\nquestions to detect frames in articles. For instance, the questions         corresponding frame as human interest, as the writer of the article\nabout morality are the following: \"Does the story contain any moral         is selecting \"some aspects of a perceived reality and make them\n\n\n\n\n                                                                      628\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                            ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\nmore salient\". The language subtleties with which news articles are            Table 1: Summary of deductive approaches for frame analysis\npresented cannot be captured with basic topic modeling.\n   Isoaho et al.[30] held the position that while the benefits of                  Ref Frames                                         Goal                       Technique          Number\n                                                                                                                                                                                    samples\n                                                                                                                                                                                                of\n\nscale and scope in topic modeling were clear, there were also a                    [12] 15 generic frames: \"Economic\", \"Capac-        To label frames of full    Reading the\n                                                                                                                                                                                    20000 articles\nnumber of problems, namely that topic outputs do not correspond                         ity and resources\", \"Morality\", \"Fair-        articles                   full    article,\n                                                                                        ness and equality\", \"Legality, constitu-                                 the annotator\nto the methodological definition of frames, and thus topic modeling                     tionality and jurisprudence\", \"Policy                                    defines     the\n                                                                                        prescription and evaluation\", \"Crime                                     main frame\nremained an incomplete method for frame analysis. Topic modeling,                       and punishment\", \"Security and de-\n                                                                                        fense\", \"Health and safety\", \"Quality of\nin the practice of journalistic research, is a useful technique to deal                 life\", \"Cultural identity\", \"Public opin-\nwith the large datasets that are available, yet is often not enough to                  ion\", \"Political\", \"External regulation\n                                                                                        and reputation\", \"Other\".\ndo more thorough analyses [31]. In our work, we clearly notice that                [33] 15 generic frames                             Classification             BERT based\n                                                                                                                                                                                    12000 articles\n                                                                                                                                                                 models\nframe analysis is not topic modeling. For example, two documents                   [52] 5 generic frames: \"human interest\",           To label frames of full    Yes/No ques-       2600 articles\n                                                                                        \"conflict\", \"morality\", \"attribution of       articles                   tions.             and 1522 tv\ncould be about the same topic, say Covid-19 vaccination, but one                        responsibility\", and \"economic conse-                                                       news stories\narticle could emphasize the number of deaths after vaccination,                         quences\".\n                                                                                   [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic         To label frames of full    Reading the        2990 headlines\nwhile the other emphasized the role of the vaccine as a solution to                     opinion\u201d, \u201cSociety/Culture\u201d, and              articles/ Classification   full    article,\n                                                                                        \u201cEconomic consequences\u201d , \u201c2nd                                           the annotator\nthe epidemic.                                                                           Amendment\u201d (Gun Rights), \u201cGun                                            defines     the\n                                                                                        control/regulation\u201d, \u201cMental health\u201d,                                    main frame.\n   We also consider that the larger the number of possible frame                        \u201cSchool/Public space safety\u201d, and                                        BERT based\n                                                                                        \u201cRace/Ethnicity\u201d.                                                        models\ntypes, the more likely it is to end up doing topic modeling instead of             [22] 5 generic frames + blame frame and            To label frames of full    Yes/No ques-       1170 articles\nframe analysis. Using a deductive approach, Dallas et al. [12] created                  fear frame\n                                                                                   [1] 5 generic frames\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        6713 articles\na dataset with articles about polemic topics such as immigration,                                                                     articles                   full    article,\n                                                                                                                                                                 the annotator\nsame sex marriage, or smoking, and they defined 15 types of frames:                                                                                              defines     the\n                                                                                                                                                                 main frame.\n\"economic, capacity and resources, morality, fairness and equality,                [50] 5 generic frames + pandemic frames            To label frames of full    Yes/No ques-       2742 articles\nlegality, constitutionality and jurisprudence, policy prescription and             [14] 5 generic frames, journalistic role and\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        131 headlines +\nevaluation, crime and punishment, security and defense, health and                      pandemic frames                               articles                   headline and       subheadlines\n                                                                                                                                                                 subheadline,\nsafety, quality of life, cultural identity, political, external regulation                                                                                       the annotator\n                                                                                                                                                                 defines     the\nand reputation, other\". In this case, they authors did not use a list                                                                                            main frame.\nof questions. Instead, for each article, annotators were asked to\nidentify any of the 15 framing dimensions present in the article\nand to label text blurbs that cued them (based on the definitions of\n                                                                               the potential applications, and the challenges to face when using\neach of the frame dimensions) and decide the main frame of each\n                                                                               this approach. Prompt engineering applications include knowledge\narticle. In our case, we followed the idea of detecting the main frame\n                                                                               probing [46], information extraction [53], NLP reasoning [57], ques-\nby reading the text instead of answering questions, but instead of\n                                                                               tion answering [32], text generation [20], multi-modal learning [58],\nusing the 15 frames proposed in [12] , we used the 5 generic frames\n                                                                               and text classification [24], the latter being the prompt-engineering\nproposed in [52].\n                                                                               use case in our work. Puri et al.[45] presented a very interesting\n   A final decision in our work was the type of text to analyze,\n                                                                               idea that we apply to our classification task. This consists of pro-\nwhether headlines or whole article. For this decision, the chosen\n                                                                               viding the language model with natural language descriptions of\nclassification method was also going to be important. For example,\n                                                                               classification tasks as input, and training it to generate the correct\nKhanehzar et al. [33] used traditional approaches such as SVMs as\n                                                                               answer in natural language via a language modeling objective. It is\nbaseline, and demonstrated the improvement in frame classifica-\n                                                                               a zero-shot learning approach, in which no examples are used to\ntion with the use of pre-trained languages models such as BERT,\n                                                                               explain the task to the model. Radford et al. [48] demonstrated that\nRoBERTa and XLNet, following a fine-tuning approach, setting\n                                                                               language models can learn tasks without any explicit supervision.\nas input text a maximum of 256 tokens (although the maximum\n                                                                               We have followed this approach to find an alternative way to do\nnumber of input tokens in these models is 512 tokens.) Liu et al.\n                                                                               frame analysis.\n[37] classified news headlines about the gun problem in the United\n                                                                                   As mentioned before, the emergence of giant models like GPT-3,\nStates, arguing for the choice of headlines as a unit of analysis\n                                                                               BLOOM, and ChatGPT are a very active research topic. To the best\nbased on previous journalism literature [6], [44], that advocated\n                                                                               of our knowledge, on one hand our work extends the computational\nfor the importance and influence of headlines on readers and the\n                                                                               analysis of news related to the covid-19 no-vax movement, which\nsubsequent perception of articles. From a computational viewpoint,\n                                                                               illustrates the influence of the press on the ways societies think\nusing headlines is also an advantage, since you avoid the 512 token\n                                                                               about relevant issues [40], [59], and on the other hand it adds to\nlimitation in BERT-based models. Therefore, we decided to work\n                                                                               the literature of human-machine interaction, regarding the design\nwith headlines about a controversial issue, namely the Covid-19\n                                                                               of GPT-3 prompts for classification tasks [39], [2].\nno-vax movement.\n   Continuing with the question of the methods used for classi-\nfication, much work has been developed in prompt engineering,\n                                                                               3        DATA: EUROPEAN COVID-19 NEWS\nespecially since the release of GPT-3. Liu et al.[36] presented a good                  DATASET\noverview of the work done on this new NLP paradigm, not only                   We used part of the European Covid-19 News dataset collected in\nexplaining the concept of prompt engineering, but also the differ-             our recent work [3]. This dataset contains 51320 articles on Covid-\nent strategies that can be followed both in the design of prompts,             19 vaccination from 19 newspapers from 5 different countries: Italy,\n\n\n\n\n                                                                         629\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                                                   David Alonso del Barrio and Daniel Gatica-Perez\n\n\nFrance, Spain, Switzerland and UK. The articles cover a time period                                                   [52] with 5 generic frames (attribution of responsibility, human\nof 22 months, from January 2020 to October 2021. All content was                                                      interest, conflict, morality, and economic consequences) plus one\ntranslated into English to be able to work in a common language.                                                      additional \u2019no-frame\u2019 category. Two researchers were engaged to\nThe dataset was used for various analyses, such as name entity                                                        annotate a sample of the collected newspaper articles following a\nrecognition, sentiment analysis, and subtopic modeling, to under-                                                     three-phase training procedure.\nstand how Covid-19 vaccination was reported in Europe through                                                            In the first phase, annotators had to read the codebook and get\nthe print media (in digital format.) The subtopic modeling analysis                                                   familiar with the task. In the second phase, they were asked to\nrevealed a subsample of articles on the no-vax movement, which is                                                     identify the main frame in the same subset of 50 headlines. At the\nthe one we have used in this paper. We took the headlines of the                                                      end of the second phase, the intercoder reliability (ICR) was 0.58\narticles associated with the no-vax movement, selecting all articles                                                  between the 2 annotators. We analyzed those cases where there\ncontaining any of the keywords in Table 2 in the headline or in the                                                   were discrepancies, and observed that in some cases, there was not a\nmain text. This corresponds to a total of 1786 headlines.                                                             unique main frame, because both annotators had valid arguments to\n                                                                                                                      select one of the frames. In other cases, the discrepancies were due\n      Table 2: Keywords used to identify no-vax articles                                                              to slight misunderstanding of the definitions. In the third phase, the\n                                                                                                                      annotators coded again 50 headlines, and the ICR increased to was\n                                                         Keywords                                                     0.66. We realized that the possibility of having two frames remained.\n NO VAX TOPIC    \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"         They discussed the cases in which they had disagreed, and if the\n                                                                                                                      other person\u2019s arguments were considered valid, it could be said that\n   In Table 3, we show the number of headlines per country and                                                        there were two frames. After this three-phase training procedure,\nnewspaper. France is the country with the most no-vax articles in                                                     annotators were ready to annotate the dataset independently. We\nthe corpus, with 523 articles, followed by Italy with 508. However,                                                   divided the dataset into two equal parts, and each person annotated\nnote that there are 6 newspapers from France, while only 2 from                                                       893 headlines.\nItaly. Corriere della Sera is the newspaper that dealt most frequently\nwith the subject (429 articles), while The Telegraph is the second                                                    4.2    Fine-tuning GPT-3.5 and BERT-based\none (206 articles). The total number of articles normalized by the                                                           models\nnumber of newspapers per country is also shown in the last column                                                     With the annotated dataset, we investigated two NLP approaches:\nof the Table. Using these normalized values, the ranking is Italy,                                                    the first one involves fine-tuning a pre-trained model; the second\nUK, France, Switzerland, and Spain.                                                                                   one is prompt engineering. Pre-trained language models have been\n\n Table 3: Number of headlines by newspaper and country\n\n      COUNTRY               NEWSPAPER                HEADLINES             TOTAL (NORM. TOTAL)\n      FRANCE                    La Croix                     94                       523 (87.1)\n                               Le Monde                     125\n                               Les Echos                     49\n                              Liberation                     97\n                            Lyon Capitale                     8\n                            Ouest France                    150\n        ITALY             Corriere della Sera               429                      508 (254.0)\n                            Il Sole 24 Ore                   79\n        SPAIN                 20 minutos                     27                       303 (50.5)\n                                  ABC                        50\n                               El Diario                     32                                                                    Figure 1: Pre-train, fine-tune, prompt\n                               El Mundo                      77\n                              El Espa\u00f1ol                     22\n                           La Vanguardia                     95\n                               24 heures                     97                       230 (76.6)\n   SWITZERLAND\n                              La Libert\u00e9                     22\n                                                                                                                      trained with large text strings based on two unsupervised tasks,\n                               Le Temps                     111                                                       next sentence prediction and masked language model. Figure 1\n                           The Irish News                    16                      222 (111.0)\n UNITED KINGDOM\n                           The Telegraph                    206                                                       summarizes these techniques.\n                                                                                         1786                            In the first approach, a model with a fixed architecture is pre-\n                                                                                                                      trained as a language model (LM), predicting the likelihood of the\n                                                                                                                      observed textual data. This can be done due to the availability of\n4 METHODOLOGY                                                                                                         large, raw text data needed to train LMs. This learning process can\n                                                                                                                      produce general purpose features of the modeled language. The\n4.1 Human labeling of news frames                                                                                     learning process produces robust, general-purpose features of the\nTo carry out the labeling of the frames in our corpus of headlines, we                                                language being modeled. The above pre-trained LM is then adapted\nfirst designed a codebook, which contained the definitions of each                                                    to different downstream tasks, by introducing additional parameters\nof the frame types and a couple of examples of each type, as well                                                     and adjusting them using task-specific objective functions. In this\nas a definition of the corpus subject matter and definitions of the                                                   approach, the focus was primarily on goal engineering, designing\nconcept of frame analysis, so that the annotators could understand                                                    the training targets used in both the pre-training and the fine-tuning\nthe task to be performed. The codebook follows the proposed by                                                        stages [36].\n\n\n\n\n                                                                                                                630\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                    ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n    We present an example to illustrate the idea. Imagine that the        adjective was matched with a sentiment. In a similar fashion, we\ntask is sentiment analysis, and we have a dataset with sentences          decided to build a thesaurus of adjectives that define each of the\nand their associated sentiment, and a pre-trained model, which is a       frames. For instance, the human interest frame could be \u2019interest-\nsaved neural network trained with a much larger dataset. For that         ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be:\npre-trained model to address the target task, we unfreeze a few of        \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou-\nthe top layers of the saved model base and jointly train both the         blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined,\nnewly-added classifier layers and the last layers of the base model.      we needed to define the prompt in order to get, as an answer, one\nThis allows to \"fine-tune\" the higher-order feature representations       of the adjectives in our thesaurus to match them with the frame.\nin the base model to make them more relevant for the sentiment            We used the GPT-3 playground using the headline as input and\nanalysis task. In this way, instead of having to obtain a very large      asking for the frame as output, but the strategy did not work. In\ndataset with target labels to train a model, we can reuse the pre-        our final experiment, instead of giving the headline as input, we\ntrained model and use a much smaller train dataset. We use a part         gave the definitions of each type of frame plus the headline, and we\nof our dataset as examples for the model to learn the task, while         asked the model to choose between the different types of frames\nthe other part of the dataset is used to evaluate model performance.      as output. In this way, the output of the model was directly one of\n    Previous works related to frame classification in the computing       the frames, and we avoided the step of matching adjectives with\nliterature have used fine-tuning, BERT-based models. In our work,         frames. An example is shown in Figure 2.\nwe have done the same as a baseline, but we aimed to go one step\nfurther and also produce results using fine-tuning of GPT-3.5.\n\n\n4.3    Prompt-engineering with GPT-3.5\nModel fine-tuning has been widely used, but with the emergence\nof generative models such as GPT-3, another way to approach\nclassification tasks has appeared. The idea is to use the pre-trained\nmodel directly and convert the task to be performed into a format\nas close as possible to the tasks for which it has been pre-trained.\nThat is, if the model has been pre-trained from next word prediction          Figure 2: GPT-3.5 for frame inference: input and output\nas in the case of GPT-3, classification can be done by defining a\nprompt, where the input to the model is an incomplete sentence,               For the GPT-3 configuration 1 , there are 3 main concepts:\nand the model must complete it with a word or several words, just\n                                                                                \u2022 TEMPERATURE [0-1]. This parameter controls randomness,\nas it has been trained. This avoids having to use part of the already\n                                                                                  lowering it results in less random completions.\nlabeled dataset to teach the task to be performed to the model, and\n                                                                                \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus\na previous labeling is not needed [36].\n                                                                                  sampling.\n    In this approach, instead of adapting pre-trained LMs to down-\n                                                                                \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-\nstream tasks via objective engineering, downstream tasks are re-\n                                                                                  mum number of tokens to generate,\nformulated to look more like those solved during the original LM\n                                                                                \u2022 MODEL. GPT-3 offer four main models with different levels\ntraining with the help of a textual prompt. For example, when recog-\n                                                                                  of power, suitable for different tasks. Davinci is the most\nnizing the emotion of a social media post, \u201cI missed the bus today.\u201d,\n                                                                                  capable model, and Ada is the fastest.\nwe may continue with a prompt \u201cI felt so _\u201d, and ask the LM to\nfill the blank with an emotion-bearing word. Or if we choose the          After testing with the GPT-3 playground and varying different\nprompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may           hyper-parameters to assess performance, we set the temperature to\nbe able to fill in the blank with a French translation. In this way,      0, since the higher the temperature the more random the response.\nby selecting the appropriate prompts, we can influence the model          Furthermore, the Top-p parameter was set to 1, as it would likely\nbehavior so that the pre-trained LM itself can be used to predict the     get a set of the most likely words for the model to choose from. The\ndesired output, even without any additional task-specific training        maximum number of tokens was set to 2; in this way, the model\n[36].                                                                     is asked to choose between one of the responses. As a model, we\n    We use this emerging NLP approach to classify frames at headline      used the one with the best performance at the time of experimental\nlevel. We are not aware of previous uses of this strategy to classify     design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\nframes as we propose here. The idea is the following. Prompt engi-\nneering consists of giving a prompt to the model, and understands         5     RESULTS: HUMAN LABELING OF FRAMES\nthat prompt as an incomplete sentence. To do prompt engineer-                   IN NO-VAX NEWS HEADLINES (RQ1)\ning with our dataset, we needed to define an appropriate prompt           In this section, we present and discuss the results of the analysis\nthat would produce the headline frames as output. We defined sev-         related to our first RQ.\neral experiments with the Playground of GPT-3, in order to find              Figure 3 shows the distribution of frames per country at headline\nthe best prompt for our task. In our initial experiments, we fol-         level, with human interest and no-frame being the predominant\nlowed existing approaches in prompt engineering to do sentiment\nanalysis, where the individual answer was an adjective, and this          1 https://beta.openai.com/docs/introduction\n\n\n\n\n                                                                    631\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                       David Alonso del Barrio and Daniel Gatica-Perez\n\n\nones. Attribution of responsibility is the third one except in Switzer-\nland, where the corresponding frame is conflict. Finally, morality\nand economic are the least represented in the dataset for every\ncountry.\n\n\n\n\n                                                                            Figure 4: Non-normalized monthly distribution of frames.\n\n\n\n                                                                            detail the negative and positive sentiment of each frame category,\n                                                                            we observed a few trends:\n                                                                                \u2022 Attribution of responsibility: Negative sentiment represents\n                                                                                  30-40% of the cases, while positive tone is only found in\n                                                                                  residual form in Italy, Switzerland, and the United Kingdom.\n                                                                                \u2022 Conflict: Negative sentiment represents 20-35% of the cases.\nFigure 3: Non-normalized distribution of frames per country                     \u2022 Economic: Predominantly neutral, with only negative tone\n                                                                                  in Italy and UK (in the latter case, all headlines with this\n    The monthly distribution of frames aggregated for all countries               frame were considered negative.)\nis shown in Fig. 4. We can see two big peaks, the first one in January          \u2022 Human interest: Negative sentiment represents 30-40% of\n2021 and the second one in August 2021. In all countries, the vac-                the cases, while positive tone is only found in residual form\ncination process started at the end of December 2020, so it makes                 in Italy, Spain, and Switzerland.\nsense that the no-vax movement started to be more predominant in                \u2022 Morality: Predominantly neutral, with negative tone in Italy,\nthe news in January 2021. Human interest is the most predominant                  Switzerland, and the United Kingdom,\nframe. Manual inspection shows that this is because the headlines               \u2022 No frame: 20-30% of negative content.\nare about personal cases of people who are pro- or anti- vaccine.\nAttribution of responsibility is also present. Manual inspection in-\ndicates that local politicians and health authorities had to make\ndecisions about who could be vaccinated at the beginning of the\nprocess. The second peak at the end of summer 2021 coincided\nwith the health pass (also called Covid passport in some countries),\nand we can observe a peak in the curve corresponding to the con-\nflict frame, reflecting the demonstrations against the measure of\nmandatory health passes taken by country governments.\n    In Figure 5, we compare the sentiment per frame and per country,\nto understand if there were any major differences. The sentiment\nanalysis labels were obtained using BERT-sent from the Hugging\nFace package [47], used in our previous work (please refer to our\noriginal analysis in [3] for details.) We normalized the results be-\ntween 0 and 1 to compare frames between countries. We see that the\nsentiment is predominantly neutral (in blue). Examining in more              Figure 5: Sentiment of headline by frame and by country\n\n\n\n\n                                                                      632\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                   ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n   Regarding the results of the annotation process, the fact that the             On the other hand, BERT is open-source, while GPT-3 has an\ndistribution of the 6 frame types is relatively similar between coun-         economic cost as the use of the model is not free, which monetarily\ntries suggests that the anti-vaccine movement issue was treated               limits the number of experiments that can be performed with it,\nin a similar way in these countries. The fact that human interest             as well as the different configurations one can explore to improve\nis the most dominant frame indicates that this issue was treated              performance. This is important because much of the improvement\nfrom a more human and emotional approach, with headlines about                in performance requires empirical explorations of model parameters\npersonal experiences, celebrities giving their opinion about vacci-           More specifically, the cost of an experiment for each of the folds has\nnation, and politicians defending vaccine policies. Moreover, the             a cost of 4 dollars (at the time of writing this paper.) This represents\nreason for many headlines being classified as no-frame is partly              a limitation in practice.\ndue to how data was selected. We chose articles that contained                    Furthermore, GPT-3 has a significant carbon footprint. Similarly,\nwords related to no-vax, either in the headline or in the article. This       for prompt engineering (discussed in the next subsection), choosing\nresulted in many headlines not containing anything specific related           the right prompt (i.e., the words that best define the task so that the\nto no-vax, while the no-vax content was actually included in the              model is able to perform adequately) is also based on trial and error.\nmain text of the corresponding articles.                                      This also has an impact on carbon footprint. In connection with\n   It is worth mentioning that prior to obtaining the results, we had         this topic, Strubell et al.[55] argue that improvements in the accu-\nexpected that attribution of responsibility would be among the most           racy of models depend on the availability of large computational\nprominent frames, since governments took many measures such as                resources, which involve large economic and environmental costs.\nmandatory health pass requirements to access certain sites; we had            A criticism has been made as \u2019the rich get richer\u2019, in the sense that\nalso expected that the conflict frame would be prominent, since               not all research groups have sufficient infrastructure resources and\nthere were many demonstrations in Europe. In reality, however,                access to funding needed to use these models and improve their\nthese frames categories were not reflected as frequently at the               performance. Also in relation to this analysis, the work of Bender\nheadline level.                                                               et al. [4] evaluates the costs and risks of the use of large language\n   Regarding the analysis at the temporal level, it is clear that certain     models, stating that researchers should be aware of the impact that\nevents were captured by the press, such as the start of vaccination           these models have on the environment, and assess whether the\nor the mandatory vaccination passport.                                        benefits outweigh the risks. The work in [4] provides a very telling\n   Finally, the sentiment analysis of the different frames shows that         example, where people living in the Maldives or Sudan are affected\nthe predominant tone in all of them is neutral or negative, with very         by floods and pay the environmental price of training English LLMs,\nsimilar trends between countries. This association between senti-             when similar models have not been produced for languages like\nment analysis and frames has been discussed in previous literature            Dhivehi or Sudanese Arab. In short, there is a need to establish\n[11] [43].                                                                    ways to use this technological development responsibly, and it all\n                                                                              starts with being aware of the risks it presents.\n6     RESULTS: GPT-3.5 FOR FRAME\n      CLASSIFICATION OF HEADLINES (RQ2)                                       6.2    Prompt-engineering with GPT-3.5\nHere, we present and discuss the results related to our second RQ.            For each headline, we got the frame that the model considered the\n                                                                              most likely, and we compared these GPT-3.5 inferences with the\n6.1    Fine-tuning GPT-3.5                                                    frames labeled by the annotators. The agreement between model\nTable 4 shows the results of the 6-class classification task using            and annotator was of 49%. Analyzing the results, and specifically\n5-cross validation. Three models were used: GPT-3.5 and two BERT-             looking at the cases where the annotator and GPT-3.5 disagreed,\nbased models. We observe that, on average, GPT-3.5 performs better            we discovered that according to the frame definitions, the model\nthan the BERT-based models. This is somehow expected as GPT-                  in some cases proposed a frame that indeed made sense. This ob-\n3.5 is a much larger model. Overall, in the case of fine-tuning, the          servation, together with our previous experience in the annotation\nbest performance for the six-class frame classification task is 72%           process, where headlines could have more than one valid frame,\naccuracy, which is promising, with an improvement over previous               led us to design a second post-hoc experiment. We took all the\nmodels based on BERT. Yet, it should be noted that the performance            headlines where each of the two annotators had disagreed with\ndifferences are modest (2% improvement between GPT-3.5 and                    GPT-3.5, and we asked the annotators to state whether they would\nRoBERTa).                                                                     agree (or not) with each GPT-inferred label for a given headline.\n                                                                              It is important to emphasize that the annotators did not know the\n                                                                              origin of that label, i.e., they did not know if it was the label they\nTable 4: Classification results for six-class frame classifica-               had originally assigned, or if it was a random one. In this way, we\ntion and 5-fold cross validation                                              could quantify how GPT-3.5 worked according to valid arguments\n                                                                              provided by the annotators. In this post-hoc experiment, the model\n                                                                              agreed in 76% of cases with the annotators.\n    ACCURACY          0       1       2       3       4     AVERAGE\n                                                                                  Looking at the results of the classification models, the 49% accu-\n    BERT            0.68     0.69    0.72    0.64    0.70     0.67            racy of the prompt-engineering approach can be considered low,\n    RoBERTa         0.70     0.72    0.72    0.67    0.71     0.70\n                                                                              yet we consider that it is a valid avenue for further investigation,\n    GPT3            0.75     0.70    0.72    0.71    0.71     0.72\n                                                                              as in the second post-hoc analysis, we found that the model agrees\n\n\n\n\n                                                                        633\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                David Alonso del Barrio and Daniel Gatica-Perez\n\n\nwith human annotators in 76% of the cases. Clearly, framing in-             and in an post-doc experiment, the human-machine agreement\nvolves aspects of subjectivity [42]. Much of what we do as people           reached 76%. These results have opened several new directions for\nhas a subjective component, influenced by how we feel or how we             future work.\nexpress opinions.\n   News reading is never fully objective, and the annotators en-            ACKNOWLEDGMENTS\ngaged in the frame classification task, influenced by their personal        This work was supported by the AI4Media project, funded by the\nstate of mind, experience, and culture, may perceive information            European Commission (Grant 951911) under the H2020 Programme\ndifferently. Monarch affirms that \"for simple tasks, like binary labels     ICT-48-2020. We also thank the newspapers for sharing their online\non objective tasks, the statistics are fairly straightforward to decide     articles. Finally, we thank our colleagues Haeeun Kim and Emma\nwhich is the \u2018correct\u2019 label when different annotators disagree. But        Bouton-Bessac for their support with annotations, and Victor Bros\nfor subjective tasks, or even objective tasks with continuous data,         and Oleksii Polegkyi for discussions.\nthere are no simple heuristics for deciding what the correct label\nshould be\" [42].                                                            REFERENCES\n   Subjectivity is involved in both the generation and perception            [1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online\nof information: the assumption that there is only one frame is com-              media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.\n                                                                                 Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.\nplicated by the point of view of the reader. In the case of news, the        [2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess\ninformation sender (the journalist) has an intention, but the receiver           Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021.\n(the reader) plays a role and is influenced by it. In psychology, this           RAFT: A real-world few-shot text classification benchmark. arXiv preprint\n                                                                                 arXiv:2109.14076 (2021).\nis known as the lens model of interpersonal communication, where             [3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press\nthe sender has certain objectives, but the receiver can interpret                Cover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35\u201343.\n                                                                                 https://doi.org/10.1145/3512732.3533588\nor re-interpret what the sender wants to say, with more or less              [4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\naccuracy [26].                                                                   Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\n   Following this discussion on subjectivity, the question arose as to           Be Too Big? (2021), 610\u2013623.\n                                                                             [5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in\nwhat would happen if, instead of headlines, we used the complete                 journalism: A boon or bane? In Optimization in machine learning and applications.\narticle as a source of analysis. We wondered if longer text could                Springer, 155\u2013167.\nmake the frame labeling task clearer than when using headlines.              [6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015.\n                                                                                 Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013\nYet another possible hypothesis is that having to read longer texts              2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.\ncould lead to the same subject being presented from different angles.        [7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.\n                                                                                 https://doi.org/10.48550/ARXIV.2212.14402\nPlease recall that in the existing literature discussed in Section 2,        [8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe,\nboth headlines and full articles have been used from frame analysis              Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour-\n(see Table 1.) This remains as an issue for future work.                         nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695.\n                                                                             [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                 Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                 Askell, et al. 2020. Language models are few-shot learners. Advances in neural\n7    CONCLUSIONS                                                                 information processing systems 33 (2020), 1877\u20131901.\n                                                                            [10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H\nIn this paper, we first presented an analysis of human-generated                 De Vreese. 2014. Teaching the computer to code frames in news: Comparing\nnews frames on the covid-19 no-vax movement in Europe, and                       two supervised machine learning approaches to frame analysis. Communication\n                                                                                 Methods and Measures 8, 3 (2014), 190\u2013206.\nthen studied different approaches using large language models for           [11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond\nautomatic inference of frames. We conclude by answering the two                  words: Applying cluster and sentiment analysis to news coverage of the nuclear\nresearch questions we posed:                                                     power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.\n                                                                            [12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015.\n   RQ1: What are the main frames in the news headlines about the                 The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015),\ncovid-19 anti-vaccine movement in 5 European countries? After                    438\u2013444. https://doi.org/10.3115/v1/P15-2072\nannotating the headlines, we found that of the 1786 headlines,              [13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age\n                                                                                 of coronavirus and fake news: analysis of journalistic sources in the Spanish\nthe predominant frame is human interest (45.3% of cases), which                  quality press. International Journal of Environmental Research and Public Health\npresents a news item with an emotional angle, putting a face to a                17, 21 (2020), 8136.\n                                                                            [14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust\nproblem or situation. We also found that a substantial proportion                of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n\nof headlines were annotated as not presenting any frame (40.2% of                social 74 (2019), 786\u2013802.\ncases). Finally, the other frame types are found more infrequently.         [15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology\n                                                                                 for evaluating data journalism, computational journalism, and computer-assisted\n   RQ2: Can prompt engineering be used for classification of head-               reporting. Digital journalism 3, 3 (2015), 331\u2013348.\nlines according to frames? We first used fine-tuning of a number of         [16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News\nlanguage models, and found that GPT-3.5 produced classification ac-              Framing Analysis. Routledge, 151\u2013172.\n                                                                            [17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,\ncuracy of 72% on a six-frame classification task. This represented a             1 (2021), 113\u2013118.\nmodest 2% improvement over BERT-based models, at a significantly            [18] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A de-\n                                                                                 ductive frame-analysis of Dutch and French climate change coverage dur-\nlarger environmental cost. We then presented a new way of classi-                ing the annual UN Conferences of the Parties. Public Understanding of\nfying frames using prompts. At the headline level, inferences made               Science 19, 6 (2010), 732\u2013742.         https://doi.org/10.1177/0963662509352044\nwith GPT-3.5 reached 49% of agreement with human-generated                       arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.\n                                                                            [19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame-\nframe labels. In many cases, the GPT-3.5 model inferred frame                    analysis of Dutch and French climate change coverage during the annual UN\ntypes that were considered as valid choices by human annotators,                 Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.\n\n\n\n\n                                                                      634\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                        ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig.             [46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with\n     2020. Gsum: A general framework for guided neural abstractive summarization.                 mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).\n     arXiv preprint arXiv:2010.08014 (2020).                                                 [47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent.\n[21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news                https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.\n     headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA            [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\n     Gesondheid (Online) 27 (2022), 1\u20138.                                                          et al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n[22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative                      1, 8 (2019), 9.\n     Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).                 [49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022.\n[23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.               Fake News Classification using transformer based enhanced LSTM and BERT.\n     McQuail\u2019s reader in mass communication theory 390 (1993), 397.                               International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105.\n[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language                    https://doi.org/10.1016/j.ijcce.2022.03.003\n     models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).                [50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational\n[25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across                    predictors. Cuadernos. info 50 (2021), 91\u2013112.\n     four nations: a topic modeling and sentiment analysis approach. Ieee Access 9           [51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\n     (2021), 36645\u201336656.                                                                         Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\n[26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En-                     Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language\n     coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior.                     model. arXiv preprint arXiv:2211.05100 (2022).\n     Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https:              [52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content\n     //doi.org/10.1037//0022-3514.66.2.398                                                        Analysis of Press and Television News. Journal of Communication 50 (06 2000),\n[27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based                 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x\n     transformer architectures for long document summarization. In Proceedings of            [53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em-\n     the 16th Conference of the European Chapter of the Association for Computational             manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin\n     Linguistics: Main Volume. 1792\u20131810.                                                         Van Durme. 2021. Constrained language models yield few-shot semantic parsers.\n[28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news                      arXiv preprint arXiv:2104.08768 (2021).\n     summarization using transformers. In Sustainable Advanced Computing. Springer,          [54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab-\n     249\u2013259.                                                                                     orative Work trends on Media Organizations: Mixing Qualitative and Quan-\n[29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar-                titative Approaches. Studies in Media and Communication 5 (04 2017), 63.\n     chy of hybridity in data and computational journalism. Digital Journalism 5, 2               https://doi.org/10.11114/smc.v5i1.2279\n     (2017), 159\u2013176.                                                                        [55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy\n[30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and                 considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n     text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021),     [56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\n     300\u2013324.                                                                                     the capabilities, limitations, and societal impact of large language models. arXiv\n[31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative                  preprint arXiv:2102.02503 (2021).\n     analysis of large amounts of journalistic texts using topic modelling. Digital          [57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.\n     journalism 4, 1 (2016), 89\u2013106.                                                              arXiv preprint arXiv:1806.02847 (2018).\n[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we              [58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and\n     know what language models know? Transactions of the Association for Computa-                 Felix Hill. 2021. Multimodal few-shot learning with frozen language models.\n     tional Linguistics 8 (2020), 423\u2013438.                                                        Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.\n[33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling                   [59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of\n     Political Framing Across Policy Issues and Contexts. In ALTA.                                technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.\n[34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate             [60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling\n     during election years: Focus on generic frames. The Communication Review 21, 2               for frame analysis: A study of media debates on climate change in India and USA.\n     (2018), 89\u2013115.                                                                              Global Media and Communication 18, 1 (2022), 91\u2013112.\n[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\n     hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n     2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n     (2022).\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\n     Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of\n     prompting methods in natural language processing. (2021). https://doi.org/10.\n     48550/ARXIV.2107.13586\n[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019.\n     Detecting frames in news headlines and its application to analyzing news framing\n     trends surrounding US gun violence. In Proceedings of the 23rd conference on\n     computational natural language learning (CoNLL).\n[38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames:\n     Toward Improving Reliability and Validity. Journal of Communication 58 (06\n     2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x\n[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and\n     David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3\n     User Simulation in Conversational AI. In Proceedings of the 4th Conference on\n     Conversational User Interfaces. 1\u20136.\n[40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social\n     computing for verifying social media content in breaking news. IEEE Internet\n     Computing 22, 2 (2018), 83\u201389.\n[41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021. \u2018Our task is to demystify fears\u2019:\n     Analysing newsroom management of automation in journalism. Journalism 22,\n     9 (2021), 2203\u20132221.\n[42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and\n     Annotation for Human-centered AI. Manning. https://books.google.ch/books?\n     id=LCh0zQEACAAJ\n[43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of\n     media frames: Strengths, weaknesses, and opportunities. Political Communication\n     38, 1-2 (2021), 159\u2013181.\n[44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to\n     news discourse. Political communication 10, 1 (1993), 55\u201375.\n[45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative\n     language models. arXiv preprint arXiv:1912.10165 (2019).\n\n\n\n\n                                                                                       635\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).", "Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.", "Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021)."], "publish_date": "2023-06-12", "approved": true, "id": "cfLva40B_jNEtOrdiJq_"}}, {"_index": "articles", "_id": "ybrRX40BW5-VEV64iSYh", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "AI Model for Computer games based on Case Based Reasoning and AI Planning hhhh", "abstract": "Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory. 12345", "authors": ["Vlado Menkovski", "Imed", "ayan"], "institutions": ["Athens Information Technology"], "keywords": ["Game AI", "Case Based Reasoning", "AI Planning", "Game Trees", "FIFA 2023"], "full_text": "                                                            Interactive and Adaptable Media                                                   295\n\n\n\n\n        AI Model for Computer games based on Case Based\n                     Reasoning and AI Planning\n                         Vlado Menkovski                                                          Dimitrios Metafas\n                 Athens Information Technology                                                Athens Information Technology\n                      0.8km Markopoulou Ave.                                                    0.8km Markopoulou Ave.\n                       Peania, 19002, Greece                                                     Peania, 19002, Greece\n                           vmen@ait.edu.gr                                                          dmeta@ait.edu.gr\n\n\nAbstract                                                                           With the advancement in computer hardware increasingly\n                                                                              more computing power is left for executing AI algorithms in\n      Making efficient AI models for games with imperfect                     games. In the past AI in games was mainly a cheating set of\ninformation can be a particular challenge. Considering the large              instructions that simulated the increasing difficulty in the game\nnumber of possible moves and the incorporated uncertainties                   environment so that the player had the illusion of real counterpart.\nbuilding game trees for these games becomes very difficult due to             Improvement in available memory and processing power allows\nthe exponential growth of the number of nodes at each level. This             implementation of more intelligent algorithms for building the\neffort is focused on presenting a method of combined Case Based               game environment as well as direct interaction with the human\nReasoning (CBR) with AI Planning which drastically reduces the                players.\nsize of game trees. Instead of looking at all possible combinations\n                                                                                    In this particular research the emphasis is put on the\nwe can focus only on the moves that lead us to specific strategies\n                                                                              interaction between the AI agent and a computer player in the\nin effect discarding meaningless moves. These strategies are\n                                                                              realm of the game rules. It is particularly focused on turn based\nselected by finding similarities to cases in the CBR database. The\n                                                                              games that have the elements of uncertainty like dice or concealed\nstrategies are formed by a set of desired goals. The AI planning is\n                                                                              information. At the beginning a description of Game AI\nresponsible for creating a plan to reach these goals. The plan is\n                                                                              algorithms are given; such as Game Trees and Minimax. The\nbasically a set of moves that brings the player to this goal. By\n                                                                              following section describes an approach of using AI Planning to\nfollowing these steps and not regarding the vast number of other\n                                                                              improve building Game Trees in games with imperfect\npossible moves the model develops Game Trees which grows\n                                                                              information where Game Trees tend to be very large with high\nslower so they can be built with more feature moves restricted by\n                                                                              growth ratio. Section 4 discusses another approach that provides a\nthe same amount of memory.\n                                                                              significant reduction to the number of considered moves in order\nCategories and Subject Descriptors                                            to find the favorable strategy of the AI player. This approach uses\n                                                                              AI Planning techniques and Case Base Reasoning (CBR) to plan\n                                                                              for different scenarios in predetermined strategies which would be\nI.2.1 [Applications and Expert Systems]: Games                                analogous to human player experience in the particular game. The\n                                                                              CBR database illustrates a set of past experiences for the AI\nGeneral Terms                                                                 problem and the AI Planning illustrates the procedure to deal with\nAlgorithms, Performance.\n                                                                              the given situation in the game. In the next two sections\nKeywords                                                                      implementations and evaluations of both approaches are given.\nGame AI, Case Based Reasoning, AI Planning, Game Trees                        The AI Planning approach is implemented with the Tic-tac-toe\n                                                                              game and the combined AI Planning and CBR approach is\n1. Introduction                                                               implemented with a model for the Monopoly game. The last part\n     The goal of this effort is to explore a model for design and             contains conclusions and future work ideas.\nimplementation of an AI agent for turn based games. This model\nprovides for building more capable computer opponents that rely\n                                                                              2. Game Trees and Minimax\non strategies that closely resemble human approach in solving                      Game Trees are common model for evaluating how different\nproblems opposed to classical computational centric heuristics in             combinations of moves from the player and his opponents will\ngame AI. In this manner the computational resources can be                    affect the future position of the player and eventually the end\nfocused on more sensible strategies for the game play.                        result of the game. An algorithm that decides on the next move by\n                                                                              evaluating the results from the built Game Tree is minimax [1].\n                                                                              Minimax assumes that the player at hand will always choose the\n Permission to make digital or hard copies of all or part of this work for    best possible move for him, in other words the player will try to\n personal or classroom use is granted without fee provided that copies are    select the move that maximizes the result of the evaluation\n not made or distributed for profit or commercial advantage and that          function over the game state. So basically the player at hand needs\n copies bear this notice and the full citation on the first page. To copy     to choose the best move overall while taking into account that the\n otherwise, or republish, to post on servers or to redistribute to lists,     next player(s) will try to do the same thing. Minimax tries to\n requires prior specific permission and/or a fee.                             maximize the minimum gain. Minimax can be applied to multiple\n DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.\n Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00\n\n\n\n                                3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f296                                                              DIMEA 2008\n\n\n\nlevels of nodes on the game tree, where the leaves bring the final        problem is the enormous number of possibilities that the player\nknown (or considered) game state.                                         can choose from in making his move. In addition some of the\nThe minimax theorem states:                                               moves are accompanied with probabilities based on the random\n                                                                          elements in the games. The number of possible moves\nFor every two-person, zero-sum game there is a mixed strategy             exponentially grows with each move so the depth of the search\nfor each player, such that the expected payoff for both is the same       has to be very limited to accommodate for the memory\nvalue V when the players use these strategies. Furthermore, V is          limitations.\nthe best payoff each can expect to receive from a play of the                  The basic idea behind using HTN for building game trees is\ngame; that is, these mixed strategies are the optimal strategies for      that the HTN provides the means of expressing high level goals\nthe two players.                                                          and describing strategies how to reach those goals. These goals\nThis theorem was established by John von Neumann, who is                  may be decomposed in goals at lower level called sub-goals. This\nquoted as saying \"As far as I can see, there could be no theory of        approach closely resembles the way a human player usually\ngames \u2026 without that theorem \u2026 I thought there was nothing                addresses a complex problem. It is also good for domains where\nworth publishing until the Minimax Theorem was proved\" [2].               classical search for solution is not feasible due to the vastness of\n                                                                          the problem domain or uncertainties.\n     A simple example of minimax can be observed by building a\ngame tree of the tic-tac-toe game. The tic-tac-toe game is a simple       3.2.1 Hierarchical Task Networks\ngame which can end by the first player wining, the second player\nwining or a tie. There are nine positions for each of the players in           The Hierarchical Task Network, or HTN, is an approach to\nwhich at each turn the player puts X or O sign. If the player has         automated planning in which the dependency among actions can\nthree adjacent signs in a row, column or the two diagonals he or          be given in the form of networks [9] [Figure 1].\nshe wins. This game has limited number of position and it is well              A simple task network (or just a task network for short) is an\nsuited for building the whole game tree. The leaves of this tree          acyclic digraph \u0002 \u0003 \u0004\u0005\u0006 \u0007\b in which U is the node set, E is the\nwill be final positions in the game. A heuristics evaluation              edge set, and each node       \u0005 contains a task . The edges of \u0002\nfunction will also need to be written to evaluate the value of each       define a partial ordering of U. If the partial ordering is total, then\nnode along the way.                                                       we say that \u0002 is totally ordered, in which case \u0002 can be written as\n                                                                          a sequence of tasks \u0002 \u0003 \u000e \u0006 \u000f \u0006 \u0010 \u0006 \u0011 \u0012.\n3. AI Planning for building Game Trees\n3.1.1 AI Planning                                                                                Buy milk\n      AI Planning also referred as Automated Planning and\nScheduling is a branch of Artificial Intelligence that focuses on\nfinding strategies or sequences of actions that reach a predefined\ngoal [3]. Typical execution of AI Planning algorithms is by                  Go to (shop)           Purchase             Go to (home)\nintelligent agents, autonomous robots and unmanned vehicles.\nOpposed to classical control or classification AI Planning results                  Figure 1: Simple Hierarchical Task Network\nwith complex solutions that are derived from multidimensional             A Simple Task Network (STN) method is a 4-tuple of its name,\nspace.                                                                    task, precondition and a task network. The name of the method\n      AI Planning algorithms are also common in the video game            lets us refer unambiguously to substitution instances of the\ndevelopment. They solve broad range of problems from path                 method, without having to write the preconditions and effects\nfinding to action planning. A typical planner takes three inputs: a       explicitly. The task tells what kind of task can be applied if the\ndescription of the initial state of the world, a description of the       preconditions are met. The preconditions specify the conditions\ndesired goal, and a set of possible actions. Some efforts for             that the current state needs to satisfy in order for the method to be\nincorporating planning techniques for building game trees have            applied. And the network defines the specific subtasks to\nalso shown up, similar to the approach explored in this effort. In        accomplish in order to accomplish the task.\naddition Cased Based Reasoning [4] techniques are also gathering          A method is relevant for a task if the current state satisfies the\npopularity in developing strategies based in prior knowledge              preconditions of a method that implements that task. This task can\nabout the problems in the games. One of the benefits from                 be then substituted with the instance of the method. The\nHierarchical Task Network (HTN) [5] planning is the possibility           substitution is basically giving the method network as a solution\nto build Game Trees based on HTN plans; this method is                    for the task.\ndescribed in the following section.                                       If there is a task \u201cGo home\u201d and the distance to home is 3km\n3.2 Game Trees with AI Planning                                           [Figure 2] and there exists a method walk-to and this method has a\n                                                                          precondition that the distance is less than 5km, then a substation\n     An adaptation of the HTN planning can be used to build               to the task \u201cGo home\u201d can be made with this method instance.\nmuch smaller and more efficient game trees. This idea has already\nbeen implemented in the Bridge Baron a computer program for                                               Go-to (from, to)\nthe game of Contact Bridge [6].\n     Computer programs based on Game Tree search techniques                  If (to \u2013 from) < 5km\nare now as good as or better than humans in many games like\nChess [7] and checkers [8], but there are some difficulties in                                                  Walk (to)\nbuilding a game tree for games that have imperfect information\nand added uncertainty like card or games with dice. The main                                    Figure 2: HTN Method\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                                 Interactive and Adaptable Media                                                     297\n\n\n\nIf the distance is larger than 5km another methhod instance needs                                                        present moves generated by\n                                                                                   plan. The branches of the game tree rep\nto be substituted [Figure 3].                                                      the methods. Tignum2 applies all metthods applicable to a given\n                                                                                   state of the world to produce new    w states of the world and\n                              Go-to (from, to)                                                                           n applicable methods that\n                                                                                   continues recursively until there are no\n                                                                                   have not already been applied to th  he appropriate state of the\n   If(to \u2013 from) < 5km                                 If(tto \u2013 from) < 200km\n                                                                                   world.\n                                                                                   In the task network generated by Tignuum2, the order in which the\n                          Walk (to)               Drive(to)                        actions will occur is determined by the total-ordering constraints.\n                                                                                   By listing the actions in the order they will occur, the task\n                         Figure 3: HTN Method 2                                                                         me tree [Figure 4] [Figure 5].\n                                                                                   network can be \u201cserialized\u201d into a gam\n   An STN planning domain is a set of operatioons O and a set of\nmethods M. A STN planning problem is a 4-tuuple of the initial\nstate S0, the task network w called initial taskk network and the\nSTN domain. A plan \u0013 \u0003 \u0014\u000e \u0006 \u0010 \u0006 \u0014\u0015 \u0012 is a solutiion for a planning\nproblem if there is a way to decompose w into \u03c0 if \u03c0 is executable\nand each decomposition is applicable in the apppropriate state of\nthe world. The algorithm that is capable to decompose these\nnetworks into plans is called Total-forward-decoomposition (TFD)\n[9] or Partial-forward-decomposition (PFD). H  However there are\ncases where one does not want to use a forwaard-decomposition\nprocedure. HTN planning is generalization of S  STN planning that\ngives the planning procedure more freedom       m about how to\nconstruct the task networks.\n     In order to provide this freedom, a bookkeeeping mechanism                               Figure 4: HTN to Game Trree Algorithm\nis needed to represent constraints that the plannning algorithm has\nnot yet enforced. The bookkeeping is done byy representing the\nunenforced constraints explicitly in the task netw\n                                                 work.\n     The HTN generalizes the definition of a task network in\nSTN. A task network is the pair \u0002 \u0003 \u0004\u0005\u0006 \u0017\b w    where \u0005 is a set of\ntask nodes and C is a set of constraints. Each constraint in C\nspecifies a requirement that must be satisfied byy every plan that is\na solution to a planning problem.\n     The definition of a method in HTN alsoo generalizes the\ndefinition used in STN planning. A HTN plaan is a 4-tuple of\nname, task, subtasks, and constraints. The ssubtasks and the\nconstraints form the task network. The HTN plannning domains are\nidentical to STN planning domains except they uuse HTN methods\ninstead of STN methods.                                                                     Figure 5: Game Tree built frrom HTN\n\n     Compared to classical planners the prim  mary advantage of                    4. Case Based Reasoning in\n                                                                                                            n Game Strategies\nHTN planners is their sophisticated knowledge rrepresentation and\nreasoning capabilities. They can represent and solve a variety of\n                                                                                   4.1 Case Based Reasoning\n                                                                                        Case-based reasoning (CBR) is a well established subfield of\nnon-classical planning problems; with a goodd set of HTNs to\n                                                                                   Artificial Intelligence (AI), both as a mean for addressing AI\nguide them, they can solve classical planning pproblems orders of\n                                                                                   problems and as a basis for standalone AI technology.\nmagnitude more quickly than classical or neocclassical planners.\nThe primary disadvantage of HTN is the neeed of the domain                         Case-based reasoning is a paradigm    m for combining problem-\nauthor to write not only a set of planning operaators but also a set               solving and learning that has became one of the most successful\nof methods.                                                                        applied subfield of AI of recent yeaars. CBR is based on the\n                                                                                                                          I means that new problems\n                                                                                   intuition that problems tend to recur. It\n3.2.2 HTN Planning in building Game Trees\n                                                                                   are often similar to previously en     ncountered problems and,\n     For a HTN planning algorithm to be adapted to build game                      therefore, that past solutions may be off use in the current situation\ntrees we need to define the domain (set of H  HTN methods and                      [10].\noperators) which is the domain of the game. Thiis is in some sense\n                                                                                   CBR is particularly applicable to probllems where earlier cases are\na knowledge representation of the rules of thee game, the game\n                                                                                   available, even when the domain is not  n understood well enough\nenvironments and possible strategies of game plaay.\n                                                                                   for a deep domain model. Helpdesks,, diagnosis or classification\n     In this domain the game rules as well as knnown strategies to                 systems have been the most successfu    ul areas of application, e.g.,\ntackle specific task are defined. The implem  mentation of Game                    to determine a fault or diagnostic an illness from observed\nTree building with HTN is called Tignnum2 [9]. This                                attributes, or to determine whether orr not a certain treatment or\nimplementation uses a procedure similaar to forward-                               repair is necessary given a set of past solved cases [11].\ndecomposition, but adapted to build up a game tree rather than a\n\n\n\n                                      3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f298                                                             DIMEA 2008\n\n\n\nCentral tasks that all CBR methods have to deal with are [12]: \"to       And in the third position if the two of center, middle top and\nidentify the current problem situation, find a past case similar to      middle left are available the position is a certain victory.\nthe new one, use that case to suggest a solution to the current\nproblem, evaluate the proposed solution, and update the system by             There are many different arrangements of the player\u2019s tokens\nlearning from this experience. How this is done, what part of the        that give equivalent positions as these three positions. By using\nprocess that is focused, what type of problems that drives the           planning we do not need to consider all possible layouts but just\nmethods, etc. varies considerably, however\".                             consider these three similar to what a human would consider.\n\n     While the underlying ideas of CBR can be applied                          The game starts from an empty table.\nconsistently across application domains, the specific                          The two relevant strategies that would lead to these positions\nimplementation of the CBR methods \u2013in particular retrieval and           are to take one corner or to take the center [Figure 7].\nsimilarity functions\u2013 is highly customized to the application at\nhand.\n4.2 CBR and Games\n     Many different implementations of CBR exist in games.\nCBR technology is nicely suited for recognizing complex\nsituations much easier and more elegant than traditional parameter                   Figure 7: Tic-tac-toe Two starting moves\ncomparison or function evaluation. There are especially evident\ncases in real time strategies where different attack and defense of            The center position as we can see in the simulation results\nglobal strategies are nicely defined by CBR datasets and later used      lead to a bigger number of victorious endings but it is also a\nin the running games. Also intelligent bots behavior is also             straight forward strategy with obvious defense strategy.\nanother typical example. Depending on the number of enemy bots                At this point we need to consider the moves of the opponent.\nthe layout of the terrain and position of human players the CBR          If we take the left branch the opponent moves can be a center, a\nsystem finds the closest CBR case and employs that strategy              corner or a middle field. We also need to differentiate with a\nagainst the human players which in prior evaluation was proved to        move to a corner adjacent with our like top left or bottom right or\nbe highly efficient.                                                     across the center to bottom right [Figure 8].\n5. Game Trees with AI Planning \u2013 Tic-tac-toe\n      In order to show the expressive power of AI Planning in\ndefining strategies for games, and the use of these plans to build\nGame Trees I implemented an algorithm that builds Game Trees\nfor the Tic-Tac-Toe game.\n                                                                             Figure 8: Tic-tac-toe opponent response to corner move\n     The game tree of Tic-Tac-Toe shows 255,168 possible\n                                                                               In cases one and two, we have a clear path to executing\ngames of which 131,184 are won by X (the first player), 77904\n                                                                         strategy 3 so we need to capture the diagonally opposite field.\nare won by O and the rest 46,080 are draw [13]. All these games\n                                                                         And as for the third case the best way to go is to capture the center\ncan be derived from building a complete Game Tree.\n                                                                         and go for strategy 1 or 2 depending of the opponent\u2019s next move.\n     Even though it is possible to build a complete game tree of\nTic-tac-toe it is definitely not an optimal solution. Many of the\nmoves in this tree would be symmetrical and also there are a many\nmoves that would be illogical or at least a bad strategy to even\nconsider.\n     So what strategy should X (the first player) choose in order                Figure 9: Tic-tac-toe move 2 after corner opening\nto win the game?                                                              The first move leads to certain victory, O will have to go to\n                                                                         the center and X will achieve strategy 3 [Figure 9]. The second\n     There are few positions that lead to certain victory. These\n                                                                         move is a possible way to strategy 3 if O makes a mistake in the\npositions involve simultaneous attack on two positions so the\n                                                                         next loop, so X goes to the opposite corner. For the third case\nother player could not defend, basically the only trick in Tic-Tac-\n                                                                         since O is playing a valid strategy the only move that leaves a\nToe.\n                                                                         possible mistake from O would be to take the center and wait for\n                                                                         O to go to the middle and then achieve strategy 1 or 3 which will\n                                                                         be a symmetric situation to the one that we will find if we\n                                                                         branched with the center.\n\n\n        Figure 6: Tic-tac-toe winning strategy positions\n     Position 1 leads to victory if the two of the three fields: top\nmiddle, bottom left corner and bottom right corner are free\n[Figure 6].\n    Position 2 lead to victory if two of the three fields: top right         Figure 10: Tic-tac-toe opponent response to center move\ncorner, bottom right corner and bottom middle are free [Figure ].            If we go back to the second branch [Figure 10], a possible\n                                                                         way for the second player to engage is corner or middle. The first\n\n\n\n                             3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                         Interactive and Adaptable Media                                                      299\n\n\n\nmove is a valid strategy for O and can be meeet with a opposite                  This HTN when executed will reesult with plans for possible\ncorner move from X to try a mistake from O in the future exactly           game scenarios. By creating nodes from m each position and linking\nthe same as in the third case above from the preevious branch, and         them with branches with the move of the\n                                                                                                                 t player we create a game\nanother move would be go to the middle whhere X eventually                 tree for the Tic-tac-toe game over whiich we can run the minimax\nachieves strategy 1 or 2.                                                  algorithm.\n                                                                                 This set up with 7 plans with 3 taarget strategies creates a tree\n                                                                           for Tic-tac-toe which considers all posssible moves for the second\n                                                                           player with only 457 games, 281 of whichw       X wins 176 are draw\n                                                                           and 0 where the second opponent wins.    w      This is a significant\n       Figure 11: Tic-tac-toe Move 2 after centter opening                 reduction over the 255, 168 possible games with a complete game\n     The fist move will lead to win if O moves to the middle or a          tree. These reductions can be very useeful for devices with limited\ndraw if it goes for the corners [Figure 11]. In tthe second case O         computing capabilities but also we prrove a very important point\nhas to block the lower left corner which leavees X to go for the                                                    d\n                                                                           that planning can be very efficient if designing    meaningful game\nmiddle left or corner left which are strategy 1 andd 2.                    trees by applying reasoning very similar to human player\n     To sum the strategies for the planning, first we have center or       reasoning.\ncorner strategy for the beginning. Then for the ceenter we try to get            Further improvements to the gam   me tree are also possible if\nthe corners with the particularly the one oppoosite to the one O           the opponents moves are also planned    d, in other words if we drop\nholds. If the center is empty for the second strateegy we go for it or                                           m\n                                                                           all the meaningless and symmetrical moves      of the opponent.\nwe go for the opposite corner. After this point wwe either block the       6. Game AI in Monopoly\nopponent or try to implement strategies 1, 2 orr 3 which lead to\nvictory.\n                                                                           6.1 Overview of the AI Imp\n                                                                                                    plementation\nPlan 1: Take center                                                              The AI agent is responsible for the moves of the artificial\nPreconditions: Center empty                                                players in the game. The core principlee of the AI agent is building\nPlan 2: Take corner                                                        a Game Tree with all the sensible movees that all the players would\nPreconditions: All corners empty                                           make from the current point of timee forward. Then using the\nPlan 3: Take corner after center                                           minimax algorithm the agent selects the  t move that in the future\nPreconditions: We have center take corner oppossite to the one the         would bring the computer player mo     ost favorable game position\nopponent has                                                               with the highest probability. Building a Game Tree in this game\nPlan 4: Take diagonal corner                                               that would be big enough to consider sufficient number of moves\nPreconditions: We have a corner, the opponent ha    as the ce\u2212nter and\n                                                                           is obstructed by the vastness of posssible moves in combination\n the corner opposite to the one we have is free.\nPlan 5: Block                                                              with all the possible random landings of the dice. The number of\nPrecondition: The opponent has tree tokens in a rrow, colu\u2212mn or di        nodes of the game tree exponentially     y grows at each level. To\nagonal                                                                     tackle this problem the AI agents incorporates two already\nPlan 6: Win                                                                discussed technologies: Case Based Reeasoning and AI Planning.\nPreconditions: We have two tokens in a row, column or dia\u2212gonal a                The technologies are employed in the following manner.\nnd the third place is free                                                 First the agent searches the CBR datab base to find the case with the\nPlan 7: Tie                                                                largest similarity with the current statee of the board. This case is\nPreconditions: If all places are taken, it\u2019s a tie.                        associated with a playing strategy. Th  he strategy consists of goal\n5.1 Hierarchical Task Network                                                                                      f and the plans consist of\n                                                                           that the planner needs to build plans for,\n     Top level task is Play [Figure 12]. This is a complex task and        consecutive player moves that bring th   he player to that goal. This\ncan be derived into: Win, Block, Tie or Searrch for Plan. The              way only moves that are part of that strrategy are considered, those\nSearch for plan is derived to both Plan 1 and Plaan 2 or Plan 3 and        being a small fraction of the overall poossible moves the number of\nPlan 4, which later leads to a call for the opponnent\u2019s move and a         edges of the game tree at each level deccreases immensely.\nrecursive call to Play.                                                          At each level of the game tree thee model considers the moves\n                                                                           of a single player. After the strateg    gies of the AI player are\n                                                                           considered the response to those strateegies needs to be considered\n                                                                           by the opponent(s). The move of the opponent(s) depends of the\n                                                                           probability distribution of the dice as well as the strategy of the\n                                                                           player. A more general strategy needss to be implemented for the\n                                                                           opponent\u2019s (human player) moves sin     nce we cannot be aware of\n                                                                           the expertise of the opponent. This geeneral strategy would bring\n                                                                           more plausible moves than the focused  d strategy of the AI player.\n                                                                                 After covering all opponents the   t    agent comes back to\n                                                                           deducting a feature move of the com     mputer player by using the\n                                                                           CBR selected plan strategy. After creating several loops of\n                                                                           strategies and reaching a reasonable size\n                                                                                                                   s    of a Game Tree taking\n                                                                           into account the memory limits an        nd the rapidly decreasing\n                                                                           probabilities that the move is possiblee due to the distribution of\n                                                                           the dice the building of the Game Treee stops. Then the minimax\n                                                                           algorithm searches the Game Tree and decides on the most\n                                                                           favorable move for the AI player usiing the minimax algorithm.\n                      Figure 12: Tic-tac-toe HT\n                                              TN                           The process is repeated each time the AIA player is up.\n\n\n\n                               3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f300                                                             DIMEA 2008\n\n\n\n      Buying, auctioning and trading game moves are always                    On the other hand the MonopolySolution class holds the\naccompanied by return of investment calculations in making the           three particular attributes that are needed for the planning, the\nplans. These calculations represent adaptation of the more general       planning Domain, State and TaskList.\nplanning associated with the cases in the CBR database. These\nadaptations are necessary due to the fact that the cases do not               The game is implemented by using the Model-View-\nidentically correspond to the situation on the table. In addition        Controller software development pattern. The controller is\ncalculating the game position value of each node of the game tree        responsible for implementing the game rules and handling all of\nis done by heuristic functions that incorporate economic                 the events in the game like roll of dice, input commands for\ncalculations of net present value, cash, and strategic layout and so     trading, auctioning and etc from the players. The View layer is\non. For example railroads in monopoly are known to be                    responsible for displaying the board and all of the input widgets\nstrategically effective because they bring constant income even          on to the game screen, and the models are data structures\nthough the income can be smaller than building on other                  representing the game state [Figure 14].\nproperties.\n6.2 Details on the CBR Implementation\n     The implementation of the CBR is by using the JColibri2\nplatform. JColibri2 is an object-oriented framework in Java for\nbuilding CBR systems that is an evolution of previous work on\nknowledge intensive CBR [14].\n     For this implementation we need to look into three particular\nclasses of the JColibri2 platform. The StandardCBRApplication,\nConnector, CBRQuery. For a JColibri2 implementation the\nStandardCBRApplication interface needs to be implemented.\n     The CBR cycle executed accepts an instance of CBRQuery.\nThis class represents a CBR query to the CBR database. The\ndescription component (instance of CaseComponent) represents\nthe description of the case that will be looked up in the database.\nAll cases and case solutions are implementing the\nCaseComponent interface.\n     The JColibri2 platform connects to the CBR database via a\nConnector class. Each connector implements all the necessary\nmethods for accessing the database, retrieval of cases, storing and\ndeletion of cases. This implementation uses a custom XML\nstructure for holding the CBR cases. Since the game will not\nupdate the CBR database only read it, a XML solution satisfies\nthe needs. The XML file to a certain extent is similar to the XML\nrepresentation of the board. We are interested in finding one\nCBRCase that is the most similar case to the situation in the game\nat the time of the search. This procedure is done in the cycle\nmethod of the CBRApplication. The JColibri2 CBR comparison is\ndone by Nearest Neighbor (NN) search method.\n     JColibri2 offers implementations for NN search algorithms\nof simple attributes. These implementations are called local\nsimilarities. For complex attributes like in our case global                    Figure 14: Class diagram of the Monopoly models\ncustomized similarity mechanisms need to be implemented.\n     The MonopolyDescription class [Figure 13] is basically a\n                                                                         6.2.1 Complex Similarity representation in CBR\nserialization of the GameState. It holds all the information about             The similarity measurement part of the Nearest Neighbor\n                                                                         algorithm JColibri2 is implemented by implementing the\nthe state of the board, the players, their amount of cash etc.\n                                                                         LocalSimiralrityFunction and the GlobalSimiralityFunction\n                                                                         interface. A local similarity function is applied to simple attributes\n                                                                         by the NN algorithm, and a global similarity function is applied to\n                                                                         compound attributes. In the case of our implementation the\n                                                                         attributes of the MonopolyDescription are compound attributes\n                                                                         describing the state of the board, number of players, amount of\n                                                                         cash for every player and etc. Since MonopolyDescription is a\n                                                                         custom CaseComponent a global similarity function needs to be\n                                                                         implemented to accurately find the distance between different\n                                                                         CBR cases.\n                                                                             The similarity mechanism is inseparable core element of the\n                                                                         CBR system. This mechanism represents how the CBR decides\n                                                                         which strategy is best suited for the particular situation by\n Figure 13: Class diagram of the Monopoly Case component\n                          models\n\n\n\n                             3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                       Interactive and Adaptable Media                                                    301\n\n\n\ncalculating the distance or similarity to other cases in the                   JSHOP2 uses ordered task decomposition in reducing the\ndatabase.                                                                 HTN to list of primitive tasks which form the plans. An ordered\n                                                                          task decomposition planner is an HTN planner that plans for tasks\n     For the monopoly implementation we need to consider                  in the same order that they will be executed. This reduces the\nseveral basic strategies. Monopoly is based on investing in               complexity of reasoning by removing a great deal of uncertainty\nproperties and receiving revenues from those investments. One of          about the world, which makes it easy to incorporate substantial\nthe basic strategies of the game is to build a set of properties that     expressive power into the planning algorithm. In addition to the\nwill bring constant income larger than the one of the opponents.          usual HTN methods and operators, the planners can make use of\nSo in time the opponents will have to declare bankruptcy. But on          axioms, can do mixed symbolic/numeric conditions, and can do\nthe other hand over investment can lead to too stretched resources        external function calls.\nwith low income that will eventually drove the player to\nbankruptcy. To decide on these two we need a clear separation                   In order for the JSHOP2 planer to generate plans it needs\ninto two groups of cases in the CBR database. The first group of          tree crucial components: Domain, State and Tasks. The Domain\ncases will represent a situation on the board where the player has        defines all the functionalities that the particular domain offers.\nsignificant income per loop formed of one or more color group             These are simple and complex tasks. The complex tasks also\nproperties, maybe railroads, some buildings on them and so on. It         called methods create the hierarchy with the fact that they can be\nis important to note that in this case the player is better situated      evaluated by simple tasks of other complex tasks. This is how a\nthan his opponents so he only needs to survive long enough to win         hierarchical structure of tasks is formed. The problem reduction is\nthe game. In the other group of cases either the opponent is not          done by reducing the high level complex tasks to simpler until all\nwell positioned on the board or its opponents are better situated.        the tasks are primitive. The list of primitive tasks forms the plan.\nIn this case further investments are necessary to improve the\nsituation so the player can have a chance of winning in the long                The State represents the state of the system. It is a simple\nrun.                                                                      database of facts that represent the state of the system. The State\n                                                                          is necessary to determine the way the problems or tasks are\n      These metrics can be owning color groups, valuing groups of         reduced to their primitive level. The reduction is done by\nrailroads, evaluating the other opponents as well, and considering        satisfying different prerequisites set in the methods; these\nthe amount of cash. As it is obvious in monopoly the number of            prerequisites are defined in the state. The Tasks are high level\nstreets is not as nearly as important as the combination of streets       tasks or methods defined in the Domain. The planner based on the\nthe player owns. It is also important to note that one CBR case           State and the goals selects one or more high level tasks that need\ndoes not hold only a single strategy in place, but its solution can       to be reduced to plans [Figure 15].\nhave multiple different strategic goals. For example one CBR case\nmight simultaneously say buy this land to form a color group but                       Tasks\nalso trade some other unimportant property to increase cash\n                                                                                                                    Core Planner\namount.\n     The cases do not represent all possible combinations of board\npositions. They are only representation of typical game scenarios.                     State                        Plan\nThe CBR Case solutions do not give exact instructions in general\nbut rather strategic goals. For example one CBR Solution might\n                                                                                           Figure 15: Diagram of a Planner\nsay trade the streets that you only have one of each for the ones\nthat you have two of that color already. Then the planner based on            The plans then generate the game moves. The number of\nthe situation on the board needs to decompose this high level task        moves generated by the plans is just a fraction of the possible\nto a low level operations. Like offer \"Mediterranean Avenue\" for          moves at that point. This reduces the game tree providing the\n\"Reading Railroad\" and offer $50. The exact amounts and actual            opportunity to generate smaller and deeper game trees and making\nstreets are left to the planer to evaluate.                               more efficient decisions in general.\n\n      The monopoly CBR database is currently in development on            7. Conclusion\na monopoly clone game called Spaceopoly. The cases are                          Even though the results from the CBR database are not\narchitected based on human player experience and knowledge.               complete at this time partial strategies are implemented as cases\nThere is a plan of making a number of slightly different strategies       and recognized during game play by the CBR system. These\nthat differ on the style of playing and then running simulation           smaller local strategies coupled with more global higher level\ntests that would determine the particular validity of each database       strategies that are particularly important at the beginning of the\nas well as validity of certain segments of the strategy or even           game would form a complete CBR database and represent a\nparticular cases in the database.                                         knowledge engineered style of playing of the AI player.\n      The actual execution of the strategies will not differ from              The AI Planning approach is a proven method by the tic-tac-\nstrategy to strategy since the plan execution is more related to the      toe experiment and is suitable for implementing the strategies\nstructure and rules of the game than to the actual playing strategy.      associated with the CBR cases.\n6.3 Details on the Planning Implementation                                     This approach in general benefits from both technologies,\n     For the purpose of planning this implementation uses a               CBR as well as AI Planning and comprises an elegant solution.\nmodification of the JSHOP2 planner. The Java Simple                       Even though AI Planning can be enough as a single technology\nHierarchical Ordered Planner 2 is a domain independent HTN                for some simpler problems like tic-tac-toe the complexity of\nplanning system [15].                                                     Monopoly would mean that the Planner would have to incorporate\n\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f302                                                              DIMEA 2008\n\n\n\nlarge and complex domain and a very big state model. The CBR              reasoning. People in addition to logic of reasoning in situations\napplication helps reduce this complexity by focusing the planning         with lack of information rely to planning strategies and prior\non smaller domain of the game. Basically the CBR reduces the              experience, exactly the intuition behind CBR \u2013 AI Planning\noverall goal of the play (wining the game) to smaller more                architecture.\nconcrete goals suitable to the particular state of the game, thus\nreducing the need for global planning strategies and complex              9. ACKNOWLEDGMENTS\nplanning domain.                                                               We would like to thank Prof. Sofia Tsekeridou for her\n                                                                          involvement in the valuable discussions we had on the topic of\n     Furthermore this symbiosis of technologies gives way for             CBR.\nmore precise and finely tuned strategies which can be difficult to\ninclude into global plan for the whole game. One simple example           10. REFERENCES\nfor the Monopoly game would be this: Sometimes it\u2019s better to             [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]\nstay in jail because rolling double increases the probability of              http://en.wikipedia.org/wiki/Minimax.\nlanding on some field (two, four, six, eight, ten or twelve steps\nfrom the jail) that can be of great importance to the rest of the         [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.\ngame. These and similar small local strategies can be easily                  Annalen. 100 (1928) 295-320\nrecognized by similar cases in the CBR database.                          [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,\n                                                                              2008.] http://en.wikipedia.org/wiki/Automated_planning.\n      In other words the system is flexible enough so that new\nstrategies can be incorporated easily missing strategies can be also      [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based\nrecognized by the distance metrics as well as wrong assumptions               Strategy Game with Plan Adaptation and Ontology-based\nin the strategies can be easily recognized.                                   retrieval.\n\n     One other important property of the system is that is highly         [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for\nconfigurable. The game its self can be diversely different                    hierarchical task-network planning. Technical Report TR-94-\ndepending on the configuration of the board. Even though the                  31, UMIACS.\nplatform is restricted to Monopoly type of games, changing the            [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning\nlayout and values of the fields effectively brings completely                 approach decrarer play in contract bridge. Computational\ndifferent properties of the game. In addition the CBR database                Intelligence. 1996, Vol. 12, 1.\nrepresents the entire experience of the AI Player. It can be filled       [7] One Jump Ahead: Challenging Human Supremacy in\nwith rich set of strategies or even configured with different flavors         Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.\nof difficulties of play, this of course coupled with the domain of\nthe planner which can differ from a case to a case as well.               [8] IBM. How Deep Blue works. [Online] 1997. [Cited: April\n                                                                              23, 2008.]\n8. Future Work                                                                http://www.research.ibm.com/deepblue/meet/html/d.3.2.html\n      Further exploration of this technology would go towards             [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.\ncomplete implementation of an AI aware agent for monopoly.                    Automated Planning theory and practice. s.l. : Morgan\nInitial results from the local cases with more specific strategies            Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.\nshow CBR as a capable tool for representing expertise in playing\nthe game. Completing the more general strategies and coupling             [10] Case Based Reasoning. Experiences, Lessons and Future.\nthem with the planning domain will give precise results on the                 Leake, David. s.l. : AAAI Press. MIT Press., 1997.\nbenefits from this architecture.                                          [11] Applying case-based reasoning: techniques for enterprise\n                                                                               systems. Watson, I. San Francisco, CA, USA : Morgan\n     There is also need for exploring the planning of strategies of\n                                                                               Kaufmann Publishers Inc., 1998.\nopponents. This task is to some extent different because we\ncannot always expect the opponent to select the best move we              [12] Plaza, A. Aamodt and E. Case-based reasoning:\nthink. In the Tic-tac-toe example all possible moves of the                    Foundational issues, methodological. AI Communications.\nopponent were taken into consideration, if we used the same                    1994, 7(i).\nplanner for the opponent only tie games would result from the             [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]\ngame tree. In other words mistakes of the players also need to be              http://en.wikipedia.org/wiki/Tic-tac-toe.\nconsidered.\n                                                                          [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An\n      The CBR Platform brings other functionalities well worth of              architecture for knowledge intensive CBR systems. Advances\nexploring as well. The revision stage of the JColibri2 platform is             in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :\nbasically capable of fine tuning strategies or even developing new             Springer-Verlag, Berlin Heidelberg, 2000.\nstrategies for the games. A well written underlying AI planning           [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to\nmodel with a capable feedback of the game tree evaluation back                 Synthesize Problem-Specific Planners. 2003.\nto the CBR revision capability can be an interesting concept in\nautomatic experience acquisition for the AI model.\n     There are also many other fields were combined CBR and\nplanning approach can be incorporated into a problem solution.\nThis combination is analogous in a big extent to a human way of\n\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["Von Neumann, J : Zur theorie der gesellschaftsspiele  Math. \nAnnalen. 100 (1928) 295-320", "Automated Planning. Wikipedia. [Online] [Cited: April 23, \n2008.] http://en.wikipedia.org/wiki/Automated_planning .", "Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based \nStrategy Game with Plan Adaptation and Ontology-based \nretrieval .", "joao felix"], "publish_date": "2023-09-11", "approved": true, "id": "ybrRX40BW5-VEV64iSYh", "refrences": ["Von Neumann, J : Zur theorie der gesellschaftsspiele  Math. \nAnnalen. 100 (1928) 295-320", "Automated Planning. Wikipedia. [Online] [Cited: April 23, \n2008.] http://en.wikipedia.org/wiki/Automated_planning .", "Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based \nStrategy Game with Plan Adaptation and Ontology-based \nretrieval .", "zdazd"]}}, {"_index": "articles", "_id": "X6VqZI0B6PUZ6XlnHs6b", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "AI Model for Computer games based on Case Based Reasoning and AI Planning", "abstract": "Making efficient AI models for games with imperfect information can be a particular challenge. Considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. This effort is focused on presenting a method of combined Case Based Reasoning (CBR) with AI Planning which drastically reduces the size of game trees. Instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. These strategies are selected by finding similarities to cases in the CBR database. The strategies are formed by a set of desired goals. The AI planning is responsible for creating a plan to reach these goals. The plan is basically a set of moves that brings the player to this goal. By following these steps and not regarding the vast number of other possible moves the model develops Game Trees which grows slower so they can be built with more feature moves restricted by the same amount of memory.", "authors": ["Vlado Menkovski"], "institutions": ["Athens Information Technology"], "keywords": ["Game AI", "Case Based Reasoning", "AI Planning", "Game Trees"], "full_text": "                                                            Interactive and Adaptable Media                                                   295\n\n\n\n\n        AI Model for Computer games based on Case Based\n                     Reasoning and AI Planning\n                         Vlado Menkovski                                                          Dimitrios Metafas\n                 Athens Information Technology                                                Athens Information Technology\n                      0.8km Markopoulou Ave.                                                    0.8km Markopoulou Ave.\n                       Peania, 19002, Greece                                                     Peania, 19002, Greece\n                           vmen@ait.edu.gr                                                          dmeta@ait.edu.gr\n\n\nAbstract                                                                           With the advancement in computer hardware increasingly\n                                                                              more computing power is left for executing AI algorithms in\n      Making efficient AI models for games with imperfect                     games. In the past AI in games was mainly a cheating set of\ninformation can be a particular challenge. Considering the large              instructions that simulated the increasing difficulty in the game\nnumber of possible moves and the incorporated uncertainties                   environment so that the player had the illusion of real counterpart.\nbuilding game trees for these games becomes very difficult due to             Improvement in available memory and processing power allows\nthe exponential growth of the number of nodes at each level. This             implementation of more intelligent algorithms for building the\neffort is focused on presenting a method of combined Case Based               game environment as well as direct interaction with the human\nReasoning (CBR) with AI Planning which drastically reduces the                players.\nsize of game trees. Instead of looking at all possible combinations\n                                                                                    In this particular research the emphasis is put on the\nwe can focus only on the moves that lead us to specific strategies\n                                                                              interaction between the AI agent and a computer player in the\nin effect discarding meaningless moves. These strategies are\n                                                                              realm of the game rules. It is particularly focused on turn based\nselected by finding similarities to cases in the CBR database. The\n                                                                              games that have the elements of uncertainty like dice or concealed\nstrategies are formed by a set of desired goals. The AI planning is\n                                                                              information. At the beginning a description of Game AI\nresponsible for creating a plan to reach these goals. The plan is\n                                                                              algorithms are given; such as Game Trees and Minimax. The\nbasically a set of moves that brings the player to this goal. By\n                                                                              following section describes an approach of using AI Planning to\nfollowing these steps and not regarding the vast number of other\n                                                                              improve building Game Trees in games with imperfect\npossible moves the model develops Game Trees which grows\n                                                                              information where Game Trees tend to be very large with high\nslower so they can be built with more feature moves restricted by\n                                                                              growth ratio. Section 4 discusses another approach that provides a\nthe same amount of memory.\n                                                                              significant reduction to the number of considered moves in order\nCategories and Subject Descriptors                                            to find the favorable strategy of the AI player. This approach uses\n                                                                              AI Planning techniques and Case Base Reasoning (CBR) to plan\n                                                                              for different scenarios in predetermined strategies which would be\nI.2.1 [Applications and Expert Systems]: Games                                analogous to human player experience in the particular game. The\n                                                                              CBR database illustrates a set of past experiences for the AI\nGeneral Terms                                                                 problem and the AI Planning illustrates the procedure to deal with\nAlgorithms, Performance.\n                                                                              the given situation in the game. In the next two sections\nKeywords                                                                      implementations and evaluations of both approaches are given.\nGame AI, Case Based Reasoning, AI Planning, Game Trees                        The AI Planning approach is implemented with the Tic-tac-toe\n                                                                              game and the combined AI Planning and CBR approach is\n1. Introduction                                                               implemented with a model for the Monopoly game. The last part\n     The goal of this effort is to explore a model for design and             contains conclusions and future work ideas.\nimplementation of an AI agent for turn based games. This model\nprovides for building more capable computer opponents that rely\n                                                                              2. Game Trees and Minimax\non strategies that closely resemble human approach in solving                      Game Trees are common model for evaluating how different\nproblems opposed to classical computational centric heuristics in             combinations of moves from the player and his opponents will\ngame AI. In this manner the computational resources can be                    affect the future position of the player and eventually the end\nfocused on more sensible strategies for the game play.                        result of the game. An algorithm that decides on the next move by\n                                                                              evaluating the results from the built Game Tree is minimax [1].\n                                                                              Minimax assumes that the player at hand will always choose the\n Permission to make digital or hard copies of all or part of this work for    best possible move for him, in other words the player will try to\n personal or classroom use is granted without fee provided that copies are    select the move that maximizes the result of the evaluation\n not made or distributed for profit or commercial advantage and that          function over the game state. So basically the player at hand needs\n copies bear this notice and the full citation on the first page. To copy     to choose the best move overall while taking into account that the\n otherwise, or republish, to post on servers or to redistribute to lists,     next player(s) will try to do the same thing. Minimax tries to\n requires prior specific permission and/or a fee.                             maximize the minimum gain. Minimax can be applied to multiple\n DIMEA\u201908, September 10\u201312, 2008, Athens, Greece.\n Copyright 2008 ACM 978-1-60558-248-1/08/09... $5.00\n\n\n\n                                3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f296                                                              DIMEA 2008\n\n\n\nlevels of nodes on the game tree, where the leaves bring the final        problem is the enormous number of possibilities that the player\nknown (or considered) game state.                                         can choose from in making his move. In addition some of the\nThe minimax theorem states:                                               moves are accompanied with probabilities based on the random\n                                                                          elements in the games. The number of possible moves\nFor every two-person, zero-sum game there is a mixed strategy             exponentially grows with each move so the depth of the search\nfor each player, such that the expected payoff for both is the same       has to be very limited to accommodate for the memory\nvalue V when the players use these strategies. Furthermore, V is          limitations.\nthe best payoff each can expect to receive from a play of the                  The basic idea behind using HTN for building game trees is\ngame; that is, these mixed strategies are the optimal strategies for      that the HTN provides the means of expressing high level goals\nthe two players.                                                          and describing strategies how to reach those goals. These goals\nThis theorem was established by John von Neumann, who is                  may be decomposed in goals at lower level called sub-goals. This\nquoted as saying \"As far as I can see, there could be no theory of        approach closely resembles the way a human player usually\ngames \u2026 without that theorem \u2026 I thought there was nothing                addresses a complex problem. It is also good for domains where\nworth publishing until the Minimax Theorem was proved\" [2].               classical search for solution is not feasible due to the vastness of\n                                                                          the problem domain or uncertainties.\n     A simple example of minimax can be observed by building a\ngame tree of the tic-tac-toe game. The tic-tac-toe game is a simple       3.2.1 Hierarchical Task Networks\ngame which can end by the first player wining, the second player\nwining or a tie. There are nine positions for each of the players in           The Hierarchical Task Network, or HTN, is an approach to\nwhich at each turn the player puts X or O sign. If the player has         automated planning in which the dependency among actions can\nthree adjacent signs in a row, column or the two diagonals he or          be given in the form of networks [9] [Figure 1].\nshe wins. This game has limited number of position and it is well              A simple task network (or just a task network for short) is an\nsuited for building the whole game tree. The leaves of this tree          acyclic digraph \u0002 \u0003 \u0004\u0005\u0006 \u0007\b in which U is the node set, E is the\nwill be final positions in the game. A heuristics evaluation              edge set, and each node       \u0005 contains a task . The edges of \u0002\nfunction will also need to be written to evaluate the value of each       define a partial ordering of U. If the partial ordering is total, then\nnode along the way.                                                       we say that \u0002 is totally ordered, in which case \u0002 can be written as\n                                                                          a sequence of tasks \u0002 \u0003 \u000e \u0006 \u000f \u0006 \u0010 \u0006 \u0011 \u0012.\n3. AI Planning for building Game Trees\n3.1.1 AI Planning                                                                                Buy milk\n      AI Planning also referred as Automated Planning and\nScheduling is a branch of Artificial Intelligence that focuses on\nfinding strategies or sequences of actions that reach a predefined\ngoal [3]. Typical execution of AI Planning algorithms is by                  Go to (shop)           Purchase             Go to (home)\nintelligent agents, autonomous robots and unmanned vehicles.\nOpposed to classical control or classification AI Planning results                  Figure 1: Simple Hierarchical Task Network\nwith complex solutions that are derived from multidimensional             A Simple Task Network (STN) method is a 4-tuple of its name,\nspace.                                                                    task, precondition and a task network. The name of the method\n      AI Planning algorithms are also common in the video game            lets us refer unambiguously to substitution instances of the\ndevelopment. They solve broad range of problems from path                 method, without having to write the preconditions and effects\nfinding to action planning. A typical planner takes three inputs: a       explicitly. The task tells what kind of task can be applied if the\ndescription of the initial state of the world, a description of the       preconditions are met. The preconditions specify the conditions\ndesired goal, and a set of possible actions. Some efforts for             that the current state needs to satisfy in order for the method to be\nincorporating planning techniques for building game trees have            applied. And the network defines the specific subtasks to\nalso shown up, similar to the approach explored in this effort. In        accomplish in order to accomplish the task.\naddition Cased Based Reasoning [4] techniques are also gathering          A method is relevant for a task if the current state satisfies the\npopularity in developing strategies based in prior knowledge              preconditions of a method that implements that task. This task can\nabout the problems in the games. One of the benefits from                 be then substituted with the instance of the method. The\nHierarchical Task Network (HTN) [5] planning is the possibility           substitution is basically giving the method network as a solution\nto build Game Trees based on HTN plans; this method is                    for the task.\ndescribed in the following section.                                       If there is a task \u201cGo home\u201d and the distance to home is 3km\n3.2 Game Trees with AI Planning                                           [Figure 2] and there exists a method walk-to and this method has a\n                                                                          precondition that the distance is less than 5km, then a substation\n     An adaptation of the HTN planning can be used to build               to the task \u201cGo home\u201d can be made with this method instance.\nmuch smaller and more efficient game trees. This idea has already\nbeen implemented in the Bridge Baron a computer program for                                               Go-to (from, to)\nthe game of Contact Bridge [6].\n     Computer programs based on Game Tree search techniques                  If (to \u2013 from) < 5km\nare now as good as or better than humans in many games like\nChess [7] and checkers [8], but there are some difficulties in                                                  Walk (to)\nbuilding a game tree for games that have imperfect information\nand added uncertainty like card or games with dice. The main                                    Figure 2: HTN Method\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                                 Interactive and Adaptable Media                                                     297\n\n\n\nIf the distance is larger than 5km another methhod instance needs                                                        present moves generated by\n                                                                                   plan. The branches of the game tree rep\nto be substituted [Figure 3].                                                      the methods. Tignum2 applies all metthods applicable to a given\n                                                                                   state of the world to produce new    w states of the world and\n                              Go-to (from, to)                                                                           n applicable methods that\n                                                                                   continues recursively until there are no\n                                                                                   have not already been applied to th  he appropriate state of the\n   If(to \u2013 from) < 5km                                 If(tto \u2013 from) < 200km\n                                                                                   world.\n                                                                                   In the task network generated by Tignuum2, the order in which the\n                          Walk (to)               Drive(to)                        actions will occur is determined by the total-ordering constraints.\n                                                                                   By listing the actions in the order they will occur, the task\n                         Figure 3: HTN Method 2                                                                         me tree [Figure 4] [Figure 5].\n                                                                                   network can be \u201cserialized\u201d into a gam\n   An STN planning domain is a set of operatioons O and a set of\nmethods M. A STN planning problem is a 4-tuuple of the initial\nstate S0, the task network w called initial taskk network and the\nSTN domain. A plan \u0013 \u0003 \u0014\u000e \u0006 \u0010 \u0006 \u0014\u0015 \u0012 is a solutiion for a planning\nproblem if there is a way to decompose w into \u03c0 if \u03c0 is executable\nand each decomposition is applicable in the apppropriate state of\nthe world. The algorithm that is capable to decompose these\nnetworks into plans is called Total-forward-decoomposition (TFD)\n[9] or Partial-forward-decomposition (PFD). H  However there are\ncases where one does not want to use a forwaard-decomposition\nprocedure. HTN planning is generalization of S  STN planning that\ngives the planning procedure more freedom       m about how to\nconstruct the task networks.\n     In order to provide this freedom, a bookkeeeping mechanism                               Figure 4: HTN to Game Trree Algorithm\nis needed to represent constraints that the plannning algorithm has\nnot yet enforced. The bookkeeping is done byy representing the\nunenforced constraints explicitly in the task netw\n                                                 work.\n     The HTN generalizes the definition of a task network in\nSTN. A task network is the pair \u0002 \u0003 \u0004\u0005\u0006 \u0017\b w    where \u0005 is a set of\ntask nodes and C is a set of constraints. Each constraint in C\nspecifies a requirement that must be satisfied byy every plan that is\na solution to a planning problem.\n     The definition of a method in HTN alsoo generalizes the\ndefinition used in STN planning. A HTN plaan is a 4-tuple of\nname, task, subtasks, and constraints. The ssubtasks and the\nconstraints form the task network. The HTN plannning domains are\nidentical to STN planning domains except they uuse HTN methods\ninstead of STN methods.                                                                     Figure 5: Game Tree built frrom HTN\n\n     Compared to classical planners the prim  mary advantage of                    4. Case Based Reasoning in\n                                                                                                            n Game Strategies\nHTN planners is their sophisticated knowledge rrepresentation and\nreasoning capabilities. They can represent and solve a variety of\n                                                                                   4.1 Case Based Reasoning\n                                                                                        Case-based reasoning (CBR) is a well established subfield of\nnon-classical planning problems; with a goodd set of HTNs to\n                                                                                   Artificial Intelligence (AI), both as a mean for addressing AI\nguide them, they can solve classical planning pproblems orders of\n                                                                                   problems and as a basis for standalone AI technology.\nmagnitude more quickly than classical or neocclassical planners.\nThe primary disadvantage of HTN is the neeed of the domain                         Case-based reasoning is a paradigm    m for combining problem-\nauthor to write not only a set of planning operaators but also a set               solving and learning that has became one of the most successful\nof methods.                                                                        applied subfield of AI of recent yeaars. CBR is based on the\n                                                                                                                          I means that new problems\n                                                                                   intuition that problems tend to recur. It\n3.2.2 HTN Planning in building Game Trees\n                                                                                   are often similar to previously en     ncountered problems and,\n     For a HTN planning algorithm to be adapted to build game                      therefore, that past solutions may be off use in the current situation\ntrees we need to define the domain (set of H  HTN methods and                      [10].\noperators) which is the domain of the game. Thiis is in some sense\n                                                                                   CBR is particularly applicable to probllems where earlier cases are\na knowledge representation of the rules of thee game, the game\n                                                                                   available, even when the domain is not  n understood well enough\nenvironments and possible strategies of game plaay.\n                                                                                   for a deep domain model. Helpdesks,, diagnosis or classification\n     In this domain the game rules as well as knnown strategies to                 systems have been the most successfu    ul areas of application, e.g.,\ntackle specific task are defined. The implem  mentation of Game                    to determine a fault or diagnostic an illness from observed\nTree building with HTN is called Tignnum2 [9]. This                                attributes, or to determine whether orr not a certain treatment or\nimplementation uses a procedure similaar to forward-                               repair is necessary given a set of past solved cases [11].\ndecomposition, but adapted to build up a game tree rather than a\n\n\n\n                                      3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f298                                                             DIMEA 2008\n\n\n\nCentral tasks that all CBR methods have to deal with are [12]: \"to       And in the third position if the two of center, middle top and\nidentify the current problem situation, find a past case similar to      middle left are available the position is a certain victory.\nthe new one, use that case to suggest a solution to the current\nproblem, evaluate the proposed solution, and update the system by             There are many different arrangements of the player\u2019s tokens\nlearning from this experience. How this is done, what part of the        that give equivalent positions as these three positions. By using\nprocess that is focused, what type of problems that drives the           planning we do not need to consider all possible layouts but just\nmethods, etc. varies considerably, however\".                             consider these three similar to what a human would consider.\n\n     While the underlying ideas of CBR can be applied                          The game starts from an empty table.\nconsistently across application domains, the specific                          The two relevant strategies that would lead to these positions\nimplementation of the CBR methods \u2013in particular retrieval and           are to take one corner or to take the center [Figure 7].\nsimilarity functions\u2013 is highly customized to the application at\nhand.\n4.2 CBR and Games\n     Many different implementations of CBR exist in games.\nCBR technology is nicely suited for recognizing complex\nsituations much easier and more elegant than traditional parameter                   Figure 7: Tic-tac-toe Two starting moves\ncomparison or function evaluation. There are especially evident\ncases in real time strategies where different attack and defense of            The center position as we can see in the simulation results\nglobal strategies are nicely defined by CBR datasets and later used      lead to a bigger number of victorious endings but it is also a\nin the running games. Also intelligent bots behavior is also             straight forward strategy with obvious defense strategy.\nanother typical example. Depending on the number of enemy bots                At this point we need to consider the moves of the opponent.\nthe layout of the terrain and position of human players the CBR          If we take the left branch the opponent moves can be a center, a\nsystem finds the closest CBR case and employs that strategy              corner or a middle field. We also need to differentiate with a\nagainst the human players which in prior evaluation was proved to        move to a corner adjacent with our like top left or bottom right or\nbe highly efficient.                                                     across the center to bottom right [Figure 8].\n5. Game Trees with AI Planning \u2013 Tic-tac-toe\n      In order to show the expressive power of AI Planning in\ndefining strategies for games, and the use of these plans to build\nGame Trees I implemented an algorithm that builds Game Trees\nfor the Tic-Tac-Toe game.\n                                                                             Figure 8: Tic-tac-toe opponent response to corner move\n     The game tree of Tic-Tac-Toe shows 255,168 possible\n                                                                               In cases one and two, we have a clear path to executing\ngames of which 131,184 are won by X (the first player), 77904\n                                                                         strategy 3 so we need to capture the diagonally opposite field.\nare won by O and the rest 46,080 are draw [13]. All these games\n                                                                         And as for the third case the best way to go is to capture the center\ncan be derived from building a complete Game Tree.\n                                                                         and go for strategy 1 or 2 depending of the opponent\u2019s next move.\n     Even though it is possible to build a complete game tree of\nTic-tac-toe it is definitely not an optimal solution. Many of the\nmoves in this tree would be symmetrical and also there are a many\nmoves that would be illogical or at least a bad strategy to even\nconsider.\n     So what strategy should X (the first player) choose in order                Figure 9: Tic-tac-toe move 2 after corner opening\nto win the game?                                                              The first move leads to certain victory, O will have to go to\n                                                                         the center and X will achieve strategy 3 [Figure 9]. The second\n     There are few positions that lead to certain victory. These\n                                                                         move is a possible way to strategy 3 if O makes a mistake in the\npositions involve simultaneous attack on two positions so the\n                                                                         next loop, so X goes to the opposite corner. For the third case\nother player could not defend, basically the only trick in Tic-Tac-\n                                                                         since O is playing a valid strategy the only move that leaves a\nToe.\n                                                                         possible mistake from O would be to take the center and wait for\n                                                                         O to go to the middle and then achieve strategy 1 or 3 which will\n                                                                         be a symmetric situation to the one that we will find if we\n                                                                         branched with the center.\n\n\n        Figure 6: Tic-tac-toe winning strategy positions\n     Position 1 leads to victory if the two of the three fields: top\nmiddle, bottom left corner and bottom right corner are free\n[Figure 6].\n    Position 2 lead to victory if two of the three fields: top right         Figure 10: Tic-tac-toe opponent response to center move\ncorner, bottom right corner and bottom middle are free [Figure ].            If we go back to the second branch [Figure 10], a possible\n                                                                         way for the second player to engage is corner or middle. The first\n\n\n\n                             3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                         Interactive and Adaptable Media                                                      299\n\n\n\nmove is a valid strategy for O and can be meeet with a opposite                  This HTN when executed will reesult with plans for possible\ncorner move from X to try a mistake from O in the future exactly           game scenarios. By creating nodes from m each position and linking\nthe same as in the third case above from the preevious branch, and         them with branches with the move of the\n                                                                                                                 t player we create a game\nanother move would be go to the middle whhere X eventually                 tree for the Tic-tac-toe game over whiich we can run the minimax\nachieves strategy 1 or 2.                                                  algorithm.\n                                                                                 This set up with 7 plans with 3 taarget strategies creates a tree\n                                                                           for Tic-tac-toe which considers all posssible moves for the second\n                                                                           player with only 457 games, 281 of whichw       X wins 176 are draw\n                                                                           and 0 where the second opponent wins.    w      This is a significant\n       Figure 11: Tic-tac-toe Move 2 after centter opening                 reduction over the 255, 168 possible games with a complete game\n     The fist move will lead to win if O moves to the middle or a          tree. These reductions can be very useeful for devices with limited\ndraw if it goes for the corners [Figure 11]. In tthe second case O         computing capabilities but also we prrove a very important point\nhas to block the lower left corner which leavees X to go for the                                                    d\n                                                                           that planning can be very efficient if designing    meaningful game\nmiddle left or corner left which are strategy 1 andd 2.                    trees by applying reasoning very similar to human player\n     To sum the strategies for the planning, first we have center or       reasoning.\ncorner strategy for the beginning. Then for the ceenter we try to get            Further improvements to the gam   me tree are also possible if\nthe corners with the particularly the one oppoosite to the one O           the opponents moves are also planned    d, in other words if we drop\nholds. If the center is empty for the second strateegy we go for it or                                           m\n                                                                           all the meaningless and symmetrical moves      of the opponent.\nwe go for the opposite corner. After this point wwe either block the       6. Game AI in Monopoly\nopponent or try to implement strategies 1, 2 orr 3 which lead to\nvictory.\n                                                                           6.1 Overview of the AI Imp\n                                                                                                    plementation\nPlan 1: Take center                                                              The AI agent is responsible for the moves of the artificial\nPreconditions: Center empty                                                players in the game. The core principlee of the AI agent is building\nPlan 2: Take corner                                                        a Game Tree with all the sensible movees that all the players would\nPreconditions: All corners empty                                           make from the current point of timee forward. Then using the\nPlan 3: Take corner after center                                           minimax algorithm the agent selects the  t move that in the future\nPreconditions: We have center take corner oppossite to the one the         would bring the computer player mo     ost favorable game position\nopponent has                                                               with the highest probability. Building a Game Tree in this game\nPlan 4: Take diagonal corner                                               that would be big enough to consider sufficient number of moves\nPreconditions: We have a corner, the opponent ha    as the ce\u2212nter and\n                                                                           is obstructed by the vastness of posssible moves in combination\n the corner opposite to the one we have is free.\nPlan 5: Block                                                              with all the possible random landings of the dice. The number of\nPrecondition: The opponent has tree tokens in a rrow, colu\u2212mn or di        nodes of the game tree exponentially     y grows at each level. To\nagonal                                                                     tackle this problem the AI agents incorporates two already\nPlan 6: Win                                                                discussed technologies: Case Based Reeasoning and AI Planning.\nPreconditions: We have two tokens in a row, column or dia\u2212gonal a                The technologies are employed in the following manner.\nnd the third place is free                                                 First the agent searches the CBR datab base to find the case with the\nPlan 7: Tie                                                                largest similarity with the current statee of the board. This case is\nPreconditions: If all places are taken, it\u2019s a tie.                        associated with a playing strategy. Th  he strategy consists of goal\n5.1 Hierarchical Task Network                                                                                      f and the plans consist of\n                                                                           that the planner needs to build plans for,\n     Top level task is Play [Figure 12]. This is a complex task and        consecutive player moves that bring th   he player to that goal. This\ncan be derived into: Win, Block, Tie or Searrch for Plan. The              way only moves that are part of that strrategy are considered, those\nSearch for plan is derived to both Plan 1 and Plaan 2 or Plan 3 and        being a small fraction of the overall poossible moves the number of\nPlan 4, which later leads to a call for the opponnent\u2019s move and a         edges of the game tree at each level deccreases immensely.\nrecursive call to Play.                                                          At each level of the game tree thee model considers the moves\n                                                                           of a single player. After the strateg    gies of the AI player are\n                                                                           considered the response to those strateegies needs to be considered\n                                                                           by the opponent(s). The move of the opponent(s) depends of the\n                                                                           probability distribution of the dice as well as the strategy of the\n                                                                           player. A more general strategy needss to be implemented for the\n                                                                           opponent\u2019s (human player) moves sin     nce we cannot be aware of\n                                                                           the expertise of the opponent. This geeneral strategy would bring\n                                                                           more plausible moves than the focused  d strategy of the AI player.\n                                                                                 After covering all opponents the   t    agent comes back to\n                                                                           deducting a feature move of the com     mputer player by using the\n                                                                           CBR selected plan strategy. After creating several loops of\n                                                                           strategies and reaching a reasonable size\n                                                                                                                   s    of a Game Tree taking\n                                                                           into account the memory limits an        nd the rapidly decreasing\n                                                                           probabilities that the move is possiblee due to the distribution of\n                                                                           the dice the building of the Game Treee stops. Then the minimax\n                                                                           algorithm searches the Game Tree and decides on the most\n                                                                           favorable move for the AI player usiing the minimax algorithm.\n                      Figure 12: Tic-tac-toe HT\n                                              TN                           The process is repeated each time the AIA player is up.\n\n\n\n                               3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f300                                                             DIMEA 2008\n\n\n\n      Buying, auctioning and trading game moves are always                    On the other hand the MonopolySolution class holds the\naccompanied by return of investment calculations in making the           three particular attributes that are needed for the planning, the\nplans. These calculations represent adaptation of the more general       planning Domain, State and TaskList.\nplanning associated with the cases in the CBR database. These\nadaptations are necessary due to the fact that the cases do not               The game is implemented by using the Model-View-\nidentically correspond to the situation on the table. In addition        Controller software development pattern. The controller is\ncalculating the game position value of each node of the game tree        responsible for implementing the game rules and handling all of\nis done by heuristic functions that incorporate economic                 the events in the game like roll of dice, input commands for\ncalculations of net present value, cash, and strategic layout and so     trading, auctioning and etc from the players. The View layer is\non. For example railroads in monopoly are known to be                    responsible for displaying the board and all of the input widgets\nstrategically effective because they bring constant income even          on to the game screen, and the models are data structures\nthough the income can be smaller than building on other                  representing the game state [Figure 14].\nproperties.\n6.2 Details on the CBR Implementation\n     The implementation of the CBR is by using the JColibri2\nplatform. JColibri2 is an object-oriented framework in Java for\nbuilding CBR systems that is an evolution of previous work on\nknowledge intensive CBR [14].\n     For this implementation we need to look into three particular\nclasses of the JColibri2 platform. The StandardCBRApplication,\nConnector, CBRQuery. For a JColibri2 implementation the\nStandardCBRApplication interface needs to be implemented.\n     The CBR cycle executed accepts an instance of CBRQuery.\nThis class represents a CBR query to the CBR database. The\ndescription component (instance of CaseComponent) represents\nthe description of the case that will be looked up in the database.\nAll cases and case solutions are implementing the\nCaseComponent interface.\n     The JColibri2 platform connects to the CBR database via a\nConnector class. Each connector implements all the necessary\nmethods for accessing the database, retrieval of cases, storing and\ndeletion of cases. This implementation uses a custom XML\nstructure for holding the CBR cases. Since the game will not\nupdate the CBR database only read it, a XML solution satisfies\nthe needs. The XML file to a certain extent is similar to the XML\nrepresentation of the board. We are interested in finding one\nCBRCase that is the most similar case to the situation in the game\nat the time of the search. This procedure is done in the cycle\nmethod of the CBRApplication. The JColibri2 CBR comparison is\ndone by Nearest Neighbor (NN) search method.\n     JColibri2 offers implementations for NN search algorithms\nof simple attributes. These implementations are called local\nsimilarities. For complex attributes like in our case global                    Figure 14: Class diagram of the Monopoly models\ncustomized similarity mechanisms need to be implemented.\n     The MonopolyDescription class [Figure 13] is basically a\n                                                                         6.2.1 Complex Similarity representation in CBR\nserialization of the GameState. It holds all the information about             The similarity measurement part of the Nearest Neighbor\n                                                                         algorithm JColibri2 is implemented by implementing the\nthe state of the board, the players, their amount of cash etc.\n                                                                         LocalSimiralrityFunction and the GlobalSimiralityFunction\n                                                                         interface. A local similarity function is applied to simple attributes\n                                                                         by the NN algorithm, and a global similarity function is applied to\n                                                                         compound attributes. In the case of our implementation the\n                                                                         attributes of the MonopolyDescription are compound attributes\n                                                                         describing the state of the board, number of players, amount of\n                                                                         cash for every player and etc. Since MonopolyDescription is a\n                                                                         custom CaseComponent a global similarity function needs to be\n                                                                         implemented to accurately find the distance between different\n                                                                         CBR cases.\n                                                                             The similarity mechanism is inseparable core element of the\n                                                                         CBR system. This mechanism represents how the CBR decides\n                                                                         which strategy is best suited for the particular situation by\n Figure 13: Class diagram of the Monopoly Case component\n                          models\n\n\n\n                             3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f                                                       Interactive and Adaptable Media                                                    301\n\n\n\ncalculating the distance or similarity to other cases in the                   JSHOP2 uses ordered task decomposition in reducing the\ndatabase.                                                                 HTN to list of primitive tasks which form the plans. An ordered\n                                                                          task decomposition planner is an HTN planner that plans for tasks\n     For the monopoly implementation we need to consider                  in the same order that they will be executed. This reduces the\nseveral basic strategies. Monopoly is based on investing in               complexity of reasoning by removing a great deal of uncertainty\nproperties and receiving revenues from those investments. One of          about the world, which makes it easy to incorporate substantial\nthe basic strategies of the game is to build a set of properties that     expressive power into the planning algorithm. In addition to the\nwill bring constant income larger than the one of the opponents.          usual HTN methods and operators, the planners can make use of\nSo in time the opponents will have to declare bankruptcy. But on          axioms, can do mixed symbolic/numeric conditions, and can do\nthe other hand over investment can lead to too stretched resources        external function calls.\nwith low income that will eventually drove the player to\nbankruptcy. To decide on these two we need a clear separation                   In order for the JSHOP2 planer to generate plans it needs\ninto two groups of cases in the CBR database. The first group of          tree crucial components: Domain, State and Tasks. The Domain\ncases will represent a situation on the board where the player has        defines all the functionalities that the particular domain offers.\nsignificant income per loop formed of one or more color group             These are simple and complex tasks. The complex tasks also\nproperties, maybe railroads, some buildings on them and so on. It         called methods create the hierarchy with the fact that they can be\nis important to note that in this case the player is better situated      evaluated by simple tasks of other complex tasks. This is how a\nthan his opponents so he only needs to survive long enough to win         hierarchical structure of tasks is formed. The problem reduction is\nthe game. In the other group of cases either the opponent is not          done by reducing the high level complex tasks to simpler until all\nwell positioned on the board or its opponents are better situated.        the tasks are primitive. The list of primitive tasks forms the plan.\nIn this case further investments are necessary to improve the\nsituation so the player can have a chance of winning in the long                The State represents the state of the system. It is a simple\nrun.                                                                      database of facts that represent the state of the system. The State\n                                                                          is necessary to determine the way the problems or tasks are\n      These metrics can be owning color groups, valuing groups of         reduced to their primitive level. The reduction is done by\nrailroads, evaluating the other opponents as well, and considering        satisfying different prerequisites set in the methods; these\nthe amount of cash. As it is obvious in monopoly the number of            prerequisites are defined in the state. The Tasks are high level\nstreets is not as nearly as important as the combination of streets       tasks or methods defined in the Domain. The planner based on the\nthe player owns. It is also important to note that one CBR case           State and the goals selects one or more high level tasks that need\ndoes not hold only a single strategy in place, but its solution can       to be reduced to plans [Figure 15].\nhave multiple different strategic goals. For example one CBR case\nmight simultaneously say buy this land to form a color group but                       Tasks\nalso trade some other unimportant property to increase cash\n                                                                                                                    Core Planner\namount.\n     The cases do not represent all possible combinations of board\npositions. They are only representation of typical game scenarios.                     State                        Plan\nThe CBR Case solutions do not give exact instructions in general\nbut rather strategic goals. For example one CBR Solution might\n                                                                                           Figure 15: Diagram of a Planner\nsay trade the streets that you only have one of each for the ones\nthat you have two of that color already. Then the planner based on            The plans then generate the game moves. The number of\nthe situation on the board needs to decompose this high level task        moves generated by the plans is just a fraction of the possible\nto a low level operations. Like offer \"Mediterranean Avenue\" for          moves at that point. This reduces the game tree providing the\n\"Reading Railroad\" and offer $50. The exact amounts and actual            opportunity to generate smaller and deeper game trees and making\nstreets are left to the planer to evaluate.                               more efficient decisions in general.\n\n      The monopoly CBR database is currently in development on            7. Conclusion\na monopoly clone game called Spaceopoly. The cases are                          Even though the results from the CBR database are not\narchitected based on human player experience and knowledge.               complete at this time partial strategies are implemented as cases\nThere is a plan of making a number of slightly different strategies       and recognized during game play by the CBR system. These\nthat differ on the style of playing and then running simulation           smaller local strategies coupled with more global higher level\ntests that would determine the particular validity of each database       strategies that are particularly important at the beginning of the\nas well as validity of certain segments of the strategy or even           game would form a complete CBR database and represent a\nparticular cases in the database.                                         knowledge engineered style of playing of the AI player.\n      The actual execution of the strategies will not differ from              The AI Planning approach is a proven method by the tic-tac-\nstrategy to strategy since the plan execution is more related to the      toe experiment and is suitable for implementing the strategies\nstructure and rules of the game than to the actual playing strategy.      associated with the CBR cases.\n6.3 Details on the Planning Implementation                                     This approach in general benefits from both technologies,\n     For the purpose of planning this implementation uses a               CBR as well as AI Planning and comprises an elegant solution.\nmodification of the JSHOP2 planner. The Java Simple                       Even though AI Planning can be enough as a single technology\nHierarchical Ordered Planner 2 is a domain independent HTN                for some simpler problems like tic-tac-toe the complexity of\nplanning system [15].                                                     Monopoly would mean that the Planner would have to incorporate\n\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f302                                                              DIMEA 2008\n\n\n\nlarge and complex domain and a very big state model. The CBR              reasoning. People in addition to logic of reasoning in situations\napplication helps reduce this complexity by focusing the planning         with lack of information rely to planning strategies and prior\non smaller domain of the game. Basically the CBR reduces the              experience, exactly the intuition behind CBR \u2013 AI Planning\noverall goal of the play (wining the game) to smaller more                architecture.\nconcrete goals suitable to the particular state of the game, thus\nreducing the need for global planning strategies and complex              9. ACKNOWLEDGMENTS\nplanning domain.                                                               We would like to thank Prof. Sofia Tsekeridou for her\n                                                                          involvement in the valuable discussions we had on the topic of\n     Furthermore this symbiosis of technologies gives way for             CBR.\nmore precise and finely tuned strategies which can be difficult to\ninclude into global plan for the whole game. One simple example           10. REFERENCES\nfor the Monopoly game would be this: Sometimes it\u2019s better to             [1] Minimax. Wikipedia. [Online] [Cited: April 23, 2008.]\nstay in jail because rolling double increases the probability of              http://en.wikipedia.org/wiki/Minimax.\nlanding on some field (two, four, six, eight, ten or twelve steps\nfrom the jail) that can be of great importance to the rest of the         [2] Von Neumann, J: Zur theorie der gesellschaftsspiele Math.\ngame. These and similar small local strategies can be easily                  Annalen. 100 (1928) 295-320\nrecognized by similar cases in the CBR database.                          [3] Automated Planning. Wikipedia. [Online] [Cited: April 23,\n                                                                              2008.] http://en.wikipedia.org/wiki/Automated_planning.\n      In other words the system is flexible enough so that new\nstrategies can be incorporated easily missing strategies can be also      [4] Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based\nrecognized by the distance metrics as well as wrong assumptions               Strategy Game with Plan Adaptation and Ontology-based\nin the strategies can be easily recognized.                                   retrieval.\n\n     One other important property of the system is that is highly         [5] K. Erol, J. Hendler, and D. Nau (1994). Semantics for\nconfigurable. The game its self can be diversely different                    hierarchical task-network planning. Technical Report TR-94-\ndepending on the configuration of the board. Even though the                  31, UMIACS.\nplatform is restricted to Monopoly type of games, changing the            [6] Smith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning\nlayout and values of the fields effectively brings completely                 approach decrarer play in contract bridge. Computational\ndifferent properties of the game. In addition the CBR database                Intelligence. 1996, Vol. 12, 1.\nrepresents the entire experience of the AI Player. It can be filled       [7] One Jump Ahead: Challenging Human Supremacy in\nwith rich set of strategies or even configured with different flavors         Checkers. J.Schaeffer. s.l. : Springer-Verlag, 1997.\nof difficulties of play, this of course coupled with the domain of\nthe planner which can differ from a case to a case as well.               [8] IBM. How Deep Blue works. [Online] 1997. [Cited: April\n                                                                              23, 2008.]\n8. Future Work                                                                http://www.research.ibm.com/deepblue/meet/html/d.3.2.html\n      Further exploration of this technology would go towards             [9] Ghallab, Malik, Nau, Dana and Traverso, Paolo.\ncomplete implementation of an AI aware agent for monopoly.                    Automated Planning theory and practice. s.l. : Morgan\nInitial results from the local cases with more specific strategies            Kaufmann Publishers, May 2004. ISBN 1-55860-856-7.\nshow CBR as a capable tool for representing expertise in playing\nthe game. Completing the more general strategies and coupling             [10] Case Based Reasoning. Experiences, Lessons and Future.\nthem with the planning domain will give precise results on the                 Leake, David. s.l. : AAAI Press. MIT Press., 1997.\nbenefits from this architecture.                                          [11] Applying case-based reasoning: techniques for enterprise\n                                                                               systems. Watson, I. San Francisco, CA, USA : Morgan\n     There is also need for exploring the planning of strategies of\n                                                                               Kaufmann Publishers Inc., 1998.\nopponents. This task is to some extent different because we\ncannot always expect the opponent to select the best move we              [12] Plaza, A. Aamodt and E. Case-based reasoning:\nthink. In the Tic-tac-toe example all possible moves of the                    Foundational issues, methodological. AI Communications.\nopponent were taken into consideration, if we used the same                    1994, 7(i).\nplanner for the opponent only tie games would result from the             [13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.]\ngame tree. In other words mistakes of the players also need to be              http://en.wikipedia.org/wiki/Tic-tac-toe.\nconsidered.\n                                                                          [14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An\n      The CBR Platform brings other functionalities well worth of              architecture for knowledge intensive CBR systems. Advances\nexploring as well. The revision stage of the JColibri2 platform is             in Case-Based Reasoning \u2013 (EWCBR\u201900). New York :\nbasically capable of fine tuning strategies or even developing new             Springer-Verlag, Berlin Heidelberg, 2000.\nstrategies for the games. A well written underlying AI planning           [15] Ilghami, Okhtay and Nau, Dana S. A General Approach to\nmodel with a capable feedback of the game tree evaluation back                 Synthesize Problem-Specific Planners. 2003.\nto the CBR revision capability can be an interesting concept in\nautomatic experience acquisition for the AI model.\n     There are also many other fields were combined CBR and\nplanning approach can be incorporated into a problem solution.\nThis combination is analogous in a big extent to a human way of\n\n\n\n\n                              3rd International Conference on Digital Interactive Media in Entertainment and Arts\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nVon Neumann, J : Zur theorie der gesellschaftsspiele  Math. \nAnnalen. 100 (1928) 295-320 ", " Automated Planning. Wikipedia. [Online] [Cited: April 23, \n2008.] http://en.wikipedia.org/wiki/Automated_planning . ", " Sanchez-Ruiz, Antonio, et al. Game AI for a Turn-based \nStrategy Game with Plan Adaptation and Ontology-based \nretrieval ."], "publish_date": "2008-09-10", "approved": true}}, {"_index": "articles", "_id": "eikwcI0Bf-PcuVTA6FKO", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Framing the News: From Human Perception to Large Language Model Inferences", "abstract": "Identifying the frames of news is important to understand the articles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.", "authors": ["David Alonso del Barrio", "Daniel Gatica-Perez"], "institutions": ["Idiap Research Institute", "EPFL"], "keywords": ["Covid-19 no-vax", "news framing", "GPT-3", "prompt-engineering", "transformers", "large language models"], "full_text": "                      Framing the News:\n    From Human Perception to Large Language Model Inferences\n                           David Alonso del Barrio                                                                      Daniel Gatica-Perez\n                                  ddbarrio@idiap.ch                                                                        gatica@idiap.ch\n                               Idiap Research Institute                                                           Idiap Research Institute and EPFL\n                                     Switzerland                                                                             Switzerland\nABSTRACT                                                                                              1   INTRODUCTION\nIdentifying the frames of news is important to understand the arti-                                   In recent years, there has been a proliferation in the use of concepts\ncles\u2019 vision, intention, message to be conveyed, and which aspects                                    such as data journalism, computational journalism, and computer-\nof the news are emphasized. Framing is a widely studied concept                                       assisted reporting [15] [29], which all share the vision of bridging\nin journalism, and has emerged as a new topic in computing, with                                      journalism and technology. The progress made in NLP has been\nthe potential to automate processes and facilitate the work of jour-                                  gradually integrated into the journalistic field [5][8][54]. More\nnalism professionals. In this paper, we study this issue with articles                                specifically, machine learning models based on transformers have\nrelated to the Covid-19 anti-vaccine movement. First, to under-                                       been integrated in the media sector in different tasks [41] such as\nstand the perspectives used to treat this theme, we developed a                                       the creation of headlines with generative languages models [17],\nprotocol for human labeling of frames for 1786 headlines of No-                                       summarization of news articles [28][27], false news detection [49],\nVax movement articles of European newspapers from 5 countries.                                        and topic modeling and sentiment analysis [25]. The development of\nHeadlines are key units in the written press, and worth of analysis                                   large language models such as GPT-3 [9], BLOOM [51] or ChatGPT\nas many people only read headlines (or use them to guide their                                        show a clear trend towards human-machine interaction becoming\ndecision for further reading.) Second, considering advances in Nat-                                   easier and more intuitive, opening up a wide range of research\nural Language Processing (NLP) with large language models, we                                         possibilities. At the same time, the use of these models is also\ninvestigated two approaches for frame inference of news headlines:                                    associated with a lack of transparency regarding how these models\nfirst with a GPT-3.5 fine-tuning approach, and second with GPT-                                       work, but efforts are being made to bring some transparency to\n3.5 prompt-engineering. Our work contributes to the study and                                         these models, and to analyze use cases where they can be useful and\nanalysis of the performance that these models have to facilitate                                      where they cannot [35]. Based on the premises that these models\njournalistic tasks like classification of frames, while understanding                                 open up a wide range of research directions [7], and that at the same\nwhether the models are able to replicate human perception in the                                      time (and needless to say) they are not the solution to all problems,\nidentification of these frames.                                                                       we are interested in identifying use cases and tasks where they\n                                                                                                      can be potentially useful, while acknowledging and systematically\nCCS CONCEPTS                                                                                          documenting their limitations [56]. More specifically, the aim of\n                                                                                                      this work is to analyze the performance of GPT-3.5 for a specific\n\u2022 Computing methodologies \u2192 Information extraction; \u2022\n                                                                                                      use case, namely the analysis of frames in news, from an empirical\nHuman-centered computing \u2192 Text input.\n                                                                                                      point of view, with the objective of shedding light on a potential\n                                                                                                      use of generative models in journalistic tasks.\nKEYWORDS                                                                                                 Frame analysis is a concept from journalism, which consists of\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering, trans-                                      studying the way in which news stories are presented on an issue,\nformers, large language models                                                                        and what aspects are emphasized: Is a merely informative vision\n                                                                                                      given in an article? Or is it intended to leave a moral lesson? Is\nACM Reference Format:                                                                                 a news article being presented from an economic point of view?\nDavid Alonso del Barrio and Daniel Gatica-Perez. 2023. Framing the News:                              Or from a more human, emotional angle? The examples above\nFrom Human Perception to Large Language Model Inferences . In Inter-                                  correspond to different frames with which an article can be written.\nnational Conference on Multimedia Retrieval (ICMR \u201923), June 12\u201315, 2023,\n                                                                                                         The concept of news framing has been studied in computing as\nThessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3591106.3592278\n                                                                                                      a step beyond topic modeling and sentiment analysis, and for this\n                                                                                                      purpose, in recent years, pre-trained language models have been\n                                                                                                      used for fine-tuning the classification process of these frames [60]\nPermission to make digital or hard copies of all or part of this work for personal or                 [10], but the emergence of generative models opens the possibility\nclassroom use is granted without fee provided that copies are not made or distributed                 of doing prompt-engineering of these classification tasks, instead\nfor profit or commercial advantage and that copies bear this notice and the full citation             of the fine-tuning approach investigated so far.\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or                   Our work aims to address this research gap by posing the fol-\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission         lowing research questions:\nand/or a fee. Request permissions from permissions@acm.org.\n                                                                                                         RQ1: What are the main frames in the news headlines about\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.                     the anti-vaccine movement, as reported in newspapers across 5\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00                                                          European countries?\nhttps://doi.org/10.1145/3591106.3592278\n\n\n\n\n                                                                                                627\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                        David Alonso del Barrio and Daniel Gatica-Perez\n\n\n    RQ2: Can prompt engineering be used for classification of head-         message? Does the story make reference to morality, God, and other\nlines according to frames?                                                  religious tenets? Does the story offer specific social prescriptions\n    By addressing the above research questions, our work makes the          about how to behave?\", and so on for each of the frame types. This\nfollowing contributions:                                                    categorization of frames has been used in various topics such as\n    Contribution 1. We implemented a process to do human an-                climate change [18] [19], vaccine hesitance [13], or immigration\nnotation of the main frame of 1786 headlines of articles about the          [34].\nCovid-19 no-vax movement, as reported in 19 newspapers from 5                   We now compare the two approaches on a common topic, such\nEuropean countries (France, Italy, Spain, Switzerland and United            as Covid-19. Ebrahim et al. [21] followed an inductive approach\nKingdom.) At the headline level, we found that the predominant              in which the frames were not predefined but emerged from the\nframe was human interest, where this frame corresponds to a per-            text (e.g., deadly spread, stay home, what if, the cost of Covid-19)\nsonification of an event, either through a statement by a person,           using headlines as the unit of analysis. In contrast, the deductive\nor the explanation of a specific event that happened to a person.           approach has studied very different labels. El-Behary et al. [22]\nFurthermore, we found a large number of headlines annotated as              followed the method of yes/no questions, but in addition to the\ncontaining no frame, as they simply present information without             5 generic frames presented before, they also used blame frame\nentering into evaluations. We also found that for all the countries         and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5\ninvolved, the distribution of frame types was very similar, i.e., hu-       generic frames with yes/no questions, while Catal\u00e1n-Matamoros et\nman interest and no frame are the two predominant frames. Finally,          al. [14] used the 5 frames and read the headline and subheadline\nthe generated annotations allowed to subsequently study the per-            to decide the main frame. Table 1 summarizes some of the the\nformance of a large language model.                                         existing approaches. This previous work showed how frame labels\n    Contribution 2. We studied the performance of GPT-3.5 on                can be different, and also that frame analysis has been done at both\nthe task of frame classification of headlines. In addition to using         headline and article levels. These two approaches (inductive and\nthe fine-tuning approach from previous literature, we propose an            deductive) that originated in journalism have since been replicated\nalternative approach for frame classification that requires no labeled      in the computing literature.\ndata for training, namely prompt-engineering using GPT-3.5. The                 We decided to follow the deductive approach because a prede-\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy            fined list of frames allows to compare among topics, countries,\n(slightly higher than other smaller models), and that the prompt-           previous literature, and also because they represent a fixed list of\nengineering approach results in lower performance (49% accuracy.)           labels for machine classification models. Furthermore, the induc-\nOur analysis also shows that the subjectivity of the human labeling         tive approach tends to be more specific to a topic, and from the\ntask has an effect on the obtained accufracy.                               computing viewpoint, past work has tried to justify topic modeling\n    The paper is organized as follows. In Section 2, we discuss related     as a technique to extract frames from articles.\nwork. In Section 3, we describe the news dataset. In Section 4, we              Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex-\ndescribe the methodology for both human labeling and machine                traction technique. They argued that topics can be interpreted as\nclassification of news frames. We present and discuss results for           frames if three requirements are met: frames are operationalized as\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide          connections between concepts; subject-specific data is selected; and\nconclusions in Section 7.                                                   topics are adequately validated as frames, for which they suggested\n                                                                            a practical procedure. This approach was based on the choice of a\n                                                                            specific topic (e.g., climate change) and the use of Latent Dirichlet\n2    RELATED WORK                                                           Allocation (LDA) as a technique to extract a number of subtopics.\nFraming has been a concept widely studied in journalism, with a             In a second phase, a qualitative study of the top 10 words of each\ndefinition that is rooted in the study of this domain [23]: \u201cTo frame       subtopic was performed, and the different subtopics were elimi-\nis to select some aspects of a perceived reality and make them more         nated or grouped, reducing the number and establishing a tentative\nsalient in a communicating text, in such a way as to promote a par-         description. In a third phase, the top 10 articles belonging to that\nticular problem definition, causal interpretation, moral evaluation,        frame/topic were taken, and if the description of the topic fitted\nand/or treatment recommendation for the item described.\u201d                    at least 8 of the 10 articles, that topic/frame remained. The frames\n    For frame recognition, there are two main approaches: the induc-        found in this article were: green growth, emission cuts, negotiations\ntive approach [16], where one can extract the frames after reading          and treaties, environmental risk, cost of carbon emissions, Chinese\nthe article, and the deductive approach [38], where a predefined            emissions, economics of energy production, climate change, en-\nlist of frames exists and the goal is to interpret if any of them ap-       vironmental activism, North-South burden sharing, state leaders\npears in the article. In the deductive case, there are generic frames       negotiating, and citizen participation.\nand subject-specific frames, and the way to detect them typically               From Entman\u2019s definition of frame [23], it seems that the deduc-\ninvolves reading and identifying one frame at a time, or through            tive approach is more refined than the inductive approach (which\nanswers to yes/no questions that represent the frames. Semetko et           seems to resemble the detection of sub-themes.) For example, with\nal. [52] used 5 types of generic frames (attribution of responsibility,     regard to climate change, there are stories on how people have been\nhuman interest, conflict, morality, and economic consequences)              affected by climate change from an emotional point of view, thus\nbased on previous literature, and they defined a list of 20 yes/no          personalizing the problem. In this case, we could categorize the\nquestions to detect frames in articles. For instance, the questions         corresponding frame as human interest, as the writer of the article\nabout morality are the following: \"Does the story contain any moral         is selecting \"some aspects of a perceived reality and make them\n\n\n\n\n                                                                      628\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                            ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\nmore salient\". The language subtleties with which news articles are            Table 1: Summary of deductive approaches for frame analysis\npresented cannot be captured with basic topic modeling.\n   Isoaho et al.[30] held the position that while the benefits of                  Ref Frames                                         Goal                       Technique          Number\n                                                                                                                                                                                    samples\n                                                                                                                                                                                                of\n\nscale and scope in topic modeling were clear, there were also a                    [12] 15 generic frames: \"Economic\", \"Capac-        To label frames of full    Reading the\n                                                                                                                                                                                    20000 articles\nnumber of problems, namely that topic outputs do not correspond                         ity and resources\", \"Morality\", \"Fair-        articles                   full    article,\n                                                                                        ness and equality\", \"Legality, constitu-                                 the annotator\nto the methodological definition of frames, and thus topic modeling                     tionality and jurisprudence\", \"Policy                                    defines     the\n                                                                                        prescription and evaluation\", \"Crime                                     main frame\nremained an incomplete method for frame analysis. Topic modeling,                       and punishment\", \"Security and de-\n                                                                                        fense\", \"Health and safety\", \"Quality of\nin the practice of journalistic research, is a useful technique to deal                 life\", \"Cultural identity\", \"Public opin-\nwith the large datasets that are available, yet is often not enough to                  ion\", \"Political\", \"External regulation\n                                                                                        and reputation\", \"Other\".\ndo more thorough analyses [31]. In our work, we clearly notice that                [33] 15 generic frames                             Classification             BERT based\n                                                                                                                                                                                    12000 articles\n                                                                                                                                                                 models\nframe analysis is not topic modeling. For example, two documents                   [52] 5 generic frames: \"human interest\",           To label frames of full    Yes/No ques-       2600 articles\n                                                                                        \"conflict\", \"morality\", \"attribution of       articles                   tions.             and 1522 tv\ncould be about the same topic, say Covid-19 vaccination, but one                        responsibility\", and \"economic conse-                                                       news stories\narticle could emphasize the number of deaths after vaccination,                         quences\".\n                                                                                   [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic         To label frames of full    Reading the        2990 headlines\nwhile the other emphasized the role of the vaccine as a solution to                     opinion\u201d, \u201cSociety/Culture\u201d, and              articles/ Classification   full    article,\n                                                                                        \u201cEconomic consequences\u201d , \u201c2nd                                           the annotator\nthe epidemic.                                                                           Amendment\u201d (Gun Rights), \u201cGun                                            defines     the\n                                                                                        control/regulation\u201d, \u201cMental health\u201d,                                    main frame.\n   We also consider that the larger the number of possible frame                        \u201cSchool/Public space safety\u201d, and                                        BERT based\n                                                                                        \u201cRace/Ethnicity\u201d.                                                        models\ntypes, the more likely it is to end up doing topic modeling instead of             [22] 5 generic frames + blame frame and            To label frames of full    Yes/No ques-       1170 articles\nframe analysis. Using a deductive approach, Dallas et al. [12] created                  fear frame\n                                                                                   [1] 5 generic frames\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        6713 articles\na dataset with articles about polemic topics such as immigration,                                                                     articles                   full    article,\n                                                                                                                                                                 the annotator\nsame sex marriage, or smoking, and they defined 15 types of frames:                                                                                              defines     the\n                                                                                                                                                                 main frame.\n\"economic, capacity and resources, morality, fairness and equality,                [50] 5 generic frames + pandemic frames            To label frames of full    Yes/No ques-       2742 articles\nlegality, constitutionality and jurisprudence, policy prescription and             [14] 5 generic frames, journalistic role and\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        131 headlines +\nevaluation, crime and punishment, security and defense, health and                      pandemic frames                               articles                   headline and       subheadlines\n                                                                                                                                                                 subheadline,\nsafety, quality of life, cultural identity, political, external regulation                                                                                       the annotator\n                                                                                                                                                                 defines     the\nand reputation, other\". In this case, they authors did not use a list                                                                                            main frame.\nof questions. Instead, for each article, annotators were asked to\nidentify any of the 15 framing dimensions present in the article\nand to label text blurbs that cued them (based on the definitions of\n                                                                               the potential applications, and the challenges to face when using\neach of the frame dimensions) and decide the main frame of each\n                                                                               this approach. Prompt engineering applications include knowledge\narticle. In our case, we followed the idea of detecting the main frame\n                                                                               probing [46], information extraction [53], NLP reasoning [57], ques-\nby reading the text instead of answering questions, but instead of\n                                                                               tion answering [32], text generation [20], multi-modal learning [58],\nusing the 15 frames proposed in [12] , we used the 5 generic frames\n                                                                               and text classification [24], the latter being the prompt-engineering\nproposed in [52].\n                                                                               use case in our work. Puri et al.[45] presented a very interesting\n   A final decision in our work was the type of text to analyze,\n                                                                               idea that we apply to our classification task. This consists of pro-\nwhether headlines or whole article. For this decision, the chosen\n                                                                               viding the language model with natural language descriptions of\nclassification method was also going to be important. For example,\n                                                                               classification tasks as input, and training it to generate the correct\nKhanehzar et al. [33] used traditional approaches such as SVMs as\n                                                                               answer in natural language via a language modeling objective. It is\nbaseline, and demonstrated the improvement in frame classifica-\n                                                                               a zero-shot learning approach, in which no examples are used to\ntion with the use of pre-trained languages models such as BERT,\n                                                                               explain the task to the model. Radford et al. [48] demonstrated that\nRoBERTa and XLNet, following a fine-tuning approach, setting\n                                                                               language models can learn tasks without any explicit supervision.\nas input text a maximum of 256 tokens (although the maximum\n                                                                               We have followed this approach to find an alternative way to do\nnumber of input tokens in these models is 512 tokens.) Liu et al.\n                                                                               frame analysis.\n[37] classified news headlines about the gun problem in the United\n                                                                                   As mentioned before, the emergence of giant models like GPT-3,\nStates, arguing for the choice of headlines as a unit of analysis\n                                                                               BLOOM, and ChatGPT are a very active research topic. To the best\nbased on previous journalism literature [6], [44], that advocated\n                                                                               of our knowledge, on one hand our work extends the computational\nfor the importance and influence of headlines on readers and the\n                                                                               analysis of news related to the covid-19 no-vax movement, which\nsubsequent perception of articles. From a computational viewpoint,\n                                                                               illustrates the influence of the press on the ways societies think\nusing headlines is also an advantage, since you avoid the 512 token\n                                                                               about relevant issues [40], [59], and on the other hand it adds to\nlimitation in BERT-based models. Therefore, we decided to work\n                                                                               the literature of human-machine interaction, regarding the design\nwith headlines about a controversial issue, namely the Covid-19\n                                                                               of GPT-3 prompts for classification tasks [39], [2].\nno-vax movement.\n   Continuing with the question of the methods used for classi-\nfication, much work has been developed in prompt engineering,\n                                                                               3        DATA: EUROPEAN COVID-19 NEWS\nespecially since the release of GPT-3. Liu et al.[36] presented a good                  DATASET\noverview of the work done on this new NLP paradigm, not only                   We used part of the European Covid-19 News dataset collected in\nexplaining the concept of prompt engineering, but also the differ-             our recent work [3]. This dataset contains 51320 articles on Covid-\nent strategies that can be followed both in the design of prompts,             19 vaccination from 19 newspapers from 5 different countries: Italy,\n\n\n\n\n                                                                         629\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                                                   David Alonso del Barrio and Daniel Gatica-Perez\n\n\nFrance, Spain, Switzerland and UK. The articles cover a time period                                                   [52] with 5 generic frames (attribution of responsibility, human\nof 22 months, from January 2020 to October 2021. All content was                                                      interest, conflict, morality, and economic consequences) plus one\ntranslated into English to be able to work in a common language.                                                      additional \u2019no-frame\u2019 category. Two researchers were engaged to\nThe dataset was used for various analyses, such as name entity                                                        annotate a sample of the collected newspaper articles following a\nrecognition, sentiment analysis, and subtopic modeling, to under-                                                     three-phase training procedure.\nstand how Covid-19 vaccination was reported in Europe through                                                            In the first phase, annotators had to read the codebook and get\nthe print media (in digital format.) The subtopic modeling analysis                                                   familiar with the task. In the second phase, they were asked to\nrevealed a subsample of articles on the no-vax movement, which is                                                     identify the main frame in the same subset of 50 headlines. At the\nthe one we have used in this paper. We took the headlines of the                                                      end of the second phase, the intercoder reliability (ICR) was 0.58\narticles associated with the no-vax movement, selecting all articles                                                  between the 2 annotators. We analyzed those cases where there\ncontaining any of the keywords in Table 2 in the headline or in the                                                   were discrepancies, and observed that in some cases, there was not a\nmain text. This corresponds to a total of 1786 headlines.                                                             unique main frame, because both annotators had valid arguments to\n                                                                                                                      select one of the frames. In other cases, the discrepancies were due\n      Table 2: Keywords used to identify no-vax articles                                                              to slight misunderstanding of the definitions. In the third phase, the\n                                                                                                                      annotators coded again 50 headlines, and the ICR increased to was\n                                                         Keywords                                                     0.66. We realized that the possibility of having two frames remained.\n NO VAX TOPIC    \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"         They discussed the cases in which they had disagreed, and if the\n                                                                                                                      other person\u2019s arguments were considered valid, it could be said that\n   In Table 3, we show the number of headlines per country and                                                        there were two frames. After this three-phase training procedure,\nnewspaper. France is the country with the most no-vax articles in                                                     annotators were ready to annotate the dataset independently. We\nthe corpus, with 523 articles, followed by Italy with 508. However,                                                   divided the dataset into two equal parts, and each person annotated\nnote that there are 6 newspapers from France, while only 2 from                                                       893 headlines.\nItaly. Corriere della Sera is the newspaper that dealt most frequently\nwith the subject (429 articles), while The Telegraph is the second                                                    4.2    Fine-tuning GPT-3.5 and BERT-based\none (206 articles). The total number of articles normalized by the                                                           models\nnumber of newspapers per country is also shown in the last column                                                     With the annotated dataset, we investigated two NLP approaches:\nof the Table. Using these normalized values, the ranking is Italy,                                                    the first one involves fine-tuning a pre-trained model; the second\nUK, France, Switzerland, and Spain.                                                                                   one is prompt engineering. Pre-trained language models have been\n\n Table 3: Number of headlines by newspaper and country\n\n      COUNTRY               NEWSPAPER                HEADLINES             TOTAL (NORM. TOTAL)\n      FRANCE                    La Croix                     94                       523 (87.1)\n                               Le Monde                     125\n                               Les Echos                     49\n                              Liberation                     97\n                            Lyon Capitale                     8\n                            Ouest France                    150\n        ITALY             Corriere della Sera               429                      508 (254.0)\n                            Il Sole 24 Ore                   79\n        SPAIN                 20 minutos                     27                       303 (50.5)\n                                  ABC                        50\n                               El Diario                     32                                                                    Figure 1: Pre-train, fine-tune, prompt\n                               El Mundo                      77\n                              El Espa\u00f1ol                     22\n                           La Vanguardia                     95\n                               24 heures                     97                       230 (76.6)\n   SWITZERLAND\n                              La Libert\u00e9                     22\n                                                                                                                      trained with large text strings based on two unsupervised tasks,\n                               Le Temps                     111                                                       next sentence prediction and masked language model. Figure 1\n                           The Irish News                    16                      222 (111.0)\n UNITED KINGDOM\n                           The Telegraph                    206                                                       summarizes these techniques.\n                                                                                         1786                            In the first approach, a model with a fixed architecture is pre-\n                                                                                                                      trained as a language model (LM), predicting the likelihood of the\n                                                                                                                      observed textual data. This can be done due to the availability of\n4 METHODOLOGY                                                                                                         large, raw text data needed to train LMs. This learning process can\n                                                                                                                      produce general purpose features of the modeled language. The\n4.1 Human labeling of news frames                                                                                     learning process produces robust, general-purpose features of the\nTo carry out the labeling of the frames in our corpus of headlines, we                                                language being modeled. The above pre-trained LM is then adapted\nfirst designed a codebook, which contained the definitions of each                                                    to different downstream tasks, by introducing additional parameters\nof the frame types and a couple of examples of each type, as well                                                     and adjusting them using task-specific objective functions. In this\nas a definition of the corpus subject matter and definitions of the                                                   approach, the focus was primarily on goal engineering, designing\nconcept of frame analysis, so that the annotators could understand                                                    the training targets used in both the pre-training and the fine-tuning\nthe task to be performed. The codebook follows the proposed by                                                        stages [36].\n\n\n\n\n                                                                                                                630\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                    ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n    We present an example to illustrate the idea. Imagine that the        adjective was matched with a sentiment. In a similar fashion, we\ntask is sentiment analysis, and we have a dataset with sentences          decided to build a thesaurus of adjectives that define each of the\nand their associated sentiment, and a pre-trained model, which is a       frames. For instance, the human interest frame could be \u2019interest-\nsaved neural network trained with a much larger dataset. For that         ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be:\npre-trained model to address the target task, we unfreeze a few of        \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou-\nthe top layers of the saved model base and jointly train both the         blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined,\nnewly-added classifier layers and the last layers of the base model.      we needed to define the prompt in order to get, as an answer, one\nThis allows to \"fine-tune\" the higher-order feature representations       of the adjectives in our thesaurus to match them with the frame.\nin the base model to make them more relevant for the sentiment            We used the GPT-3 playground using the headline as input and\nanalysis task. In this way, instead of having to obtain a very large      asking for the frame as output, but the strategy did not work. In\ndataset with target labels to train a model, we can reuse the pre-        our final experiment, instead of giving the headline as input, we\ntrained model and use a much smaller train dataset. We use a part         gave the definitions of each type of frame plus the headline, and we\nof our dataset as examples for the model to learn the task, while         asked the model to choose between the different types of frames\nthe other part of the dataset is used to evaluate model performance.      as output. In this way, the output of the model was directly one of\n    Previous works related to frame classification in the computing       the frames, and we avoided the step of matching adjectives with\nliterature have used fine-tuning, BERT-based models. In our work,         frames. An example is shown in Figure 2.\nwe have done the same as a baseline, but we aimed to go one step\nfurther and also produce results using fine-tuning of GPT-3.5.\n\n\n4.3    Prompt-engineering with GPT-3.5\nModel fine-tuning has been widely used, but with the emergence\nof generative models such as GPT-3, another way to approach\nclassification tasks has appeared. The idea is to use the pre-trained\nmodel directly and convert the task to be performed into a format\nas close as possible to the tasks for which it has been pre-trained.\nThat is, if the model has been pre-trained from next word prediction          Figure 2: GPT-3.5 for frame inference: input and output\nas in the case of GPT-3, classification can be done by defining a\nprompt, where the input to the model is an incomplete sentence,               For the GPT-3 configuration 1 , there are 3 main concepts:\nand the model must complete it with a word or several words, just\n                                                                                \u2022 TEMPERATURE [0-1]. This parameter controls randomness,\nas it has been trained. This avoids having to use part of the already\n                                                                                  lowering it results in less random completions.\nlabeled dataset to teach the task to be performed to the model, and\n                                                                                \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus\na previous labeling is not needed [36].\n                                                                                  sampling.\n    In this approach, instead of adapting pre-trained LMs to down-\n                                                                                \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-\nstream tasks via objective engineering, downstream tasks are re-\n                                                                                  mum number of tokens to generate,\nformulated to look more like those solved during the original LM\n                                                                                \u2022 MODEL. GPT-3 offer four main models with different levels\ntraining with the help of a textual prompt. For example, when recog-\n                                                                                  of power, suitable for different tasks. Davinci is the most\nnizing the emotion of a social media post, \u201cI missed the bus today.\u201d,\n                                                                                  capable model, and Ada is the fastest.\nwe may continue with a prompt \u201cI felt so _\u201d, and ask the LM to\nfill the blank with an emotion-bearing word. Or if we choose the          After testing with the GPT-3 playground and varying different\nprompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may           hyper-parameters to assess performance, we set the temperature to\nbe able to fill in the blank with a French translation. In this way,      0, since the higher the temperature the more random the response.\nby selecting the appropriate prompts, we can influence the model          Furthermore, the Top-p parameter was set to 1, as it would likely\nbehavior so that the pre-trained LM itself can be used to predict the     get a set of the most likely words for the model to choose from. The\ndesired output, even without any additional task-specific training        maximum number of tokens was set to 2; in this way, the model\n[36].                                                                     is asked to choose between one of the responses. As a model, we\n    We use this emerging NLP approach to classify frames at headline      used the one with the best performance at the time of experimental\nlevel. We are not aware of previous uses of this strategy to classify     design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\nframes as we propose here. The idea is the following. Prompt engi-\nneering consists of giving a prompt to the model, and understands         5     RESULTS: HUMAN LABELING OF FRAMES\nthat prompt as an incomplete sentence. To do prompt engineer-                   IN NO-VAX NEWS HEADLINES (RQ1)\ning with our dataset, we needed to define an appropriate prompt           In this section, we present and discuss the results of the analysis\nthat would produce the headline frames as output. We defined sev-         related to our first RQ.\neral experiments with the Playground of GPT-3, in order to find              Figure 3 shows the distribution of frames per country at headline\nthe best prompt for our task. In our initial experiments, we fol-         level, with human interest and no-frame being the predominant\nlowed existing approaches in prompt engineering to do sentiment\nanalysis, where the individual answer was an adjective, and this          1 https://beta.openai.com/docs/introduction\n\n\n\n\n                                                                    631\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                       David Alonso del Barrio and Daniel Gatica-Perez\n\n\nones. Attribution of responsibility is the third one except in Switzer-\nland, where the corresponding frame is conflict. Finally, morality\nand economic are the least represented in the dataset for every\ncountry.\n\n\n\n\n                                                                            Figure 4: Non-normalized monthly distribution of frames.\n\n\n\n                                                                            detail the negative and positive sentiment of each frame category,\n                                                                            we observed a few trends:\n                                                                                \u2022 Attribution of responsibility: Negative sentiment represents\n                                                                                  30-40% of the cases, while positive tone is only found in\n                                                                                  residual form in Italy, Switzerland, and the United Kingdom.\n                                                                                \u2022 Conflict: Negative sentiment represents 20-35% of the cases.\nFigure 3: Non-normalized distribution of frames per country                     \u2022 Economic: Predominantly neutral, with only negative tone\n                                                                                  in Italy and UK (in the latter case, all headlines with this\n    The monthly distribution of frames aggregated for all countries               frame were considered negative.)\nis shown in Fig. 4. We can see two big peaks, the first one in January          \u2022 Human interest: Negative sentiment represents 30-40% of\n2021 and the second one in August 2021. In all countries, the vac-                the cases, while positive tone is only found in residual form\ncination process started at the end of December 2020, so it makes                 in Italy, Spain, and Switzerland.\nsense that the no-vax movement started to be more predominant in                \u2022 Morality: Predominantly neutral, with negative tone in Italy,\nthe news in January 2021. Human interest is the most predominant                  Switzerland, and the United Kingdom,\nframe. Manual inspection shows that this is because the headlines               \u2022 No frame: 20-30% of negative content.\nare about personal cases of people who are pro- or anti- vaccine.\nAttribution of responsibility is also present. Manual inspection in-\ndicates that local politicians and health authorities had to make\ndecisions about who could be vaccinated at the beginning of the\nprocess. The second peak at the end of summer 2021 coincided\nwith the health pass (also called Covid passport in some countries),\nand we can observe a peak in the curve corresponding to the con-\nflict frame, reflecting the demonstrations against the measure of\nmandatory health passes taken by country governments.\n    In Figure 5, we compare the sentiment per frame and per country,\nto understand if there were any major differences. The sentiment\nanalysis labels were obtained using BERT-sent from the Hugging\nFace package [47], used in our previous work (please refer to our\noriginal analysis in [3] for details.) We normalized the results be-\ntween 0 and 1 to compare frames between countries. We see that the\nsentiment is predominantly neutral (in blue). Examining in more              Figure 5: Sentiment of headline by frame and by country\n\n\n\n\n                                                                      632\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                   ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n   Regarding the results of the annotation process, the fact that the             On the other hand, BERT is open-source, while GPT-3 has an\ndistribution of the 6 frame types is relatively similar between coun-         economic cost as the use of the model is not free, which monetarily\ntries suggests that the anti-vaccine movement issue was treated               limits the number of experiments that can be performed with it,\nin a similar way in these countries. The fact that human interest             as well as the different configurations one can explore to improve\nis the most dominant frame indicates that this issue was treated              performance. This is important because much of the improvement\nfrom a more human and emotional approach, with headlines about                in performance requires empirical explorations of model parameters\npersonal experiences, celebrities giving their opinion about vacci-           More specifically, the cost of an experiment for each of the folds has\nnation, and politicians defending vaccine policies. Moreover, the             a cost of 4 dollars (at the time of writing this paper.) This represents\nreason for many headlines being classified as no-frame is partly              a limitation in practice.\ndue to how data was selected. We chose articles that contained                    Furthermore, GPT-3 has a significant carbon footprint. Similarly,\nwords related to no-vax, either in the headline or in the article. This       for prompt engineering (discussed in the next subsection), choosing\nresulted in many headlines not containing anything specific related           the right prompt (i.e., the words that best define the task so that the\nto no-vax, while the no-vax content was actually included in the              model is able to perform adequately) is also based on trial and error.\nmain text of the corresponding articles.                                      This also has an impact on carbon footprint. In connection with\n   It is worth mentioning that prior to obtaining the results, we had         this topic, Strubell et al.[55] argue that improvements in the accu-\nexpected that attribution of responsibility would be among the most           racy of models depend on the availability of large computational\nprominent frames, since governments took many measures such as                resources, which involve large economic and environmental costs.\nmandatory health pass requirements to access certain sites; we had            A criticism has been made as \u2019the rich get richer\u2019, in the sense that\nalso expected that the conflict frame would be prominent, since               not all research groups have sufficient infrastructure resources and\nthere were many demonstrations in Europe. In reality, however,                access to funding needed to use these models and improve their\nthese frames categories were not reflected as frequently at the               performance. Also in relation to this analysis, the work of Bender\nheadline level.                                                               et al. [4] evaluates the costs and risks of the use of large language\n   Regarding the analysis at the temporal level, it is clear that certain     models, stating that researchers should be aware of the impact that\nevents were captured by the press, such as the start of vaccination           these models have on the environment, and assess whether the\nor the mandatory vaccination passport.                                        benefits outweigh the risks. The work in [4] provides a very telling\n   Finally, the sentiment analysis of the different frames shows that         example, where people living in the Maldives or Sudan are affected\nthe predominant tone in all of them is neutral or negative, with very         by floods and pay the environmental price of training English LLMs,\nsimilar trends between countries. This association between senti-             when similar models have not been produced for languages like\nment analysis and frames has been discussed in previous literature            Dhivehi or Sudanese Arab. In short, there is a need to establish\n[11] [43].                                                                    ways to use this technological development responsibly, and it all\n                                                                              starts with being aware of the risks it presents.\n6     RESULTS: GPT-3.5 FOR FRAME\n      CLASSIFICATION OF HEADLINES (RQ2)                                       6.2    Prompt-engineering with GPT-3.5\nHere, we present and discuss the results related to our second RQ.            For each headline, we got the frame that the model considered the\n                                                                              most likely, and we compared these GPT-3.5 inferences with the\n6.1    Fine-tuning GPT-3.5                                                    frames labeled by the annotators. The agreement between model\nTable 4 shows the results of the 6-class classification task using            and annotator was of 49%. Analyzing the results, and specifically\n5-cross validation. Three models were used: GPT-3.5 and two BERT-             looking at the cases where the annotator and GPT-3.5 disagreed,\nbased models. We observe that, on average, GPT-3.5 performs better            we discovered that according to the frame definitions, the model\nthan the BERT-based models. This is somehow expected as GPT-                  in some cases proposed a frame that indeed made sense. This ob-\n3.5 is a much larger model. Overall, in the case of fine-tuning, the          servation, together with our previous experience in the annotation\nbest performance for the six-class frame classification task is 72%           process, where headlines could have more than one valid frame,\naccuracy, which is promising, with an improvement over previous               led us to design a second post-hoc experiment. We took all the\nmodels based on BERT. Yet, it should be noted that the performance            headlines where each of the two annotators had disagreed with\ndifferences are modest (2% improvement between GPT-3.5 and                    GPT-3.5, and we asked the annotators to state whether they would\nRoBERTa).                                                                     agree (or not) with each GPT-inferred label for a given headline.\n                                                                              It is important to emphasize that the annotators did not know the\n                                                                              origin of that label, i.e., they did not know if it was the label they\nTable 4: Classification results for six-class frame classifica-               had originally assigned, or if it was a random one. In this way, we\ntion and 5-fold cross validation                                              could quantify how GPT-3.5 worked according to valid arguments\n                                                                              provided by the annotators. In this post-hoc experiment, the model\n                                                                              agreed in 76% of cases with the annotators.\n    ACCURACY          0       1       2       3       4     AVERAGE\n                                                                                  Looking at the results of the classification models, the 49% accu-\n    BERT            0.68     0.69    0.72    0.64    0.70     0.67            racy of the prompt-engineering approach can be considered low,\n    RoBERTa         0.70     0.72    0.72    0.67    0.71     0.70\n                                                                              yet we consider that it is a valid avenue for further investigation,\n    GPT3            0.75     0.70    0.72    0.71    0.71     0.72\n                                                                              as in the second post-hoc analysis, we found that the model agrees\n\n\n\n\n                                                                        633\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                David Alonso del Barrio and Daniel Gatica-Perez\n\n\nwith human annotators in 76% of the cases. Clearly, framing in-             and in an post-doc experiment, the human-machine agreement\nvolves aspects of subjectivity [42]. Much of what we do as people           reached 76%. These results have opened several new directions for\nhas a subjective component, influenced by how we feel or how we             future work.\nexpress opinions.\n   News reading is never fully objective, and the annotators en-            ACKNOWLEDGMENTS\ngaged in the frame classification task, influenced by their personal        This work was supported by the AI4Media project, funded by the\nstate of mind, experience, and culture, may perceive information            European Commission (Grant 951911) under the H2020 Programme\ndifferently. Monarch affirms that \"for simple tasks, like binary labels     ICT-48-2020. We also thank the newspapers for sharing their online\non objective tasks, the statistics are fairly straightforward to decide     articles. Finally, we thank our colleagues Haeeun Kim and Emma\nwhich is the \u2018correct\u2019 label when different annotators disagree. But        Bouton-Bessac for their support with annotations, and Victor Bros\nfor subjective tasks, or even objective tasks with continuous data,         and Oleksii Polegkyi for discussions.\nthere are no simple heuristics for deciding what the correct label\nshould be\" [42].                                                            REFERENCES\n   Subjectivity is involved in both the generation and perception            [1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online\nof information: the assumption that there is only one frame is com-              media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.\n                                                                                 Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.\nplicated by the point of view of the reader. In the case of news, the        [2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess\ninformation sender (the journalist) has an intention, but the receiver           Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021.\n(the reader) plays a role and is influenced by it. In psychology, this           RAFT: A real-world few-shot text classification benchmark. arXiv preprint\n                                                                                 arXiv:2109.14076 (2021).\nis known as the lens model of interpersonal communication, where             [3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press\nthe sender has certain objectives, but the receiver can interpret                Cover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35\u201343.\n                                                                                 https://doi.org/10.1145/3512732.3533588\nor re-interpret what the sender wants to say, with more or less              [4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\naccuracy [26].                                                                   Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\n   Following this discussion on subjectivity, the question arose as to           Be Too Big? (2021), 610\u2013623.\n                                                                             [5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in\nwhat would happen if, instead of headlines, we used the complete                 journalism: A boon or bane? In Optimization in machine learning and applications.\narticle as a source of analysis. We wondered if longer text could                Springer, 155\u2013167.\nmake the frame labeling task clearer than when using headlines.              [6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015.\n                                                                                 Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013\nYet another possible hypothesis is that having to read longer texts              2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.\ncould lead to the same subject being presented from different angles.        [7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.\n                                                                                 https://doi.org/10.48550/ARXIV.2212.14402\nPlease recall that in the existing literature discussed in Section 2,        [8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe,\nboth headlines and full articles have been used from frame analysis              Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour-\n(see Table 1.) This remains as an issue for future work.                         nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695.\n                                                                             [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                 Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                 Askell, et al. 2020. Language models are few-shot learners. Advances in neural\n7    CONCLUSIONS                                                                 information processing systems 33 (2020), 1877\u20131901.\n                                                                            [10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H\nIn this paper, we first presented an analysis of human-generated                 De Vreese. 2014. Teaching the computer to code frames in news: Comparing\nnews frames on the covid-19 no-vax movement in Europe, and                       two supervised machine learning approaches to frame analysis. Communication\n                                                                                 Methods and Measures 8, 3 (2014), 190\u2013206.\nthen studied different approaches using large language models for           [11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond\nautomatic inference of frames. We conclude by answering the two                  words: Applying cluster and sentiment analysis to news coverage of the nuclear\nresearch questions we posed:                                                     power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.\n                                                                            [12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015.\n   RQ1: What are the main frames in the news headlines about the                 The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015),\ncovid-19 anti-vaccine movement in 5 European countries? After                    438\u2013444. https://doi.org/10.3115/v1/P15-2072\nannotating the headlines, we found that of the 1786 headlines,              [13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age\n                                                                                 of coronavirus and fake news: analysis of journalistic sources in the Spanish\nthe predominant frame is human interest (45.3% of cases), which                  quality press. International Journal of Environmental Research and Public Health\npresents a news item with an emotional angle, putting a face to a                17, 21 (2020), 8136.\n                                                                            [14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust\nproblem or situation. We also found that a substantial proportion                of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n\nof headlines were annotated as not presenting any frame (40.2% of                social 74 (2019), 786\u2013802.\ncases). Finally, the other frame types are found more infrequently.         [15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology\n                                                                                 for evaluating data journalism, computational journalism, and computer-assisted\n   RQ2: Can prompt engineering be used for classification of head-               reporting. Digital journalism 3, 3 (2015), 331\u2013348.\nlines according to frames? We first used fine-tuning of a number of         [16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News\nlanguage models, and found that GPT-3.5 produced classification ac-              Framing Analysis. Routledge, 151\u2013172.\n                                                                            [17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,\ncuracy of 72% on a six-frame classification task. This represented a             1 (2021), 113\u2013118.\nmodest 2% improvement over BERT-based models, at a significantly            [18] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A de-\n                                                                                 ductive frame-analysis of Dutch and French climate change coverage dur-\nlarger environmental cost. We then presented a new way of classi-                ing the annual UN Conferences of the Parties. Public Understanding of\nfying frames using prompts. At the headline level, inferences made               Science 19, 6 (2010), 732\u2013742.         https://doi.org/10.1177/0963662509352044\nwith GPT-3.5 reached 49% of agreement with human-generated                       arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.\n                                                                            [19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame-\nframe labels. In many cases, the GPT-3.5 model inferred frame                    analysis of Dutch and French climate change coverage during the annual UN\ntypes that were considered as valid choices by human annotators,                 Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.\n\n\n\n\n                                                                      634\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                        ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig.             [46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with\n     2020. Gsum: A general framework for guided neural abstractive summarization.                 mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).\n     arXiv preprint arXiv:2010.08014 (2020).                                                 [47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent.\n[21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news                https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.\n     headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA            [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\n     Gesondheid (Online) 27 (2022), 1\u20138.                                                          et al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n[22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative                      1, 8 (2019), 9.\n     Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).                 [49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022.\n[23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.               Fake News Classification using transformer based enhanced LSTM and BERT.\n     McQuail\u2019s reader in mass communication theory 390 (1993), 397.                               International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105.\n[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language                    https://doi.org/10.1016/j.ijcce.2022.03.003\n     models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).                [50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational\n[25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across                    predictors. Cuadernos. info 50 (2021), 91\u2013112.\n     four nations: a topic modeling and sentiment analysis approach. Ieee Access 9           [51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\n     (2021), 36645\u201336656.                                                                         Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\n[26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En-                     Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language\n     coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior.                     model. arXiv preprint arXiv:2211.05100 (2022).\n     Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https:              [52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content\n     //doi.org/10.1037//0022-3514.66.2.398                                                        Analysis of Press and Television News. Journal of Communication 50 (06 2000),\n[27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based                 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x\n     transformer architectures for long document summarization. In Proceedings of            [53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em-\n     the 16th Conference of the European Chapter of the Association for Computational             manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin\n     Linguistics: Main Volume. 1792\u20131810.                                                         Van Durme. 2021. Constrained language models yield few-shot semantic parsers.\n[28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news                      arXiv preprint arXiv:2104.08768 (2021).\n     summarization using transformers. In Sustainable Advanced Computing. Springer,          [54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab-\n     249\u2013259.                                                                                     orative Work trends on Media Organizations: Mixing Qualitative and Quan-\n[29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar-                titative Approaches. Studies in Media and Communication 5 (04 2017), 63.\n     chy of hybridity in data and computational journalism. Digital Journalism 5, 2               https://doi.org/10.11114/smc.v5i1.2279\n     (2017), 159\u2013176.                                                                        [55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy\n[30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and                 considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n     text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021),     [56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\n     300\u2013324.                                                                                     the capabilities, limitations, and societal impact of large language models. arXiv\n[31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative                  preprint arXiv:2102.02503 (2021).\n     analysis of large amounts of journalistic texts using topic modelling. Digital          [57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.\n     journalism 4, 1 (2016), 89\u2013106.                                                              arXiv preprint arXiv:1806.02847 (2018).\n[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we              [58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and\n     know what language models know? Transactions of the Association for Computa-                 Felix Hill. 2021. Multimodal few-shot learning with frozen language models.\n     tional Linguistics 8 (2020), 423\u2013438.                                                        Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.\n[33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling                   [59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of\n     Political Framing Across Policy Issues and Contexts. In ALTA.                                technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.\n[34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate             [60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling\n     during election years: Focus on generic frames. The Communication Review 21, 2               for frame analysis: A study of media debates on climate change in India and USA.\n     (2018), 89\u2013115.                                                                              Global Media and Communication 18, 1 (2022), 91\u2013112.\n[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\n     hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n     2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n     (2022).\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\n     Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of\n     prompting methods in natural language processing. (2021). https://doi.org/10.\n     48550/ARXIV.2107.13586\n[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019.\n     Detecting frames in news headlines and its application to analyzing news framing\n     trends surrounding US gun violence. In Proceedings of the 23rd conference on\n     computational natural language learning (CoNLL).\n[38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames:\n     Toward Improving Reliability and Validity. Journal of Communication 58 (06\n     2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x\n[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and\n     David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3\n     User Simulation in Conversational AI. In Proceedings of the 4th Conference on\n     Conversational User Interfaces. 1\u20136.\n[40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social\n     computing for verifying social media content in breaking news. IEEE Internet\n     Computing 22, 2 (2018), 83\u201389.\n[41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021. \u2018Our task is to demystify fears\u2019:\n     Analysing newsroom management of automation in journalism. Journalism 22,\n     9 (2021), 2203\u20132221.\n[42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and\n     Annotation for Human-centered AI. Manning. https://books.google.ch/books?\n     id=LCh0zQEACAAJ\n[43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of\n     media frames: Strengths, weaknesses, and opportunities. Political Communication\n     38, 1-2 (2021), 159\u2013181.\n[44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to\n     news discourse. Political communication 10, 1 (1993), 55\u201375.\n[45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative\n     language models. arXiv preprint arXiv:1912.10165 (2019).\n\n\n\n\n                                                                                       635\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).", "Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.", "Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021)."], "publish_date": "2023-06-12", "approved": false}}, {"_index": "articles", "_id": "eykxcI0Bf-PcuVTALVJE", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "Large Language Model Augmented Narrative Driven Recommendations", "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.", "authors": ["Sheshera Mysore", "Andrew McCallum", "Hamed Zamani"], "institutions": ["University of Massachusetts Amherst"], "keywords": ["narrative-driven recommendation", "information access", "user-item interaction", "data augmentation", "large language models"], "full_text": "                 Large Language Model Augmented Narrative Driven\n                                Recommendations\n                  Sheshera Mysore                                              Andrew McCallum                                              Hamed Zamani\n          smysore@cs.umass.edu                                           mccallum@cs.umass.edu                                       hzamani@cs.umass.edu\n    University of Massachusetts Amherst                             University of Massachusetts Amherst                        University of Massachusetts Amherst\n                    USA                                                             USA                                                        USA\nABSTRACT                                                                                              interactions are effective, users soliciting recommendations often\nNarrative-driven recommendation (NDR) presents an information                                         start with a vague idea about their desired target items or may\naccess problem where users solicit recommendations with verbose                                       desire recommendations depending on the context of use, often\ndescriptions of their preferences and context, for example, travelers                                 missing in historical interaction data (Figure 1). In these scenarios,\nsoliciting recommendations for points of interest while describ-                                      it is common for users to solicit recommendations through long-\ning their likes/dislikes and travel circumstances. These requests                                     form narrative queries describing their broad interests and context.\nare increasingly important with the rise of natural language-based                                    Information access tasks like these have been studied as narrative-\nconversational interfaces for search and recommendation systems.                                      driven recommendations (NDR) for items ranging from books [5]\nHowever, NDR lacks abundant training data for models, and current                                     and movies [18], to points of interest [1]. Bogers and Koolen [5]\nplatforms commonly do not support these requests. Fortunately,                                        note these narrative requests to be common on discussion forums\nclassical user-item interaction datasets contain rich textual data,                                   and several subreddits1 , but, there is a lack of support for these\ne.g., reviews, which often describe user preferences and context                                      complex natural language queries in current recommenders.\n\u2013 this may be used to bootstrap training for NDR models. In this                                          However, with the emergence of conversational interfaces for\nwork, we explore using large language models (LLMs) for data                                          information access tasks, support for complex NDR tasks is likely\naugmentation to train NDR models. We use LLMs for authoring                                           to become necessary. In this context, recent work has noted an\nsynthetic narrative queries from user-item interactions with few-                                     increase in complex and subjective natural language requests com-\nshot prompting and train retrieval models for NDR on synthetic                                        pared to more conventional search interfaces [13, 34]. Furthermore,\nqueries and user-item interaction data. Our experiments demon-                                        the emergence of large language models (LLM) with strong lan-\nstrate that this is an effective strategy for training small-parameter                                guage understanding capabilities presents the potential for fulfilling\nretrieval models that outperform other retrieval and LLM baselines                                    such complex requests [9, 33]. This work explores the potential for\nfor narrative-driven recommendation.                                                                  re-purposing historical user-item recommendation datasets, tra-\n                                                                                                      ditionally used for training collaborative filtering recommenders,\nCCS CONCEPTS                                                                                          with LLMs to support NDR.\n                                                                                                          Specifically, given a user\u2019s interactions, \ud835\udc37\ud835\udc62 , with items and\n\u2022 Information systems \u2192 Recommender systems; Users and inter-\n                                                                                                      their accompanying text documents (e.g., reviews, descriptions)\nactive retrieval; \u2022 Computing methodologies \u2192 Natural language                                                   \ud835\udc41\ud835\udc62\ngeneration.                                                                                           \ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56 }\ud835\udc56=1  , selected from a user-item interaction dataset I, we\n                                                                                                      prompt InstructGPT, a 175B parameter LLM, to author a synthetic\nACM Reference Format:                                                                                 narrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the\nSheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large\n                                                                                                      query \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user\nLanguage Model Augmented Narrative Driven Recommendations. In Sev-\nenteenth ACM Conference on Recommender Systems (RecSys \u201923), Septem-\n                                                                                                      reviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based\nber 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages.                               on a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user doc-\nhttps://doi.org/10.1145/3604915.3608829                                                               ument, \ud835\udc51\ud835\udc56 . Then, a pre-trained LM based retrieval model (110M\n                                                                                                      parameters) is fine-tuned for retrieval on the synthetic queries and\n1     INTRODUCTION                                                                                    filtered reviews.\n                                                                                                          Our approach, which we refer to as Mint2 , follows from the\nRecommender systems personalized to users are an important com-\n                                                                                                      observation that while narrative queries and suggestions are often\nponent of several industry-scale platforms [16, 17, 46]. These sys-\n                                                                                                      made in online discussion forums, and could serve as training data,\ntems function by inferring users\u2019 interests from their prior inter-\n                                                                                                      the number of these posts and the diversity of domains for which\nactions on the platform and making recommendations based on\n                                                                                                      they are available is significantly smaller than the size and diversity\nthese inferred interests. While recommendations based on historical\n                                                                                                      of passively gathered user-item interaction datasets. E.g. while\nPermission to make digital or hard copies of all or part of this work for personal or                 Bogers and Koolen [5] note nearly 25,000 narrative requests for\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation             books on the LibraryThing discussion forum, a publicly available\non the first page. Copyrights for components of this work owned by others than the                    user-item interaction dataset for Goodreads contains interactions\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\n                                                                                                      with nearly 2.2M books by 460k users [43] .\nand/or a fee. Request permissions from permissions@acm.org.                                               We empirically evaluate Mint in a publicly available test collec-\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                               tion for point of interest recommendation: pointrec [1]. To train\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n                                                                                                      1 r/MovieSuggestions, r/booksuggestions, r/Animesuggest\nACM ISBN 979-8-4007-0241-9/23/09. . . $15.00\nhttps://doi.org/10.1145/3604915.3608829                                                               2 Mint: Data augMentation with INteraction narraTives.\n\n\n\n\n                                                                                                777\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                  Mysore, McCallum, Zamani\n\n\n\n\nFigure 1: An example narrative query soliciting point of                      Figure 2: The format of the prompt used in Mint for\ninterest recommendations. The query describes the users                       generating synthetic narrative queries from user-item\npreferences and the context of their request.                                 interaction with a large language model.\n\n\nour NDR models, we generate synthetic training data based on                  Besides creating queries for ad-hoc retrieval tasks, concurrent\nuser-item interaction datasets from Yelp. Models (110M parameters)         work of Leszczynski et al. [25] has also explored the creation of syn-\ntrained with Mint significantly outperform several baseline models         thetic conversational search datasets from music recommendation\nand match the performance of significantly larger LLM baselines            datasets with LLMs. The synthetic queries and user documents are\nautoregressively generating recommendations. Code and synthetic            then used to train bi-encoder retrieval models for conversational\ndatasets are available:3                                                   search. Our work resembles this in creating synthetic queries from\n                                                                           sets of user items found in recommendation interaction datasets.\n2    RELATED WORK                                                          However, it differs in the task of focus, creating long-form narra-\nData Augmentation for Information Access. A line of recent                 tive queries for NDR. Finally, our work also builds on the recent\nwork has explored using language models to generate synthetic              perspective of Radlinski et al. [36] who make a case for natural\nqueries for data augmentation to train models for information re-          language user profiles driving recommenders \u2013 narrative requests\ntrieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of     tie closely to natural language user profiles. Our work presents a\ninterest, a pre-trained language model is used to create synthetic         step toward these systems.\nqueries for the document collection. An optional filtering step ex-           Finally, while our work explores data augmentation from user-\ncludes noisy queries, and finally, a bi-encoder or a cross-encoder is      item interactions for a retrieval-oriented NDR task, prior work has\ntrained for the retrieval task. While earlier work of Ma et al. [31]       also explored data augmentation of the user-item graph for training\ntrain a custom query generation model on web-text datasets, more           collaborative filtering models. This work has often explored aug-\nrecent work has leveraged large language models for zero/few-shot          mentation to improve recommendation performance for minority\nquestion generation [7, 8, 15, 23]. In generating synthetic queries,       [12, 47] or cold-start users [11, 28, 45]. And has leveraged genera-\nthis work indicates the effectiveness of smaller parameter LLMs            tive models [11, 45] and text similarity models [28] for augmenting\n(up to 6B parameters) for generating synthetic queries in simpler          the user-item graph.\ninformation-retrieval tasks [7, 8, 23], and finds larger models (100B         Complex Queries in Information Access. With the advent\nparameters and above) to be necessary for harder tasks such as             of performant models for text understanding, focus on complex\nargument retrieval [15, 23]. Similar to this work, we explore the          and interactive information access tasks has seen a resurgence\ngeneration of synthetic queries with LLMs for a retrieval task. Un-        [2, 29, 32, 48]. NDR presents an example of this \u2013 NDR was first\nlike this work, we demonstrate a data augmentation method for              formalized in Bogers and Koolen [5] for the case of book recommen-\ncreating effective training data from sets of user documents found in      dation and subsequently studied in other domains [3, 4, 6]. Bogers\nrecommendation datasets rather than individual documents. Other            and Koolen [5] systematically examined narrative requests posted\nwork in this space has also explored training more efficient multi-        by users on discussion forums. They defined NDR as a task requir-\nvector models from synthetic queries instead of more expensive             ing item recommendation based on a long-form narrative query\ncross-encoder models [39] and generating queries with a diverse            and prior-user item interactions. While this formulation resembles\nrange of intents than the ones available in implicit feedback datasets     personalized search [42] and query-driven recommendation [20],\nto enhance item retrievability [35].                                       the length and complexity of requests differentiate these from NDR.\n                                                                           Other work has also demonstrated the effectiveness of re-ranking\n                                                                           initial recommendations from collaborative filtering approaches\n3 https://github.com/iesl/narrative-driven-rec-mint/\n\n\n\n\n                                                                     778\n\fLarge Language Model Augmented Narrative Driven Recommendations                                          RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\n\n\nFigure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering\nmodels for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with\na large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic\nqueries and user items.\n\n\nbased on the narrative query [18]. More recent work of Afzali et al.            3.2.1 Narrative Queries from LLMs. To author a narrative query \ud835\udc5e\ud835\udc62\n[1] formulate the NDR task without access to the prior interactions             for a user in I, we make use of the 175B parameter InstructGPT4\nof a user while also noting the value of contextual cues contained              model as our query generation model QGen. We include the text\n                                                                                                         \ud835\udc41\ud835\udc62\nin the narrative request. In our work, we focus on this latter for-             of interacted items {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 in the prompt for QGen, and instruct it\nmulation of NDR, given the lack of focus on effectively using the               to author a narrative query (Figure 2). To improve the coherence\nrich narrative queries in most prior work. Further, we demonstrate              of generated queries and obtain correctly formatted outputs, we\nthe usefulness of data augmentation from LLMs and user-item                     manually author narrative queries for 3 topically diverse users\ninteraction datasets lacking narrative queries.                                 based on their interacted items and include it in the prompt for\n   Besides this, a range of work has explored more complex, long-               QGen. The same three few shot examples are used for the whole\nform, and interactive query formulations for information access;                dataset I, and the three users were chosen from I. Generating\nthese resemble queries in NDR. Arguello et al. [2] define the tip of            narrative queries based on user interactions may also be considered\ntongue retrieval task, a known-item search task where user queries              a form of multi-document summarization for generating a natural\ndescribe the rich context of items while being unable to recall item            language user profile [36].\nmetadata itself. Mysore et al. [32] formulate an aspect conditional\nquery-by example task where results must match specific aspects of              3.2.2 Filtering Items for Synthetic Queries. Since we expect user\na long natural language query. And finally, a vibrant body of work              items to capture multiple aspects of their interests and generated\nhas explored conversational critiquing of recommenders where nat-               queries to only capture a subset of these interests, we only retain\n                                                                                                                    \ud835\udc41\ud835\udc62\nural language feedback helps tune the recommendations received                  some of the items present in {\ud835\udc51\ud835\udc56 }\ud835\udc56=1   before using it for training re-\nby users [30, 44, 49].                                                          trieval models. For this, we use a pre-trained language model to com-\n                                                                                pute the likelihood of the query given each user item, \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ),\n3 METHOD                                                                        and only retain the top \ud835\udc40 highly scoring item for \ud835\udc5e\ud835\udc62 , this re-\n                                                                                sults in \ud835\udc40 training samples per user for our NDR retrieval models:\n3.1 Problem Setup                                                                          \ud835\udc40 }. In our experiments, we use FlanT5 with 3B parame-\n                                                                                {(\ud835\udc5e\ud835\udc62 , \ud835\udc51\ud835\udc56 )\ud835\udc56=1\nIn our work, we define narrative-driven recommendation (NDR) to                 ters [14] for computing and follow Sachan et al. [40] for computing\nbe a ranking task, where given a narrative query \ud835\udc5e made by a user               \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ). Note that our use of \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ) represents a query-\n\ud835\udc62, a ranking system \ud835\udc53 must generate a ranking \ud835\udc45 over a collection               likelihood model classically used for ad-hoc search and recently\nof items C. Further, we assume access to a user-item interaction                shown to be an effective unsupervised re-ranking method when\n                                                              \ud835\udc41\ud835\udc62\ndataset I consisting of user interactions with items (\ud835\udc62, {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 ). We         used with large pre-trained language models [40].\nassume the items \ud835\udc51\ud835\udc56 to be textual documents like reviews or item\ndescriptions. While we don\u2019t assume there to be any overlap in the              3.2.3 Training Retrieval Models. We train bi-encoder and cross-\nusers making narrative queries or the collection of items C and the             encoder models for NDR on the generated synthetic dataset \u2013 com-\nuser-items interaction dataset I, we assume them to be from the                 monly used models in search tasks. Bi-encoders are commonly used\nsame broad domain, e.g., books, movies, points-of-interest.                     as scalable first-stage rankers from a large collection of items. On the\n                                                                                other hand, cross-encoders allow a richer interaction between query\n3.2    Proposed Method                                                          and item and are used as second-stage re-ranking models. For both\n                                                                                models, we use a pre-trained transformer language model architec-\nOur proposed method, Mint, for NDR, re-purposes a dataset of                    ture with 110M parameters, MPnet, a model similar to Bert [41].\n                                                           \ud835\udc41\ud835\udc62\nabundantly available user-item interactions, I = {(\ud835\udc62, {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 )} into          Bi-encoder models embed the query and item independently into\ntraining data for retrieval models by using LLMs as query gener-                high dimensional vectors: q\ud835\udc62 = MPNet(\ud835\udc5e\ud835\udc62 ), d\ud835\udc56 = MPNet(\ud835\udc51\ud835\udc56 ) and\n                                                                \ud835\udc41\ud835\udc62\nation models to author narrative queries \ud835\udc5e\ud835\udc62 : D = {(\ud835\udc5e\ud835\udc62 , {\ud835\udc51\ud835\udc56 }\ud835\udc56=1   )}.         rank items for the user based on the minimum L2 distance between\nThen, retrieval models are trained on the synthetic dataset D (Fig-\nure 3).                                                                         4 https://platform.openai.com/docs/models/gpt-3, text-davinci-003\n\n\n\n\n                                                                          779\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                   Mysore, McCallum, Zamani\n\n\nq\ud835\udc62 and d\ud835\udc56 . Embeddings are obtained by averaging token embeddings           dislikes). It also retains the users whose interests are summarizable\nfrom the final layer of MPNet, and the same model is used for both          by QGen. In the Yelp dataset, this results in 45,193 retained users.\nqueries and items. Cross-encoder models input both the query and            Now, 10,000 randomly selected users are chosen for generating syn-\nitem and output a score to be used for ranking\u0010 \ud835\udc60 = \ud835\udc53Cr ([\ud835\udc5e\ud835\udc62\u0011; \ud835\udc51\ud835\udc56 ]),       thetic narrative queries. For these users, a single randomly selected\nwhere \ud835\udc53Cr is parameterized as w\ud835\udc47 dropout W\ud835\udc47 MPNet(\u00b7) . We                   sentence from 10 of their reviews is included in the prompt (Figure\n                                                                            2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some\ntrain our bi-encoder model with a margin ranking loss: L\ud835\udc35\ud835\udc56 =\n\u00cd \u00cd\ud835\udc40                                \u2032                                       items are filtered out (\u00a73.2.2). Here, we exclude 40% of the items\n  \ud835\udc62 \ud835\udc56=1 max[\ud835\udc3f2(q\ud835\udc62 , d\ud835\udc56 ) \u2212 \ud835\udc3f2(q\ud835\udc62 , d\ud835\udc56 ) + \ud835\udeff, 0] with randomly sam-          for a user. This results in about 60,000 training samples for training\n                \u2032\npled negatives \ud835\udc51 and \ud835\udeff = 1. Our cross-encoders are trained with             BiEnc-Mint and CrEnc-Mint. These decisions were made manu-\n                            \u00cd \u00cd\ud835\udc40                \ud835\udc60\na cross-entropy loss: L\ud835\udc36\ud835\udc5f = \ud835\udc62 \ud835\udc56=1       log( \u00cd \ud835\udc52 \ud835\udc60 \u2032 ). For training, 4     ally by examining the resulting datasets and the cost of authoring\n                                                      \ud835\udc51\u2032 \ud835\udc52\nnegative example items \ud835\udc51 \u2032 are randomly sampled from ranks 100-             queries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230.\n300 from our trained bi-encoder. At test time, we retrieve the top\n200 items with our trained bi-encoder and re-rank them with the             4.1.3 Baselines. We compare BiEnc-Mint and CrEnc-Mint mod-\ncross-encoder - we evaluate both these components in experiments            els against several standard and performant retrieval model base-\nand refer to them as BiEnc-Mint and CrEnc-Mint.                             lines. These span zero-shot/unsupervised rankers, supervised bi-\n                                                                            encoders, unsupervised cross-encoders, and LLM baselines. BM25:\n4     EXPERIMENTS AND RESULTS                                               A standard unsupervised sparse retrieval baseline based on term\n                                                                            overlap between query and document, with strong generalization\nNext, we evaluate Mint on a publicly available test collection for\n                                                                            performance across tasks and domains [38]. Contriver: A BERT-base\nNDR and present a series of ablations.\n                                                                            bi-encoder model pre-trained for zero-shot retrieval with weakly su-\n                                                                            pervised query-document pairs [22]. MPNet-1B: A strong Sentence-\n4.1     Experimental Setup                                                  Bert bi-encoder model initialized with MPNet-base and trained on\n4.1.1 Datasets. We perform evaluations on an NDR dataset for                1 billion supervised query-document pairs aggregated from numer-\npoint-of-interest (POI) recommendation Pointrec [1]. Pointrec               ous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned\ncontains 112 realistic narrative queries (130 words long) obtained          on supervised question-passage pairs from MSMarco. UPR: A two-\nfrom discussion forums on Reddit and items pooled from baseline             stage approach that retrieves items with a Contriver bi-encoder\nrankers. The items are annotated on a graded relevance scale by             and re-ranks the top 200 items with a query-likelihood model using\ncrowd-workers and/or discussion forum members and further vali-             a FlanT5 model with 3B parameters [14, 40]. This may be seen\ndated by the dataset authors. The item collection C in Pointrec             as an unsupervised \u201ccross-encoder\u201d model. Grounded LLM: A re-\ncontains 700k POIs with metadata (category, city) and noisy text            cently proposed two-stage approach which autoregressively gener-\nsnippets describing the POI obtained from the Bing search engine.           ates ten pseudo-relevant items using an LLM (175B InstructGPT)\nFor test time ranking, we only rank the candidate items in the city         prompted with the narrative query and generates recommenda-\nand request category (e.g., \u201cRestaurants\u201d) of the query available in        tions grounded in C by retrieving the nearest neighbors for each\nPointrec - this follows prior practice to exclude clearly irrelevant        generated item using a bi-encoder [19]. We include one few-shot\nitems [1, 26]. We use user-item interaction datasets from Yelp to           example of a narrative query and recommended items in the prompt\ngenerate synthetic queries for training.5 Note also that we limit our       to the LLM. We run this baseline three times and report average\nevaluations to Pointrec since it presents the only publicly avail-          performance across runs. We report NDCG at 5 and 10, MAP, MRR,\nable, manually annotated, and candidate pooled test collection for          and Recall at 100 and 200. Finally, our reported results should be\nNDR, to our knowledge. Other datasets for NDR use document col-             considered lower bounds on realistic performance due to the un-\nlections that are no longer publicly accessible [24], contain sparse        judged documents (about 70% at \ud835\udc58 = 10) in our test collections\nand noisy relevance judgments due to them being determined with             [10].\nautomatic rules applied to discussion threads [18, 24], lack pooling\nto gather candidates for judging relevance [18, 24], or lack realistic      4.2    Results\nnarrative queries [21]. We leave the development of more robust\n                                                                            Table 1 presents the performance of the proposed method compared\ntest collections and evaluation methods for NDR to future work.\n                                                                            against baselines. Here, bold numbers indicate the best-performing\n4.1.2 Implementation Details. Next, we describe important details           model, and superscripts indicate statistical significance computed\nfor Mint and leave finer details of the model and training to our           with two-sided t-tests at \ud835\udc5d < 0.05.\ncode release. To sample user interactions for generating synthetic             Here, we first note the performance of baseline approaches. We\nqueries from the Yelp dataset, we exclude POIs and users with               see BM25 outperformed by Contriver, a transformer bi-encoder\nfewer than ten reviews to ensure that users were regular users of           model trained for zero-shot retrieval; this mirrors prior work [22].\nthe site with well represented interests. This follows common prior         Next, we see supervised bi-encoder models trained on similar pas-\npractice in preparing user-item interaction datasets for use [27].          sage (MPNet-1B) and question-answer (BERT-MSM) pairs outper-\nThen we retain users who deliver an average rating greater than             form a weakly supervised model (Contriver) by smaller margins.\n3/5 and with 10-30 above-average reviews. This desirably biases             Finally, the Grounded LLM outperforms all bi-encoder baselines, in-\nour data to users who commonly describe their likings (rather than          dicating strong few-shot generalization and mirroring prior results\n                                                                            [19]. Examining the Mint models, we first note that the BiEnc-\n5 https://www.yelp.com/dataset                                              Mint sees statistically significant improvement compared to BM25\n\n\n\n\n                                                                      780\n\fLarge Language Model Augmented Narrative Driven Recommendations                                       RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\nTable 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts\ndenote statistically significant improvements compared to specific baseline models.\n\n                                                                                        Pointrec\n                              Model     Parameters      NDCG@5     NDCG@10        MAP        MRR          Recall@100      Recall@200\n                             1 BM25          -          0.2682     0.2464         0.1182     0.2685       0.4194          0.5429\n                         2 Contriver       110M         0.2924     0.2776         0.1660     0.3355       0.4455          0.5552\n                        3 MPNet-1B         110M         0.3038     0.2842         0.1621     0.3566       0.4439          0.5657\n                      4 BERT-MSM           110M         0.3117     0.2886         0.1528     0.3320       0.4679          0.5816\n                  5 Grounded LLM        175B+110M       0.3558     0.3251         0.1808     0.3861       0.4797          0.5797\n                               6 UPR     110M+3B        0.3586     0.3242         0.1712     0.4013       0.4489          0.5552\n                       BiEnc-Mint          110M         0.34891    0.32631        0.18901    0.39821      0.49141         0.6221\n                       CrEnc-Mint         2\u00d7110M        0.372512   0.348912       0.219214   0.43171      0.5448123       0.6221\n\n\nand outperforms the best bi-encoder baselines by 11-13% on preci-                6B LLM for Item Queries. We find a smaller 6B LLM to result\nsion measures and 5-7% on recall measures. Specifically, we see a             in poor quality data when used to generate narrative queries con-\n                                                                                                \ud835\udc41\ud835\udc62\nmodel trained for question-answering (BERT-MSM) underperform                  ditioned on {\ud835\udc51\ud835\udc56 }\ud835\udc56=1  . Here we simplify the text generation task \u2013\nBiEnc-Mint, indicating the challenge of the NDR task. Further,                using a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56 . This\nBiEnc-Mint, trained on 5 orders of magnitude lesser data than                 experiment also mirrors the setup for generating synthetic queries\nMPNet-1B, sees improved performance \u2013 indicating the quality of               for search tasks [7, 15]. Here, we use 3-few shot examples and sam-\ndata obtained from Mint. Furthermore, BiEnc-Mint also performs                ple one item per user for generating \ud835\udc5e\ud835\udc62 . Given the lower cost of\nat par with a 175B LLM while offering the inference efficiency of a           using a smaller LLM, we use all 45,193 users in our Yelp dataset\nsmall-parameter bi-encoder. Next, we see CrEnc-Mint outperform                rather than a smaller random sample. From Table 2, we see that this\nthe baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM                   results in higher quality queries than using smaller LLMs for gen-\nby 4-21% on precision measures and 7-13% on recall measures \u2013                                                      \ud835\udc41\ud835\udc62\n                                                                              erating narrative queries from {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 . The resulting BiEnc model\ndemonstrating the value of Mint for training NDR models.                      underperforms the BiEnc-Mint, indicating the value of generating\n                                                                              complex queries conditioned on multiple items as in Mint for NDR.\n4.3    Ablations                                                              We see that CrEnc approaches the performance of CrEnc-Mint\u2013\nIn Table 2, we ablate various design choices in Mint. Different               note, however, that this approach uses the performant BiEnc-Mint\nchoices result in different training sets for the BiEnc and CrEnc             for sampling negatives and first stage ranking. We leave further\nmodels. Also, note that in reporting ablation performance for CrEnc,          exploration of using small parameter LLMs for data augmentation\nwe still use the performant BiEnc-Mint model for obtaining nega-              for NDR models to future work.\ntive examples for training and first-stage ranking. Without high-\nquality negative examples, we found CrEnc to result in much poorer            5    CONCLUSIONS\nperformance.                                                                  In this paper, we present Mint, a data augmentation method for the\n   No item filtering. Since synthetic queries are unlikely to rep-            narrative-driven recommendation (NDR) task. Mint re-purposes\n                                                                  \ud835\udc41\ud835\udc62\nresent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56 }\ud835\udc56=1             historical user-item interaction datasets for NDR by using a 175B pa-\nwhich have a low likelihood of being generated from the document              rameter large language model to author long-form narrative queries\n(\u00a73.2.2). Without this step, we expect the training set for training          while conditioning on the text of items liked by users. We evaluate\nretrieval models to be larger and noisier. In Table 2, we see that            bi-encoder and cross-encoder models trained on data from Mint on\nexcluding this step leads to a lower performance for BiEnc and                the publicly available Pointrec test collection for narrative-driven\nCrEnc, indicating that the quality of data obtained is important for          point of interest recommendation. We demonstrate that the result-\nperformance.                                                                  ing models outperform several strong baselines and ablated models\n   6B LLM for QGen. Mint relies on using an expensive 175B pa-                and match or outperform a 175B LLM directly used for NDR in a\nrameter InstructGPT model for QGen. Here, we investigate the                  1-shot setup.\n                                    \ud835\udc41\ud835\udc62\nefficacy for generating \ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 with a 6B parameter Instruct-            However, Mint also presents some limitations. Given our use of\nGPT model (text-curie-001). We use an identical setup to the                  historical interaction datasets for generating synthetic training data\n175B LLM for this. In Table 2, we see that training on the synthetic          and the prevalence of popular interests in these datasets longer,\nnarrative queries of the smaller LLM results in worse models \u2013 of-            tailed interests are unlikely to be present in the generated syn-\nten underperforming the baselines in Table 1. This indicates the              thetic datasets. In turn, causing retrieval models to likely see poorer\ninability of a smaller model to generate complex narrative queries            performance on these requests. Our use of LLMs to generate syn-\nwhile conditioning on a set of user items. This necessity of a larger         thetic queries also causes the queries to be repetitive in structure,\nLLM for generating queries in complex retrieval tasks has been                likely causing novel longer-tail queries to be poorly served. These\nobserved in prior work [15, 23].                                              limitations may be addressed in future work.\n\n\n\n\n                                                                      781\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                                               Mysore, McCallum, Zamani\n\n\n                                             Table 2: Mint ablated for different design choices on Pointrec.\n\n                                                                                                      Pointrec\n                            Ablation                                NDCG@5          NDCG@10      MAP         MRR        Recall@100        Recall@200\n                            BiEnc-Mint                              0.3489          0.3263       0.1890      0.3982     0.5263            0.6221\n                            \u2212 No item filtering                     0.2949          0.2766       0.1634      0.3505     0.4979            0.5951\n                            \u2212 6B LLM for QGen                       0.2336          0.2293       0.1125      0.2287     0.426             0.5435\n                            \u2212 6B LLM for Item Queries               0.3012          0.2875       0.1721      0.3384     0.4800            0.5909\n                            CrEnc-Mint                              0.3725          0.3489       0.2192      0.4317     0.5448            0.6221\n                            \u2212 No item filtering                     0.3570          0.3379       0.2071      0.4063     0.5366            0.6221\n                            \u2212 6B LLM for QGen                       0.2618          0.2421       0.1341      0.3118     0.4841            0.6221\n                            \u2212 6B LLM for Item Queries               0.3792          0.3451       0.2128      0.4098     0.5546            0.6221\n\n\n   Besides this, other avenues also present rich future work. While                                Society: 14th International Conference, iConference 2019, Washington, DC, USA,\nMint leverages a 175B LLM for generating synthetic queries, smaller                                March 31\u2013April 3, 2019, Proceedings 14. Springer, 503\u2013515.\n                                                                                               [5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven\nparameter LLMs may be explored for this purpose - perhaps by                                       Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender\ntraining dedicated QGen models. Mint may also be expanded to                                       Systems (Como, Italy) (RecSys \u201917). Association for Computing Machinery, New\n                                                                                                   York, NY, USA, 238\u2013242. https://doi.org/10.1145/3109859.3109893\nexplore more active strategies for sampling items and users for                                [6] Toine Bogers and Marijn Koolen. 2018. \u201cI\u2019m looking for something like. . . \u201d:\nwhom narrative queries are authored - this may allow more effi-                                    Combining Narratives and Example Items for Narrative-driven Book Recommen-\ncient use of large parameter LLMs while ensuring higher quality                                    dation. In Knowledge-aware and Conversational Recommender Systems Workshop.\n                                                                                                   CEUR Workshop Proceedings.\ntraining datasets. Next, the generation of synthetic queries from                              [7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nsets of documents may be explored for a broader range of retrieval                                 InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings\ntasks beyond NDR given its promise to generate larger training                                     of the 45th International ACM SIGIR Conference on Research and Development\n                                                                                                   in Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing\nsets \u2013 a currently underexplored direction. Finally, given the lack of                             Machinery, New York, NY, USA, 2387\u20132392. https://doi.org/10.1145/3477495.\nlarger-scale test collections for NDR and the effectiveness of LLMs                                3531863\n                                                                                               [8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,\nfor authoring narrative queries from user-item interaction, fruitful                               Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-\nfuture work may also explore the creation of larger-scale datasets                                 pervised Training of Efficient Rankers. arXiv:2301.02998\nin a mixed-initiative setup to robustly evaluate models for NDR.                               [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                                   Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                                   Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\n                                                                                                   Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nACKNOWLEDGMENTS                                                                                    Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nWe thank anonymous reviewers for their invaluable feedback. This                                   Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n                                                                                                   Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nwork was partly supported by the Center for Intelligent Informa-                                   In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-\ntion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of                                  zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nNaval Research contract number N000142212688, an Amazon Alexa                                      Inc., 1877\u20131901. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n                                                                                                   1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nPrize grant, and the Chan Zuckerberg Initiative under the project                             [10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nScientific Knowledge Base Construction. Any opinions, findings                                     Information. In Proceedings of the 27th Annual International ACM SIGIR Conference\n                                                                                                   on Research and Development in Information Retrieval (Sheffield, United Kingdom)\nand conclusions or recommendations expressed here are those of                                     (SIGIR \u201904). Association for Computing Machinery, New York, NY, USA, 25\u201332.\nthe authors and do not necessarily reflect those of the sponsors.                                  https://doi.org/10.1145/1008992.1009000\n                                                                                              [11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-\n                                                                                                   CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing\nREFERENCES                                                                                         Cold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con-\n                                                                                                   ference on Research and Development in Information Retrieval (Virtual Event,\n [1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:\n                                                                                                   China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA,\n     A Test Collection for Narrative-Driven Point of Interest Recommendation. In\n                                                                                                   1251\u20131260. https://doi.org/10.1145/3397271.3401038\n     Proceedings of the 44th International ACM SIGIR Conference on Research and\n                                                                                              [12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun\n     Development in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As-\n                                                                                                   Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data\n     sociation for Computing Machinery, New York, NY, USA, 2478\u20132484. https:\n                                                                                                   Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n     //doi.org/10.1145/3404835.3463243\n                                                                                                   USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n [2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and\n                                                                                                   1012\u20131020. https://doi.org/10.1145/3543507.3583341\n     Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in\n                                                                                              [13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study\n     Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference\n                                                                                                   for Understanding Users\u2019 Attitudes Towards a Conversational Agent for News\n     on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/\n                                                                                                   Recommendation. In Proceedings of the 4th Conference on Conversational User\n     doi/10.1145/3406522.3446021\n                                                                                                   Interfaces (Glasgow, United Kingdom) (CUI \u201922). Association for Computing\n [3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018.\n                                                                                                   Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/\n     \u201cWhat was this Movie About this Chick?\u201d A Comparative Study of Relevance\n                                                                                                   3543829.3544530\n     Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter-\n                                                                                              [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\n     national Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings\n                                                                                                   Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\n     13. Springer, 323\u2013334.\n                                                                                                   instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n [4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019.\n                                                                                              [15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\n     \u201cLooking for an amazing game I can relax and sink hours into...\u201d: A Study of\n                                                                                                   Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot\n     Relevance Aspects in Video Game Discovery. In Information in Contemporary\n\n\n\n\n                                                                                        782\n\fLarge Language Model Augmented Narrative Driven Recommendations                                                          RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\n     Dense Retrieval From 8 Examples. In The Eleventh International Conference on                  Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,\n     Learning Representations. https://openreview.net/forum?id=gmL46YMpu2J                         A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\n[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.                       Inc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/\n     Google News Personalization: Scalable Online Collaborative Filtering. In Pro-                 b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n     ceedings of the 16th International Conference on World Wide Web (Banff, Alberta,         [34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,\n     Canada) (WWW \u201907). Association for Computing Machinery, New York, NY, USA,                    and Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces\n     271\u2013280. https://doi.org/10.1145/1242572.1242610                                              That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on\n[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,                Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR\n     Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi                \u201921). Association for Computing Machinery, New York, NY, USA, 261\u2013265. https:\n     Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of                     //doi.org/10.1145/3406522.3446035\n     the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys              [35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues\n     \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https:                 Bouchard. 2023. Improving Content Retrievability in Search with Controllable\n     //doi.org/10.1145/1864708.1864770                                                             Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating                     USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n     Narrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th                  3182\u20133192. https://doi.org/10.1145/3543507.3583261\n     International Conference on Intelligent User Interfaces (Marina del Ray, California)     [36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin.\n     (IUI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https:               2022. On Natural Language User Profiles for Transparent and Scrutable Rec-\n     //doi.org/10.1145/3301275.3302287                                                             ommendation. In Proceedings of the 45th International ACM SIGIR Conference\n[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot                   on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR\n     Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).             \u201922). Association for Computing Machinery, New York, NY, USA, 2863\u20132874.\n[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context                   https://doi.org/10.1145/3477495.3531873\n     Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom-                 [37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\n     mender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing                     using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\n     Machinery, New York, NY, USA, 9\u201316. https://doi.org/10.1145/2507157.2507187                   pirical Methods in Natural Language Processing. Association for Computational\n[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M               Linguistics. https://arxiv.org/abs/1908.10084\n     Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In               [38] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\n     TREC.                                                                                         Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389.\n[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-                  https://doi.org/10.1561/1500000019\n     janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-              [39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin\n     mation Retrieval with Contrastive Learning. Transactions on Machine Learning                  Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.\n     Research (2022). https://openreview.net/forum?id=jKN1pXi7b0                                   UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation\n[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,                of Rerankers. arXiv:2303.00807 [cs.IR]\n     Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as            [40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\n     Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820                      Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\n[24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo                      with Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\n     Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016.                     Empirical Methods in Natural Language Processing. Association for Computational\n     Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul-               Linguistics, Abu Dhabi, United Arab Emirates, 3781\u20133797. https://aclanthology.\n     tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa            org/2022.emnlp-main.249\n     Gon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato,            [41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked\n     and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351\u2013370.                    and Permuted Pre-training for Language Understanding. In Advances in Neural\n[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski,                   Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_\n     Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data                  files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf\n     for Conversational Music Recommendation Using Random Walks and Language                  [42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via\n     Models. arXiv:2301.11489                                                                      Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual\n[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of-                International ACM SIGIR Conference on Research and Development in Information\n     Interest Recommendation by Mining Users\u2019 Preference Transition. In Proceedings                Retrieval (Salvador, Brazil) (SIGIR \u201905). Association for Computing Machinery,\n     of the 22nd ACM International Conference on Information & Knowledge Manage-                   New York, NY, USA, 449\u2013456. https://doi.org/10.1145/1076034.1076111\n     ment (San Francisco, California, USA) (CIKM \u201913). Association for Computing Ma-          [43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic\n     chinery, New York, NY, USA, 733\u2013738. https://doi.org/10.1145/2505515.2505639                  Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender\n[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An                           Systems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association for\n     Experimental Evaluation of Point-of-Interest Recommendation in Location-Based                 Computing Machinery, New York, NY, USA, 86\u201394. https://doi.org/10.1145/\n     Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010\u20131021. https://doi.                 3240323.3240369\n     org/10.14778/3115404.3115407                                                             [44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.\n[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and                Controllable Gradient Item Retrieval. In Web Conference.\n     Lucas Dixon. 2021. Augmenting the user-item graph with textual similarity                [45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\n     models. arXiv preprint arXiv:2109.09358 (2021).                                               and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug-\n[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue                        mentation. In Proceedings of the 25th ACM SIGKDD International Conference\n     Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In                on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). As-\n     Proceedings of the 17th Conference of the European Chapter of the Association for             sociation for Computing Machinery, New York, NY, USA, 548\u2013556. https:\n     Computational Linguistics. Association for Computational Linguistics, Dubrovnik,              //doi.org/10.1145/3292500.3330873\n     Croatia, 2799\u20132829. https://aclanthology.org/2023.eacl-main.206                          [46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized\n[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear                   Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM\n     Critiquing for Conversational Recommender Systems. In The Web Conference.                     Conference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association\n[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot             for Computing Machinery, New York, NY, USA, 502\u2013505. https://doi.org/10.\n     Neural Passage Retrieval via Domain-targeted Synthetic Question Generation.                   1145/3523227.3547394\n     In Proceedings of the 16th Conference of the European Chapter of the Associa-            [47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng.\n     tion for Computational Linguistics: Main Volume. Association for Computational                2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users\n     Linguistics, Online, 1075\u20131088. https://doi.org/10.18653/v1/2021.eacl-main.92                 in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n[32] Sheshera Mysore, Tim O\u2019Gorman, Andrew McCallum, and Hamed Zamani. 2021.                       USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n     CSFCube - A Test Collection of Computer Science Research Articles for Faceted                 1396\u20131404. https://doi.org/10.1145/3543507.3583538\n     Query by Example. In Thirty-fifth Conference on Neural Information Processing            [48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con-\n     Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv.              versational information seeking. arXiv preprint arXiv:2201.08808 (2022).\n     2103.12906                                                                               [49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela                  Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer-\n     Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John                        ence on Research and Development in Information Retrieval (Virtual Event, China)\n     Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda                     (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 881\u2013890.\n     Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.                    https://doi.org/10.1145/3397271.3401180\n     Training language models to follow instructions with human feedback. In\n\n\n\n\n                                                                                        783\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\nAbhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google News Personalization: Scalable Online Collaborative Filtering. In Proceedings of the 16th International Conference on World Wide Web (Banff, Alberta, Canada) (WWW \u201907) . Association for Computing Machinery, New York, NY, USA, 271\u2013280. https://doi.org/10.1145/1242572.1242610 ", " James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https://doi.org/10.1145/1864708.1864770 ", " Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating Narrative-Driven Movie Recommendations on Reddit. In Proceedings of"], "publish_date": "2023-09-18", "approved": false}}, {"_index": "articles", "_id": "fCkxcI0Bf-PcuVTAe1KQ", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword"], "_source": {"title": "Framing the News: From Human Perception to Large Language Model Inferences", "abstract": "Identifying the frames of news is important to understand the articles\u2019 vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.", "authors": ["David Alonso del Barrio", "Daniel Gatica-Perez"], "institutions": ["Idiap Research Institute", "EPFL"], "keywords": ["Covid-19 no-vax", "news framing", "GPT-3", "prompt-engineering", "transformers", "large language models"], "full_text": "                      Framing the News:\n    From Human Perception to Large Language Model Inferences\n                           David Alonso del Barrio                                                                      Daniel Gatica-Perez\n                                  ddbarrio@idiap.ch                                                                        gatica@idiap.ch\n                               Idiap Research Institute                                                           Idiap Research Institute and EPFL\n                                     Switzerland                                                                             Switzerland\nABSTRACT                                                                                              1   INTRODUCTION\nIdentifying the frames of news is important to understand the arti-                                   In recent years, there has been a proliferation in the use of concepts\ncles\u2019 vision, intention, message to be conveyed, and which aspects                                    such as data journalism, computational journalism, and computer-\nof the news are emphasized. Framing is a widely studied concept                                       assisted reporting [15] [29], which all share the vision of bridging\nin journalism, and has emerged as a new topic in computing, with                                      journalism and technology. The progress made in NLP has been\nthe potential to automate processes and facilitate the work of jour-                                  gradually integrated into the journalistic field [5][8][54]. More\nnalism professionals. In this paper, we study this issue with articles                                specifically, machine learning models based on transformers have\nrelated to the Covid-19 anti-vaccine movement. First, to under-                                       been integrated in the media sector in different tasks [41] such as\nstand the perspectives used to treat this theme, we developed a                                       the creation of headlines with generative languages models [17],\nprotocol for human labeling of frames for 1786 headlines of No-                                       summarization of news articles [28][27], false news detection [49],\nVax movement articles of European newspapers from 5 countries.                                        and topic modeling and sentiment analysis [25]. The development of\nHeadlines are key units in the written press, and worth of analysis                                   large language models such as GPT-3 [9], BLOOM [51] or ChatGPT\nas many people only read headlines (or use them to guide their                                        show a clear trend towards human-machine interaction becoming\ndecision for further reading.) Second, considering advances in Nat-                                   easier and more intuitive, opening up a wide range of research\nural Language Processing (NLP) with large language models, we                                         possibilities. At the same time, the use of these models is also\ninvestigated two approaches for frame inference of news headlines:                                    associated with a lack of transparency regarding how these models\nfirst with a GPT-3.5 fine-tuning approach, and second with GPT-                                       work, but efforts are being made to bring some transparency to\n3.5 prompt-engineering. Our work contributes to the study and                                         these models, and to analyze use cases where they can be useful and\nanalysis of the performance that these models have to facilitate                                      where they cannot [35]. Based on the premises that these models\njournalistic tasks like classification of frames, while understanding                                 open up a wide range of research directions [7], and that at the same\nwhether the models are able to replicate human perception in the                                      time (and needless to say) they are not the solution to all problems,\nidentification of these frames.                                                                       we are interested in identifying use cases and tasks where they\n                                                                                                      can be potentially useful, while acknowledging and systematically\nCCS CONCEPTS                                                                                          documenting their limitations [56]. More specifically, the aim of\n                                                                                                      this work is to analyze the performance of GPT-3.5 for a specific\n\u2022 Computing methodologies \u2192 Information extraction; \u2022\n                                                                                                      use case, namely the analysis of frames in news, from an empirical\nHuman-centered computing \u2192 Text input.\n                                                                                                      point of view, with the objective of shedding light on a potential\n                                                                                                      use of generative models in journalistic tasks.\nKEYWORDS                                                                                                 Frame analysis is a concept from journalism, which consists of\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering, trans-                                      studying the way in which news stories are presented on an issue,\nformers, large language models                                                                        and what aspects are emphasized: Is a merely informative vision\n                                                                                                      given in an article? Or is it intended to leave a moral lesson? Is\nACM Reference Format:                                                                                 a news article being presented from an economic point of view?\nDavid Alonso del Barrio and Daniel Gatica-Perez. 2023. Framing the News:                              Or from a more human, emotional angle? The examples above\nFrom Human Perception to Large Language Model Inferences . In Inter-                                  correspond to different frames with which an article can be written.\nnational Conference on Multimedia Retrieval (ICMR \u201923), June 12\u201315, 2023,\n                                                                                                         The concept of news framing has been studied in computing as\nThessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3591106.3592278\n                                                                                                      a step beyond topic modeling and sentiment analysis, and for this\n                                                                                                      purpose, in recent years, pre-trained language models have been\n                                                                                                      used for fine-tuning the classification process of these frames [60]\nPermission to make digital or hard copies of all or part of this work for personal or                 [10], but the emergence of generative models opens the possibility\nclassroom use is granted without fee provided that copies are not made or distributed                 of doing prompt-engineering of these classification tasks, instead\nfor profit or commercial advantage and that copies bear this notice and the full citation             of the fine-tuning approach investigated so far.\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or                   Our work aims to address this research gap by posing the fol-\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission         lowing research questions:\nand/or a fee. Request permissions from permissions@acm.org.\n                                                                                                         RQ1: What are the main frames in the news headlines about\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.                     the anti-vaccine movement, as reported in newspapers across 5\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00                                                          European countries?\nhttps://doi.org/10.1145/3591106.3592278\n\n\n\n\n                                                                                                627\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                        David Alonso del Barrio and Daniel Gatica-Perez\n\n\n    RQ2: Can prompt engineering be used for classification of head-         message? Does the story make reference to morality, God, and other\nlines according to frames?                                                  religious tenets? Does the story offer specific social prescriptions\n    By addressing the above research questions, our work makes the          about how to behave?\", and so on for each of the frame types. This\nfollowing contributions:                                                    categorization of frames has been used in various topics such as\n    Contribution 1. We implemented a process to do human an-                climate change [18] [19], vaccine hesitance [13], or immigration\nnotation of the main frame of 1786 headlines of articles about the          [34].\nCovid-19 no-vax movement, as reported in 19 newspapers from 5                   We now compare the two approaches on a common topic, such\nEuropean countries (France, Italy, Spain, Switzerland and United            as Covid-19. Ebrahim et al. [21] followed an inductive approach\nKingdom.) At the headline level, we found that the predominant              in which the frames were not predefined but emerged from the\nframe was human interest, where this frame corresponds to a per-            text (e.g., deadly spread, stay home, what if, the cost of Covid-19)\nsonification of an event, either through a statement by a person,           using headlines as the unit of analysis. In contrast, the deductive\nor the explanation of a specific event that happened to a person.           approach has studied very different labels. El-Behary et al. [22]\nFurthermore, we found a large number of headlines annotated as              followed the method of yes/no questions, but in addition to the\ncontaining no frame, as they simply present information without             5 generic frames presented before, they also used blame frame\nentering into evaluations. We also found that for all the countries         and fear frame. Adiprasetio et al. [1] and Rodelo [50] used the 5\ninvolved, the distribution of frame types was very similar, i.e., hu-       generic frames with yes/no questions, while Catal\u00e1n-Matamoros et\nman interest and no frame are the two predominant frames. Finally,          al. [14] used the 5 frames and read the headline and subheadline\nthe generated annotations allowed to subsequently study the per-            to decide the main frame. Table 1 summarizes some of the the\nformance of a large language model.                                         existing approaches. This previous work showed how frame labels\n    Contribution 2. We studied the performance of GPT-3.5 on                can be different, and also that frame analysis has been done at both\nthe task of frame classification of headlines. In addition to using         headline and article levels. These two approaches (inductive and\nthe fine-tuning approach from previous literature, we propose an            deductive) that originated in journalism have since been replicated\nalternative approach for frame classification that requires no labeled      in the computing literature.\ndata for training, namely prompt-engineering using GPT-3.5. The                 We decided to follow the deductive approach because a prede-\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy            fined list of frames allows to compare among topics, countries,\n(slightly higher than other smaller models), and that the prompt-           previous literature, and also because they represent a fixed list of\nengineering approach results in lower performance (49% accuracy.)           labels for machine classification models. Furthermore, the induc-\nOur analysis also shows that the subjectivity of the human labeling         tive approach tends to be more specific to a topic, and from the\ntask has an effect on the obtained accufracy.                               computing viewpoint, past work has tried to justify topic modeling\n    The paper is organized as follows. In Section 2, we discuss related     as a technique to extract frames from articles.\nwork. In Section 3, we describe the news dataset. In Section 4, we              Yl\u00e4-Antitila et al. [60] proposed topic modeling as a frame ex-\ndescribe the methodology for both human labeling and machine                traction technique. They argued that topics can be interpreted as\nclassification of news frames. We present and discuss results for           frames if three requirements are met: frames are operationalized as\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide          connections between concepts; subject-specific data is selected; and\nconclusions in Section 7.                                                   topics are adequately validated as frames, for which they suggested\n                                                                            a practical procedure. This approach was based on the choice of a\n                                                                            specific topic (e.g., climate change) and the use of Latent Dirichlet\n2    RELATED WORK                                                           Allocation (LDA) as a technique to extract a number of subtopics.\nFraming has been a concept widely studied in journalism, with a             In a second phase, a qualitative study of the top 10 words of each\ndefinition that is rooted in the study of this domain [23]: \u201cTo frame       subtopic was performed, and the different subtopics were elimi-\nis to select some aspects of a perceived reality and make them more         nated or grouped, reducing the number and establishing a tentative\nsalient in a communicating text, in such a way as to promote a par-         description. In a third phase, the top 10 articles belonging to that\nticular problem definition, causal interpretation, moral evaluation,        frame/topic were taken, and if the description of the topic fitted\nand/or treatment recommendation for the item described.\u201d                    at least 8 of the 10 articles, that topic/frame remained. The frames\n    For frame recognition, there are two main approaches: the induc-        found in this article were: green growth, emission cuts, negotiations\ntive approach [16], where one can extract the frames after reading          and treaties, environmental risk, cost of carbon emissions, Chinese\nthe article, and the deductive approach [38], where a predefined            emissions, economics of energy production, climate change, en-\nlist of frames exists and the goal is to interpret if any of them ap-       vironmental activism, North-South burden sharing, state leaders\npears in the article. In the deductive case, there are generic frames       negotiating, and citizen participation.\nand subject-specific frames, and the way to detect them typically               From Entman\u2019s definition of frame [23], it seems that the deduc-\ninvolves reading and identifying one frame at a time, or through            tive approach is more refined than the inductive approach (which\nanswers to yes/no questions that represent the frames. Semetko et           seems to resemble the detection of sub-themes.) For example, with\nal. [52] used 5 types of generic frames (attribution of responsibility,     regard to climate change, there are stories on how people have been\nhuman interest, conflict, morality, and economic consequences)              affected by climate change from an emotional point of view, thus\nbased on previous literature, and they defined a list of 20 yes/no          personalizing the problem. In this case, we could categorize the\nquestions to detect frames in articles. For instance, the questions         corresponding frame as human interest, as the writer of the article\nabout morality are the following: \"Does the story contain any moral         is selecting \"some aspects of a perceived reality and make them\n\n\n\n\n                                                                      628\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                            ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\nmore salient\". The language subtleties with which news articles are            Table 1: Summary of deductive approaches for frame analysis\npresented cannot be captured with basic topic modeling.\n   Isoaho et al.[30] held the position that while the benefits of                  Ref Frames                                         Goal                       Technique          Number\n                                                                                                                                                                                    samples\n                                                                                                                                                                                                of\n\nscale and scope in topic modeling were clear, there were also a                    [12] 15 generic frames: \"Economic\", \"Capac-        To label frames of full    Reading the\n                                                                                                                                                                                    20000 articles\nnumber of problems, namely that topic outputs do not correspond                         ity and resources\", \"Morality\", \"Fair-        articles                   full    article,\n                                                                                        ness and equality\", \"Legality, constitu-                                 the annotator\nto the methodological definition of frames, and thus topic modeling                     tionality and jurisprudence\", \"Policy                                    defines     the\n                                                                                        prescription and evaluation\", \"Crime                                     main frame\nremained an incomplete method for frame analysis. Topic modeling,                       and punishment\", \"Security and de-\n                                                                                        fense\", \"Health and safety\", \"Quality of\nin the practice of journalistic research, is a useful technique to deal                 life\", \"Cultural identity\", \"Public opin-\nwith the large datasets that are available, yet is often not enough to                  ion\", \"Political\", \"External regulation\n                                                                                        and reputation\", \"Other\".\ndo more thorough analyses [31]. In our work, we clearly notice that                [33] 15 generic frames                             Classification             BERT based\n                                                                                                                                                                                    12000 articles\n                                                                                                                                                                 models\nframe analysis is not topic modeling. For example, two documents                   [52] 5 generic frames: \"human interest\",           To label frames of full    Yes/No ques-       2600 articles\n                                                                                        \"conflict\", \"morality\", \"attribution of       articles                   tions.             and 1522 tv\ncould be about the same topic, say Covid-19 vaccination, but one                        responsibility\", and \"economic conse-                                                       news stories\narticle could emphasize the number of deaths after vaccination,                         quences\".\n                                                                                   [37] 9 specific frames:\u201cPolitics\u201d, \u201cPublic         To label frames of full    Reading the        2990 headlines\nwhile the other emphasized the role of the vaccine as a solution to                     opinion\u201d, \u201cSociety/Culture\u201d, and              articles/ Classification   full    article,\n                                                                                        \u201cEconomic consequences\u201d , \u201c2nd                                           the annotator\nthe epidemic.                                                                           Amendment\u201d (Gun Rights), \u201cGun                                            defines     the\n                                                                                        control/regulation\u201d, \u201cMental health\u201d,                                    main frame.\n   We also consider that the larger the number of possible frame                        \u201cSchool/Public space safety\u201d, and                                        BERT based\n                                                                                        \u201cRace/Ethnicity\u201d.                                                        models\ntypes, the more likely it is to end up doing topic modeling instead of             [22] 5 generic frames + blame frame and            To label frames of full    Yes/No ques-       1170 articles\nframe analysis. Using a deductive approach, Dallas et al. [12] created                  fear frame\n                                                                                   [1] 5 generic frames\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        6713 articles\na dataset with articles about polemic topics such as immigration,                                                                     articles                   full    article,\n                                                                                                                                                                 the annotator\nsame sex marriage, or smoking, and they defined 15 types of frames:                                                                                              defines     the\n                                                                                                                                                                 main frame.\n\"economic, capacity and resources, morality, fairness and equality,                [50] 5 generic frames + pandemic frames            To label frames of full    Yes/No ques-       2742 articles\nlegality, constitutionality and jurisprudence, policy prescription and             [14] 5 generic frames, journalistic role and\n                                                                                                                                      articles\n                                                                                                                                      To label frames of full\n                                                                                                                                                                 tions.\n                                                                                                                                                                 Reading the        131 headlines +\nevaluation, crime and punishment, security and defense, health and                      pandemic frames                               articles                   headline and       subheadlines\n                                                                                                                                                                 subheadline,\nsafety, quality of life, cultural identity, political, external regulation                                                                                       the annotator\n                                                                                                                                                                 defines     the\nand reputation, other\". In this case, they authors did not use a list                                                                                            main frame.\nof questions. Instead, for each article, annotators were asked to\nidentify any of the 15 framing dimensions present in the article\nand to label text blurbs that cued them (based on the definitions of\n                                                                               the potential applications, and the challenges to face when using\neach of the frame dimensions) and decide the main frame of each\n                                                                               this approach. Prompt engineering applications include knowledge\narticle. In our case, we followed the idea of detecting the main frame\n                                                                               probing [46], information extraction [53], NLP reasoning [57], ques-\nby reading the text instead of answering questions, but instead of\n                                                                               tion answering [32], text generation [20], multi-modal learning [58],\nusing the 15 frames proposed in [12] , we used the 5 generic frames\n                                                                               and text classification [24], the latter being the prompt-engineering\nproposed in [52].\n                                                                               use case in our work. Puri et al.[45] presented a very interesting\n   A final decision in our work was the type of text to analyze,\n                                                                               idea that we apply to our classification task. This consists of pro-\nwhether headlines or whole article. For this decision, the chosen\n                                                                               viding the language model with natural language descriptions of\nclassification method was also going to be important. For example,\n                                                                               classification tasks as input, and training it to generate the correct\nKhanehzar et al. [33] used traditional approaches such as SVMs as\n                                                                               answer in natural language via a language modeling objective. It is\nbaseline, and demonstrated the improvement in frame classifica-\n                                                                               a zero-shot learning approach, in which no examples are used to\ntion with the use of pre-trained languages models such as BERT,\n                                                                               explain the task to the model. Radford et al. [48] demonstrated that\nRoBERTa and XLNet, following a fine-tuning approach, setting\n                                                                               language models can learn tasks without any explicit supervision.\nas input text a maximum of 256 tokens (although the maximum\n                                                                               We have followed this approach to find an alternative way to do\nnumber of input tokens in these models is 512 tokens.) Liu et al.\n                                                                               frame analysis.\n[37] classified news headlines about the gun problem in the United\n                                                                                   As mentioned before, the emergence of giant models like GPT-3,\nStates, arguing for the choice of headlines as a unit of analysis\n                                                                               BLOOM, and ChatGPT are a very active research topic. To the best\nbased on previous journalism literature [6], [44], that advocated\n                                                                               of our knowledge, on one hand our work extends the computational\nfor the importance and influence of headlines on readers and the\n                                                                               analysis of news related to the covid-19 no-vax movement, which\nsubsequent perception of articles. From a computational viewpoint,\n                                                                               illustrates the influence of the press on the ways societies think\nusing headlines is also an advantage, since you avoid the 512 token\n                                                                               about relevant issues [40], [59], and on the other hand it adds to\nlimitation in BERT-based models. Therefore, we decided to work\n                                                                               the literature of human-machine interaction, regarding the design\nwith headlines about a controversial issue, namely the Covid-19\n                                                                               of GPT-3 prompts for classification tasks [39], [2].\nno-vax movement.\n   Continuing with the question of the methods used for classi-\nfication, much work has been developed in prompt engineering,\n                                                                               3        DATA: EUROPEAN COVID-19 NEWS\nespecially since the release of GPT-3. Liu et al.[36] presented a good                  DATASET\noverview of the work done on this new NLP paradigm, not only                   We used part of the European Covid-19 News dataset collected in\nexplaining the concept of prompt engineering, but also the differ-             our recent work [3]. This dataset contains 51320 articles on Covid-\nent strategies that can be followed both in the design of prompts,             19 vaccination from 19 newspapers from 5 different countries: Italy,\n\n\n\n\n                                                                         629\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                                                   David Alonso del Barrio and Daniel Gatica-Perez\n\n\nFrance, Spain, Switzerland and UK. The articles cover a time period                                                   [52] with 5 generic frames (attribution of responsibility, human\nof 22 months, from January 2020 to October 2021. All content was                                                      interest, conflict, morality, and economic consequences) plus one\ntranslated into English to be able to work in a common language.                                                      additional \u2019no-frame\u2019 category. Two researchers were engaged to\nThe dataset was used for various analyses, such as name entity                                                        annotate a sample of the collected newspaper articles following a\nrecognition, sentiment analysis, and subtopic modeling, to under-                                                     three-phase training procedure.\nstand how Covid-19 vaccination was reported in Europe through                                                            In the first phase, annotators had to read the codebook and get\nthe print media (in digital format.) The subtopic modeling analysis                                                   familiar with the task. In the second phase, they were asked to\nrevealed a subsample of articles on the no-vax movement, which is                                                     identify the main frame in the same subset of 50 headlines. At the\nthe one we have used in this paper. We took the headlines of the                                                      end of the second phase, the intercoder reliability (ICR) was 0.58\narticles associated with the no-vax movement, selecting all articles                                                  between the 2 annotators. We analyzed those cases where there\ncontaining any of the keywords in Table 2 in the headline or in the                                                   were discrepancies, and observed that in some cases, there was not a\nmain text. This corresponds to a total of 1786 headlines.                                                             unique main frame, because both annotators had valid arguments to\n                                                                                                                      select one of the frames. In other cases, the discrepancies were due\n      Table 2: Keywords used to identify no-vax articles                                                              to slight misunderstanding of the definitions. In the third phase, the\n                                                                                                                      annotators coded again 50 headlines, and the ICR increased to was\n                                                         Keywords                                                     0.66. We realized that the possibility of having two frames remained.\n NO VAX TOPIC    \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"         They discussed the cases in which they had disagreed, and if the\n                                                                                                                      other person\u2019s arguments were considered valid, it could be said that\n   In Table 3, we show the number of headlines per country and                                                        there were two frames. After this three-phase training procedure,\nnewspaper. France is the country with the most no-vax articles in                                                     annotators were ready to annotate the dataset independently. We\nthe corpus, with 523 articles, followed by Italy with 508. However,                                                   divided the dataset into two equal parts, and each person annotated\nnote that there are 6 newspapers from France, while only 2 from                                                       893 headlines.\nItaly. Corriere della Sera is the newspaper that dealt most frequently\nwith the subject (429 articles), while The Telegraph is the second                                                    4.2    Fine-tuning GPT-3.5 and BERT-based\none (206 articles). The total number of articles normalized by the                                                           models\nnumber of newspapers per country is also shown in the last column                                                     With the annotated dataset, we investigated two NLP approaches:\nof the Table. Using these normalized values, the ranking is Italy,                                                    the first one involves fine-tuning a pre-trained model; the second\nUK, France, Switzerland, and Spain.                                                                                   one is prompt engineering. Pre-trained language models have been\n\n Table 3: Number of headlines by newspaper and country\n\n      COUNTRY               NEWSPAPER                HEADLINES             TOTAL (NORM. TOTAL)\n      FRANCE                    La Croix                     94                       523 (87.1)\n                               Le Monde                     125\n                               Les Echos                     49\n                              Liberation                     97\n                            Lyon Capitale                     8\n                            Ouest France                    150\n        ITALY             Corriere della Sera               429                      508 (254.0)\n                            Il Sole 24 Ore                   79\n        SPAIN                 20 minutos                     27                       303 (50.5)\n                                  ABC                        50\n                               El Diario                     32                                                                    Figure 1: Pre-train, fine-tune, prompt\n                               El Mundo                      77\n                              El Espa\u00f1ol                     22\n                           La Vanguardia                     95\n                               24 heures                     97                       230 (76.6)\n   SWITZERLAND\n                              La Libert\u00e9                     22\n                                                                                                                      trained with large text strings based on two unsupervised tasks,\n                               Le Temps                     111                                                       next sentence prediction and masked language model. Figure 1\n                           The Irish News                    16                      222 (111.0)\n UNITED KINGDOM\n                           The Telegraph                    206                                                       summarizes these techniques.\n                                                                                         1786                            In the first approach, a model with a fixed architecture is pre-\n                                                                                                                      trained as a language model (LM), predicting the likelihood of the\n                                                                                                                      observed textual data. This can be done due to the availability of\n4 METHODOLOGY                                                                                                         large, raw text data needed to train LMs. This learning process can\n                                                                                                                      produce general purpose features of the modeled language. The\n4.1 Human labeling of news frames                                                                                     learning process produces robust, general-purpose features of the\nTo carry out the labeling of the frames in our corpus of headlines, we                                                language being modeled. The above pre-trained LM is then adapted\nfirst designed a codebook, which contained the definitions of each                                                    to different downstream tasks, by introducing additional parameters\nof the frame types and a couple of examples of each type, as well                                                     and adjusting them using task-specific objective functions. In this\nas a definition of the corpus subject matter and definitions of the                                                   approach, the focus was primarily on goal engineering, designing\nconcept of frame analysis, so that the annotators could understand                                                    the training targets used in both the pre-training and the fine-tuning\nthe task to be performed. The codebook follows the proposed by                                                        stages [36].\n\n\n\n\n                                                                                                                630\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                    ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n    We present an example to illustrate the idea. Imagine that the        adjective was matched with a sentiment. In a similar fashion, we\ntask is sentiment analysis, and we have a dataset with sentences          decided to build a thesaurus of adjectives that define each of the\nand their associated sentiment, and a pre-trained model, which is a       frames. For instance, the human interest frame could be \u2019interest-\nsaved neural network trained with a much larger dataset. For that         ing\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be:\npre-trained model to address the target task, we unfreeze a few of        \u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou-\nthe top layers of the saved model base and jointly train both the         blemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined,\nnewly-added classifier layers and the last layers of the base model.      we needed to define the prompt in order to get, as an answer, one\nThis allows to \"fine-tune\" the higher-order feature representations       of the adjectives in our thesaurus to match them with the frame.\nin the base model to make them more relevant for the sentiment            We used the GPT-3 playground using the headline as input and\nanalysis task. In this way, instead of having to obtain a very large      asking for the frame as output, but the strategy did not work. In\ndataset with target labels to train a model, we can reuse the pre-        our final experiment, instead of giving the headline as input, we\ntrained model and use a much smaller train dataset. We use a part         gave the definitions of each type of frame plus the headline, and we\nof our dataset as examples for the model to learn the task, while         asked the model to choose between the different types of frames\nthe other part of the dataset is used to evaluate model performance.      as output. In this way, the output of the model was directly one of\n    Previous works related to frame classification in the computing       the frames, and we avoided the step of matching adjectives with\nliterature have used fine-tuning, BERT-based models. In our work,         frames. An example is shown in Figure 2.\nwe have done the same as a baseline, but we aimed to go one step\nfurther and also produce results using fine-tuning of GPT-3.5.\n\n\n4.3    Prompt-engineering with GPT-3.5\nModel fine-tuning has been widely used, but with the emergence\nof generative models such as GPT-3, another way to approach\nclassification tasks has appeared. The idea is to use the pre-trained\nmodel directly and convert the task to be performed into a format\nas close as possible to the tasks for which it has been pre-trained.\nThat is, if the model has been pre-trained from next word prediction          Figure 2: GPT-3.5 for frame inference: input and output\nas in the case of GPT-3, classification can be done by defining a\nprompt, where the input to the model is an incomplete sentence,               For the GPT-3 configuration 1 , there are 3 main concepts:\nand the model must complete it with a word or several words, just\n                                                                                \u2022 TEMPERATURE [0-1]. This parameter controls randomness,\nas it has been trained. This avoids having to use part of the already\n                                                                                  lowering it results in less random completions.\nlabeled dataset to teach the task to be performed to the model, and\n                                                                                \u2022 TOP_P [0-1]. This parameter controls diversity via nucleus\na previous labeling is not needed [36].\n                                                                                  sampling.\n    In this approach, instead of adapting pre-trained LMs to down-\n                                                                                \u2022 MAX_TOKENS[1-4000]. This parameter indicates the maxi-\nstream tasks via objective engineering, downstream tasks are re-\n                                                                                  mum number of tokens to generate,\nformulated to look more like those solved during the original LM\n                                                                                \u2022 MODEL. GPT-3 offer four main models with different levels\ntraining with the help of a textual prompt. For example, when recog-\n                                                                                  of power, suitable for different tasks. Davinci is the most\nnizing the emotion of a social media post, \u201cI missed the bus today.\u201d,\n                                                                                  capable model, and Ada is the fastest.\nwe may continue with a prompt \u201cI felt so _\u201d, and ask the LM to\nfill the blank with an emotion-bearing word. Or if we choose the          After testing with the GPT-3 playground and varying different\nprompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may           hyper-parameters to assess performance, we set the temperature to\nbe able to fill in the blank with a French translation. In this way,      0, since the higher the temperature the more random the response.\nby selecting the appropriate prompts, we can influence the model          Furthermore, the Top-p parameter was set to 1, as it would likely\nbehavior so that the pre-trained LM itself can be used to predict the     get a set of the most likely words for the model to choose from. The\ndesired output, even without any additional task-specific training        maximum number of tokens was set to 2; in this way, the model\n[36].                                                                     is asked to choose between one of the responses. As a model, we\n    We use this emerging NLP approach to classify frames at headline      used the one with the best performance at the time of experimental\nlevel. We are not aware of previous uses of this strategy to classify     design, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\nframes as we propose here. The idea is the following. Prompt engi-\nneering consists of giving a prompt to the model, and understands         5     RESULTS: HUMAN LABELING OF FRAMES\nthat prompt as an incomplete sentence. To do prompt engineer-                   IN NO-VAX NEWS HEADLINES (RQ1)\ning with our dataset, we needed to define an appropriate prompt           In this section, we present and discuss the results of the analysis\nthat would produce the headline frames as output. We defined sev-         related to our first RQ.\neral experiments with the Playground of GPT-3, in order to find              Figure 3 shows the distribution of frames per country at headline\nthe best prompt for our task. In our initial experiments, we fol-         level, with human interest and no-frame being the predominant\nlowed existing approaches in prompt engineering to do sentiment\nanalysis, where the individual answer was an adjective, and this          1 https://beta.openai.com/docs/introduction\n\n\n\n\n                                                                    631\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                       David Alonso del Barrio and Daniel Gatica-Perez\n\n\nones. Attribution of responsibility is the third one except in Switzer-\nland, where the corresponding frame is conflict. Finally, morality\nand economic are the least represented in the dataset for every\ncountry.\n\n\n\n\n                                                                            Figure 4: Non-normalized monthly distribution of frames.\n\n\n\n                                                                            detail the negative and positive sentiment of each frame category,\n                                                                            we observed a few trends:\n                                                                                \u2022 Attribution of responsibility: Negative sentiment represents\n                                                                                  30-40% of the cases, while positive tone is only found in\n                                                                                  residual form in Italy, Switzerland, and the United Kingdom.\n                                                                                \u2022 Conflict: Negative sentiment represents 20-35% of the cases.\nFigure 3: Non-normalized distribution of frames per country                     \u2022 Economic: Predominantly neutral, with only negative tone\n                                                                                  in Italy and UK (in the latter case, all headlines with this\n    The monthly distribution of frames aggregated for all countries               frame were considered negative.)\nis shown in Fig. 4. We can see two big peaks, the first one in January          \u2022 Human interest: Negative sentiment represents 30-40% of\n2021 and the second one in August 2021. In all countries, the vac-                the cases, while positive tone is only found in residual form\ncination process started at the end of December 2020, so it makes                 in Italy, Spain, and Switzerland.\nsense that the no-vax movement started to be more predominant in                \u2022 Morality: Predominantly neutral, with negative tone in Italy,\nthe news in January 2021. Human interest is the most predominant                  Switzerland, and the United Kingdom,\nframe. Manual inspection shows that this is because the headlines               \u2022 No frame: 20-30% of negative content.\nare about personal cases of people who are pro- or anti- vaccine.\nAttribution of responsibility is also present. Manual inspection in-\ndicates that local politicians and health authorities had to make\ndecisions about who could be vaccinated at the beginning of the\nprocess. The second peak at the end of summer 2021 coincided\nwith the health pass (also called Covid passport in some countries),\nand we can observe a peak in the curve corresponding to the con-\nflict frame, reflecting the demonstrations against the measure of\nmandatory health passes taken by country governments.\n    In Figure 5, we compare the sentiment per frame and per country,\nto understand if there were any major differences. The sentiment\nanalysis labels were obtained using BERT-sent from the Hugging\nFace package [47], used in our previous work (please refer to our\noriginal analysis in [3] for details.) We normalized the results be-\ntween 0 and 1 to compare frames between countries. We see that the\nsentiment is predominantly neutral (in blue). Examining in more              Figure 5: Sentiment of headline by frame and by country\n\n\n\n\n                                                                      632\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                   ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n   Regarding the results of the annotation process, the fact that the             On the other hand, BERT is open-source, while GPT-3 has an\ndistribution of the 6 frame types is relatively similar between coun-         economic cost as the use of the model is not free, which monetarily\ntries suggests that the anti-vaccine movement issue was treated               limits the number of experiments that can be performed with it,\nin a similar way in these countries. The fact that human interest             as well as the different configurations one can explore to improve\nis the most dominant frame indicates that this issue was treated              performance. This is important because much of the improvement\nfrom a more human and emotional approach, with headlines about                in performance requires empirical explorations of model parameters\npersonal experiences, celebrities giving their opinion about vacci-           More specifically, the cost of an experiment for each of the folds has\nnation, and politicians defending vaccine policies. Moreover, the             a cost of 4 dollars (at the time of writing this paper.) This represents\nreason for many headlines being classified as no-frame is partly              a limitation in practice.\ndue to how data was selected. We chose articles that contained                    Furthermore, GPT-3 has a significant carbon footprint. Similarly,\nwords related to no-vax, either in the headline or in the article. This       for prompt engineering (discussed in the next subsection), choosing\nresulted in many headlines not containing anything specific related           the right prompt (i.e., the words that best define the task so that the\nto no-vax, while the no-vax content was actually included in the              model is able to perform adequately) is also based on trial and error.\nmain text of the corresponding articles.                                      This also has an impact on carbon footprint. In connection with\n   It is worth mentioning that prior to obtaining the results, we had         this topic, Strubell et al.[55] argue that improvements in the accu-\nexpected that attribution of responsibility would be among the most           racy of models depend on the availability of large computational\nprominent frames, since governments took many measures such as                resources, which involve large economic and environmental costs.\nmandatory health pass requirements to access certain sites; we had            A criticism has been made as \u2019the rich get richer\u2019, in the sense that\nalso expected that the conflict frame would be prominent, since               not all research groups have sufficient infrastructure resources and\nthere were many demonstrations in Europe. In reality, however,                access to funding needed to use these models and improve their\nthese frames categories were not reflected as frequently at the               performance. Also in relation to this analysis, the work of Bender\nheadline level.                                                               et al. [4] evaluates the costs and risks of the use of large language\n   Regarding the analysis at the temporal level, it is clear that certain     models, stating that researchers should be aware of the impact that\nevents were captured by the press, such as the start of vaccination           these models have on the environment, and assess whether the\nor the mandatory vaccination passport.                                        benefits outweigh the risks. The work in [4] provides a very telling\n   Finally, the sentiment analysis of the different frames shows that         example, where people living in the Maldives or Sudan are affected\nthe predominant tone in all of them is neutral or negative, with very         by floods and pay the environmental price of training English LLMs,\nsimilar trends between countries. This association between senti-             when similar models have not been produced for languages like\nment analysis and frames has been discussed in previous literature            Dhivehi or Sudanese Arab. In short, there is a need to establish\n[11] [43].                                                                    ways to use this technological development responsibly, and it all\n                                                                              starts with being aware of the risks it presents.\n6     RESULTS: GPT-3.5 FOR FRAME\n      CLASSIFICATION OF HEADLINES (RQ2)                                       6.2    Prompt-engineering with GPT-3.5\nHere, we present and discuss the results related to our second RQ.            For each headline, we got the frame that the model considered the\n                                                                              most likely, and we compared these GPT-3.5 inferences with the\n6.1    Fine-tuning GPT-3.5                                                    frames labeled by the annotators. The agreement between model\nTable 4 shows the results of the 6-class classification task using            and annotator was of 49%. Analyzing the results, and specifically\n5-cross validation. Three models were used: GPT-3.5 and two BERT-             looking at the cases where the annotator and GPT-3.5 disagreed,\nbased models. We observe that, on average, GPT-3.5 performs better            we discovered that according to the frame definitions, the model\nthan the BERT-based models. This is somehow expected as GPT-                  in some cases proposed a frame that indeed made sense. This ob-\n3.5 is a much larger model. Overall, in the case of fine-tuning, the          servation, together with our previous experience in the annotation\nbest performance for the six-class frame classification task is 72%           process, where headlines could have more than one valid frame,\naccuracy, which is promising, with an improvement over previous               led us to design a second post-hoc experiment. We took all the\nmodels based on BERT. Yet, it should be noted that the performance            headlines where each of the two annotators had disagreed with\ndifferences are modest (2% improvement between GPT-3.5 and                    GPT-3.5, and we asked the annotators to state whether they would\nRoBERTa).                                                                     agree (or not) with each GPT-inferred label for a given headline.\n                                                                              It is important to emphasize that the annotators did not know the\n                                                                              origin of that label, i.e., they did not know if it was the label they\nTable 4: Classification results for six-class frame classifica-               had originally assigned, or if it was a random one. In this way, we\ntion and 5-fold cross validation                                              could quantify how GPT-3.5 worked according to valid arguments\n                                                                              provided by the annotators. In this post-hoc experiment, the model\n                                                                              agreed in 76% of cases with the annotators.\n    ACCURACY          0       1       2       3       4     AVERAGE\n                                                                                  Looking at the results of the classification models, the 49% accu-\n    BERT            0.68     0.69    0.72    0.64    0.70     0.67            racy of the prompt-engineering approach can be considered low,\n    RoBERTa         0.70     0.72    0.72    0.67    0.71     0.70\n                                                                              yet we consider that it is a valid avenue for further investigation,\n    GPT3            0.75     0.70    0.72    0.71    0.71     0.72\n                                                                              as in the second post-hoc analysis, we found that the model agrees\n\n\n\n\n                                                                        633\n\fICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece                                                                David Alonso del Barrio and Daniel Gatica-Perez\n\n\nwith human annotators in 76% of the cases. Clearly, framing in-             and in an post-doc experiment, the human-machine agreement\nvolves aspects of subjectivity [42]. Much of what we do as people           reached 76%. These results have opened several new directions for\nhas a subjective component, influenced by how we feel or how we             future work.\nexpress opinions.\n   News reading is never fully objective, and the annotators en-            ACKNOWLEDGMENTS\ngaged in the frame classification task, influenced by their personal        This work was supported by the AI4Media project, funded by the\nstate of mind, experience, and culture, may perceive information            European Commission (Grant 951911) under the H2020 Programme\ndifferently. Monarch affirms that \"for simple tasks, like binary labels     ICT-48-2020. We also thank the newspapers for sharing their online\non objective tasks, the statistics are fairly straightforward to decide     articles. Finally, we thank our colleagues Haeeun Kim and Emma\nwhich is the \u2018correct\u2019 label when different annotators disagree. But        Bouton-Bessac for their support with annotations, and Victor Bros\nfor subjective tasks, or even objective tasks with continuous data,         and Oleksii Polegkyi for discussions.\nthere are no simple heuristics for deciding what the correct label\nshould be\" [42].                                                            REFERENCES\n   Subjectivity is involved in both the generation and perception            [1] Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online\nof information: the assumption that there is only one frame is com-              media: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.\n                                                                                 Jurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.\nplicated by the point of view of the reader. In the case of news, the        [2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess\ninformation sender (the journalist) has an intention, but the receiver           Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. 2021.\n(the reader) plays a role and is influenced by it. In psychology, this           RAFT: A real-world few-shot text classification benchmark. arXiv preprint\n                                                                                 arXiv:2109.14076 (2021).\nis known as the lens model of interpersonal communication, where             [3] David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press\nthe sender has certain objectives, but the receiver can interpret                Cover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35\u201343.\n                                                                                 https://doi.org/10.1145/3512732.3533588\nor re-interpret what the sender wants to say, with more or less              [4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\naccuracy [26].                                                                   Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\n   Following this discussion on subjectivity, the question arose as to           Be Too Big? (2021), 610\u2013623.\n                                                                             [5] Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in\nwhat would happen if, instead of headlines, we used the complete                 journalism: A boon or bane? In Optimization in machine learning and applications.\narticle as a source of analysis. We wondered if longer text could                Springer, 155\u2013167.\nmake the frame labeling task clearer than when using headlines.              [6] Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015.\n                                                                                 Media portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013\nYet another possible hypothesis is that having to read longer texts              2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.\ncould lead to the same subject being presented from different angles.        [7] Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.\n                                                                                 https://doi.org/10.48550/ARXIV.2212.14402\nPlease recall that in the existing literature discussed in Section 2,        [8] Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe,\nboth headlines and full articles have been used from frame analysis              Michel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour-\n(see Table 1.) This remains as an issue for future work.                         nalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695.\n                                                                             [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                 Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                 Askell, et al. 2020. Language models are few-shot learners. Advances in neural\n7    CONCLUSIONS                                                                 information processing systems 33 (2020), 1877\u20131901.\n                                                                            [10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H\nIn this paper, we first presented an analysis of human-generated                 De Vreese. 2014. Teaching the computer to code frames in news: Comparing\nnews frames on the covid-19 no-vax movement in Europe, and                       two supervised machine learning approaches to frame analysis. Communication\n                                                                                 Methods and Measures 8, 3 (2014), 190\u2013206.\nthen studied different approaches using large language models for           [11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond\nautomatic inference of frames. We conclude by answering the two                  words: Applying cluster and sentiment analysis to news coverage of the nuclear\nresearch questions we posed:                                                     power issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.\n                                                                            [12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015.\n   RQ1: What are the main frames in the news headlines about the                 The Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015),\ncovid-19 anti-vaccine movement in 5 European countries? After                    438\u2013444. https://doi.org/10.3115/v1/P15-2072\nannotating the headlines, we found that of the 1786 headlines,              [13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age\n                                                                                 of coronavirus and fake news: analysis of journalistic sources in the Spanish\nthe predominant frame is human interest (45.3% of cases), which                  quality press. International Journal of Environmental Research and Public Health\npresents a news item with an emotional angle, putting a face to a                17, 21 (2020), 8136.\n                                                                            [14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust\nproblem or situation. We also found that a substantial proportion                of vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n\nof headlines were annotated as not presenting any frame (40.2% of                social 74 (2019), 786\u2013802.\ncases). Finally, the other frame types are found more infrequently.         [15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology\n                                                                                 for evaluating data journalism, computational journalism, and computer-assisted\n   RQ2: Can prompt engineering be used for classification of head-               reporting. Digital journalism 3, 3 (2015), 331\u2013348.\nlines according to frames? We first used fine-tuning of a number of         [16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News\nlanguage models, and found that GPT-3.5 produced classification ac-              Framing Analysis. Routledge, 151\u2013172.\n                                                                            [17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,\ncuracy of 72% on a six-frame classification task. This represented a             1 (2021), 113\u2013118.\nmodest 2% improvement over BERT-based models, at a significantly            [18] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A de-\n                                                                                 ductive frame-analysis of Dutch and French climate change coverage dur-\nlarger environmental cost. We then presented a new way of classi-                ing the annual UN Conferences of the Parties. Public Understanding of\nfying frames using prompts. At the headline level, inferences made               Science 19, 6 (2010), 732\u2013742.         https://doi.org/10.1177/0963662509352044\nwith GPT-3.5 reached 49% of agreement with human-generated                       arXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.\n                                                                            [19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame-\nframe labels. In many cases, the GPT-3.5 model inferred frame                    analysis of Dutch and French climate change coverage during the annual UN\ntypes that were considered as valid choices by human annotators,                 Conferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.\n\n\n\n\n                                                                      634\n\fFraming the News:\nFrom Human Perception to Large Language Model Inferences                                                                        ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\n\n[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig.             [46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with\n     2020. Gsum: A general framework for guided neural abstractive summarization.                 mixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).\n     arXiv preprint arXiv:2010.08014 (2020).                                                 [47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent.\n[21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news                https://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.\n     headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA            [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\n     Gesondheid (Online) 27 (2022), 1\u20138.                                                          et al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n[22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative                      1, 8 (2019), 9.\n     Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).                 [49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022.\n[23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.               Fake News Classification using transformer based enhanced LSTM and BERT.\n     McQuail\u2019s reader in mass communication theory 390 (1993), 397.                               International Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105.\n[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language                    https://doi.org/10.1016/j.ijcce.2022.03.003\n     models better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).                [50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational\n[25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across                    predictors. Cuadernos. info 50 (2021), 91\u2013112.\n     four nations: a topic modeling and sentiment analysis approach. Ieee Access 9           [51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\n     (2021), 36645\u201336656.                                                                         Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\n[26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En-                     Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language\n     coding and Decoding of Interpersonal Dispositions in Nonverbal Behavior.                     model. arXiv preprint arXiv:2211.05100 (2022).\n     Journal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https:              [52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content\n     //doi.org/10.1037//0022-3514.66.2.398                                                        Analysis of Press and Television News. Journal of Communication 50 (06 2000),\n[27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based                 93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x\n     transformer architectures for long document summarization. In Proceedings of            [53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em-\n     the 16th Conference of the European Chapter of the Association for Computational             manouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin\n     Linguistics: Main Volume. 1792\u20131810.                                                         Van Durme. 2021. Constrained language models yield few-shot semantic parsers.\n[28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al. 2022. Automated news                      arXiv preprint arXiv:2104.08768 (2021).\n     summarization using transformers. In Sustainable Advanced Computing. Springer,          [54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab-\n     249\u2013259.                                                                                     orative Work trends on Media Organizations: Mixing Qualitative and Quan-\n[29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar-                titative Approaches. Studies in Media and Communication 5 (04 2017), 63.\n     chy of hybridity in data and computational journalism. Digital Journalism 5, 2               https://doi.org/10.11114/smc.v5i1.2279\n     (2017), 159\u2013176.                                                                        [55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy\n[30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and                 considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n     text analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021),     [56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\n     300\u2013324.                                                                                     the capabilities, limitations, and societal impact of large language models. arXiv\n[31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative                  preprint arXiv:2102.02503 (2021).\n     analysis of large amounts of journalistic texts using topic modelling. Digital          [57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.\n     journalism 4, 1 (2016), 89\u2013106.                                                              arXiv preprint arXiv:1806.02847 (2018).\n[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we              [58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and\n     know what language models know? Transactions of the Association for Computa-                 Felix Hill. 2021. Multimodal few-shot learning with frozen language models.\n     tional Linguistics 8 (2020), 423\u2013438.                                                        Advances in Neural Information Processing Systems 34 (2021), 200\u2013212.\n[33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling                   [59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of\n     Political Framing Across Policy Issues and Contexts. In ALTA.                                technology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.\n[34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate             [60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling\n     during election years: Focus on generic frames. The Communication Review 21, 2               for frame analysis: A study of media debates on climate change in India and USA.\n     (2018), 89\u2013115.                                                                              Global Media and Communication 18, 1 (2022), 91\u2013112.\n[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\n     hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n     2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n     (2022).\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\n     Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of\n     prompting methods in natural language processing. (2021). https://doi.org/10.\n     48550/ARXIV.2107.13586\n[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019.\n     Detecting frames in news headlines and its application to analyzing news framing\n     trends surrounding US gun violence. In Proceedings of the 23rd conference on\n     computational natural language learning (CoNLL).\n[38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames:\n     Toward Improving Reliability and Validity. Journal of Communication 58 (06\n     2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x\n[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and\n     David E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3\n     User Simulation in Conversational AI. In Proceedings of the 4th Conference on\n     Conversational User Interfaces. 1\u20136.\n[40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social\n     computing for verifying social media content in breaking news. IEEE Internet\n     Computing 22, 2 (2018), 83\u201389.\n[41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021. \u2018Our task is to demystify fears\u2019:\n     Analysing newsroom management of automation in journalism. Journalism 22,\n     9 (2021), 2203\u20132221.\n[42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and\n     Annotation for Human-centered AI. Manning. https://books.google.ch/books?\n     id=LCh0zQEACAAJ\n[43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of\n     media frames: Strengths, weaknesses, and opportunities. Political Communication\n     38, 1-2 (2021), 159\u2013181.\n[44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to\n     news discourse. Political communication 10, 1 (1993), 55\u201375.\n[45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative\n     language models. arXiv preprint arXiv:1912.10165 (2019).\n\n\n\n\n                                                                                       635\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\n\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. 2020. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014 (2020).", "Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news headlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA Gesondheid (Online) 27 (2022), 1\u20138.", "Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative Analysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021)."], "publish_date": "2023-06-12", "approved": false}}, {"_index": "articles", "_id": "fSkxcI0Bf-PcuVTAvFJ8", "_score": 1.0, "_ignored": ["abstract.keyword", "full_text.keyword", "references.keyword"], "_source": {"title": "Large Language Model Augmented Narrative Driven Recommendations", "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.", "authors": ["Sheshera Mysore", "Andrew McCallum", "Hamed Zamani"], "institutions": ["University of Massachusetts Amherst"], "keywords": ["narrative-driven recommendation", "information access", "user-item interaction", "data augmentation", "retrieval models"], "full_text": "                 Large Language Model Augmented Narrative Driven\n                                Recommendations\n                  Sheshera Mysore                                              Andrew McCallum                                              Hamed Zamani\n          smysore@cs.umass.edu                                           mccallum@cs.umass.edu                                       hzamani@cs.umass.edu\n    University of Massachusetts Amherst                             University of Massachusetts Amherst                        University of Massachusetts Amherst\n                    USA                                                             USA                                                        USA\nABSTRACT                                                                                              interactions are effective, users soliciting recommendations often\nNarrative-driven recommendation (NDR) presents an information                                         start with a vague idea about their desired target items or may\naccess problem where users solicit recommendations with verbose                                       desire recommendations depending on the context of use, often\ndescriptions of their preferences and context, for example, travelers                                 missing in historical interaction data (Figure 1). In these scenarios,\nsoliciting recommendations for points of interest while describ-                                      it is common for users to solicit recommendations through long-\ning their likes/dislikes and travel circumstances. These requests                                     form narrative queries describing their broad interests and context.\nare increasingly important with the rise of natural language-based                                    Information access tasks like these have been studied as narrative-\nconversational interfaces for search and recommendation systems.                                      driven recommendations (NDR) for items ranging from books [5]\nHowever, NDR lacks abundant training data for models, and current                                     and movies [18], to points of interest [1]. Bogers and Koolen [5]\nplatforms commonly do not support these requests. Fortunately,                                        note these narrative requests to be common on discussion forums\nclassical user-item interaction datasets contain rich textual data,                                   and several subreddits1 , but, there is a lack of support for these\ne.g., reviews, which often describe user preferences and context                                      complex natural language queries in current recommenders.\n\u2013 this may be used to bootstrap training for NDR models. In this                                          However, with the emergence of conversational interfaces for\nwork, we explore using large language models (LLMs) for data                                          information access tasks, support for complex NDR tasks is likely\naugmentation to train NDR models. We use LLMs for authoring                                           to become necessary. In this context, recent work has noted an\nsynthetic narrative queries from user-item interactions with few-                                     increase in complex and subjective natural language requests com-\nshot prompting and train retrieval models for NDR on synthetic                                        pared to more conventional search interfaces [13, 34]. Furthermore,\nqueries and user-item interaction data. Our experiments demon-                                        the emergence of large language models (LLM) with strong lan-\nstrate that this is an effective strategy for training small-parameter                                guage understanding capabilities presents the potential for fulfilling\nretrieval models that outperform other retrieval and LLM baselines                                    such complex requests [9, 33]. This work explores the potential for\nfor narrative-driven recommendation.                                                                  re-purposing historical user-item recommendation datasets, tra-\n                                                                                                      ditionally used for training collaborative filtering recommenders,\nCCS CONCEPTS                                                                                          with LLMs to support NDR.\n                                                                                                          Specifically, given a user\u2019s interactions, \ud835\udc37\ud835\udc62 , with items and\n\u2022 Information systems \u2192 Recommender systems; Users and inter-\n                                                                                                      their accompanying text documents (e.g., reviews, descriptions)\nactive retrieval; \u2022 Computing methodologies \u2192 Natural language                                                   \ud835\udc41\ud835\udc62\ngeneration.                                                                                           \ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56 }\ud835\udc56=1  , selected from a user-item interaction dataset I, we\n                                                                                                      prompt InstructGPT, a 175B parameter LLM, to author a synthetic\nACM Reference Format:                                                                                 narrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the\nSheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large\n                                                                                                      query \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user\nLanguage Model Augmented Narrative Driven Recommendations. In Sev-\nenteenth ACM Conference on Recommender Systems (RecSys \u201923), Septem-\n                                                                                                      reviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based\nber 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages.                               on a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user doc-\nhttps://doi.org/10.1145/3604915.3608829                                                               ument, \ud835\udc51\ud835\udc56 . Then, a pre-trained LM based retrieval model (110M\n                                                                                                      parameters) is fine-tuned for retrieval on the synthetic queries and\n1     INTRODUCTION                                                                                    filtered reviews.\n                                                                                                          Our approach, which we refer to as Mint2 , follows from the\nRecommender systems personalized to users are an important com-\n                                                                                                      observation that while narrative queries and suggestions are often\nponent of several industry-scale platforms [16, 17, 46]. These sys-\n                                                                                                      made in online discussion forums, and could serve as training data,\ntems function by inferring users\u2019 interests from their prior inter-\n                                                                                                      the number of these posts and the diversity of domains for which\nactions on the platform and making recommendations based on\n                                                                                                      they are available is significantly smaller than the size and diversity\nthese inferred interests. While recommendations based on historical\n                                                                                                      of passively gathered user-item interaction datasets. E.g. while\nPermission to make digital or hard copies of all or part of this work for personal or                 Bogers and Koolen [5] note nearly 25,000 narrative requests for\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation             books on the LibraryThing discussion forum, a publicly available\non the first page. Copyrights for components of this work owned by others than the                    user-item interaction dataset for Goodreads contains interactions\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\n                                                                                                      with nearly 2.2M books by 460k users [43] .\nand/or a fee. Request permissions from permissions@acm.org.                                               We empirically evaluate Mint in a publicly available test collec-\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                               tion for point of interest recommendation: pointrec [1]. To train\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n                                                                                                      1 r/MovieSuggestions, r/booksuggestions, r/Animesuggest\nACM ISBN 979-8-4007-0241-9/23/09. . . $15.00\nhttps://doi.org/10.1145/3604915.3608829                                                               2 Mint: Data augMentation with INteraction narraTives.\n\n\n\n\n                                                                                                777\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                  Mysore, McCallum, Zamani\n\n\n\n\nFigure 1: An example narrative query soliciting point of                      Figure 2: The format of the prompt used in Mint for\ninterest recommendations. The query describes the users                       generating synthetic narrative queries from user-item\npreferences and the context of their request.                                 interaction with a large language model.\n\n\nour NDR models, we generate synthetic training data based on                  Besides creating queries for ad-hoc retrieval tasks, concurrent\nuser-item interaction datasets from Yelp. Models (110M parameters)         work of Leszczynski et al. [25] has also explored the creation of syn-\ntrained with Mint significantly outperform several baseline models         thetic conversational search datasets from music recommendation\nand match the performance of significantly larger LLM baselines            datasets with LLMs. The synthetic queries and user documents are\nautoregressively generating recommendations. Code and synthetic            then used to train bi-encoder retrieval models for conversational\ndatasets are available:3                                                   search. Our work resembles this in creating synthetic queries from\n                                                                           sets of user items found in recommendation interaction datasets.\n2    RELATED WORK                                                          However, it differs in the task of focus, creating long-form narra-\nData Augmentation for Information Access. A line of recent                 tive queries for NDR. Finally, our work also builds on the recent\nwork has explored using language models to generate synthetic              perspective of Radlinski et al. [36] who make a case for natural\nqueries for data augmentation to train models for information re-          language user profiles driving recommenders \u2013 narrative requests\ntrieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of     tie closely to natural language user profiles. Our work presents a\ninterest, a pre-trained language model is used to create synthetic         step toward these systems.\nqueries for the document collection. An optional filtering step ex-           Finally, while our work explores data augmentation from user-\ncludes noisy queries, and finally, a bi-encoder or a cross-encoder is      item interactions for a retrieval-oriented NDR task, prior work has\ntrained for the retrieval task. While earlier work of Ma et al. [31]       also explored data augmentation of the user-item graph for training\ntrain a custom query generation model on web-text datasets, more           collaborative filtering models. This work has often explored aug-\nrecent work has leveraged large language models for zero/few-shot          mentation to improve recommendation performance for minority\nquestion generation [7, 8, 15, 23]. In generating synthetic queries,       [12, 47] or cold-start users [11, 28, 45]. And has leveraged genera-\nthis work indicates the effectiveness of smaller parameter LLMs            tive models [11, 45] and text similarity models [28] for augmenting\n(up to 6B parameters) for generating synthetic queries in simpler          the user-item graph.\ninformation-retrieval tasks [7, 8, 23], and finds larger models (100B         Complex Queries in Information Access. With the advent\nparameters and above) to be necessary for harder tasks such as             of performant models for text understanding, focus on complex\nargument retrieval [15, 23]. Similar to this work, we explore the          and interactive information access tasks has seen a resurgence\ngeneration of synthetic queries with LLMs for a retrieval task. Un-        [2, 29, 32, 48]. NDR presents an example of this \u2013 NDR was first\nlike this work, we demonstrate a data augmentation method for              formalized in Bogers and Koolen [5] for the case of book recommen-\ncreating effective training data from sets of user documents found in      dation and subsequently studied in other domains [3, 4, 6]. Bogers\nrecommendation datasets rather than individual documents. Other            and Koolen [5] systematically examined narrative requests posted\nwork in this space has also explored training more efficient multi-        by users on discussion forums. They defined NDR as a task requir-\nvector models from synthetic queries instead of more expensive             ing item recommendation based on a long-form narrative query\ncross-encoder models [39] and generating queries with a diverse            and prior-user item interactions. While this formulation resembles\nrange of intents than the ones available in implicit feedback datasets     personalized search [42] and query-driven recommendation [20],\nto enhance item retrievability [35].                                       the length and complexity of requests differentiate these from NDR.\n                                                                           Other work has also demonstrated the effectiveness of re-ranking\n                                                                           initial recommendations from collaborative filtering approaches\n3 https://github.com/iesl/narrative-driven-rec-mint/\n\n\n\n\n                                                                     778\n\fLarge Language Model Augmented Narrative Driven Recommendations                                          RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\n\n\nFigure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering\nmodels for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with\na large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic\nqueries and user items.\n\n\nbased on the narrative query [18]. More recent work of Afzali et al.            3.2.1 Narrative Queries from LLMs. To author a narrative query \ud835\udc5e\ud835\udc62\n[1] formulate the NDR task without access to the prior interactions             for a user in I, we make use of the 175B parameter InstructGPT4\nof a user while also noting the value of contextual cues contained              model as our query generation model QGen. We include the text\n                                                                                                         \ud835\udc41\ud835\udc62\nin the narrative request. In our work, we focus on this latter for-             of interacted items {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 in the prompt for QGen, and instruct it\nmulation of NDR, given the lack of focus on effectively using the               to author a narrative query (Figure 2). To improve the coherence\nrich narrative queries in most prior work. Further, we demonstrate              of generated queries and obtain correctly formatted outputs, we\nthe usefulness of data augmentation from LLMs and user-item                     manually author narrative queries for 3 topically diverse users\ninteraction datasets lacking narrative queries.                                 based on their interacted items and include it in the prompt for\n   Besides this, a range of work has explored more complex, long-               QGen. The same three few shot examples are used for the whole\nform, and interactive query formulations for information access;                dataset I, and the three users were chosen from I. Generating\nthese resemble queries in NDR. Arguello et al. [2] define the tip of            narrative queries based on user interactions may also be considered\ntongue retrieval task, a known-item search task where user queries              a form of multi-document summarization for generating a natural\ndescribe the rich context of items while being unable to recall item            language user profile [36].\nmetadata itself. Mysore et al. [32] formulate an aspect conditional\nquery-by example task where results must match specific aspects of              3.2.2 Filtering Items for Synthetic Queries. Since we expect user\na long natural language query. And finally, a vibrant body of work              items to capture multiple aspects of their interests and generated\nhas explored conversational critiquing of recommenders where nat-               queries to only capture a subset of these interests, we only retain\n                                                                                                                    \ud835\udc41\ud835\udc62\nural language feedback helps tune the recommendations received                  some of the items present in {\ud835\udc51\ud835\udc56 }\ud835\udc56=1   before using it for training re-\nby users [30, 44, 49].                                                          trieval models. For this, we use a pre-trained language model to com-\n                                                                                pute the likelihood of the query given each user item, \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ),\n3 METHOD                                                                        and only retain the top \ud835\udc40 highly scoring item for \ud835\udc5e\ud835\udc62 , this re-\n                                                                                sults in \ud835\udc40 training samples per user for our NDR retrieval models:\n3.1 Problem Setup                                                                          \ud835\udc40 }. In our experiments, we use FlanT5 with 3B parame-\n                                                                                {(\ud835\udc5e\ud835\udc62 , \ud835\udc51\ud835\udc56 )\ud835\udc56=1\nIn our work, we define narrative-driven recommendation (NDR) to                 ters [14] for computing and follow Sachan et al. [40] for computing\nbe a ranking task, where given a narrative query \ud835\udc5e made by a user               \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ). Note that our use of \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ) represents a query-\n\ud835\udc62, a ranking system \ud835\udc53 must generate a ranking \ud835\udc45 over a collection               likelihood model classically used for ad-hoc search and recently\nof items C. Further, we assume access to a user-item interaction                shown to be an effective unsupervised re-ranking method when\n                                                              \ud835\udc41\ud835\udc62\ndataset I consisting of user interactions with items (\ud835\udc62, {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 ). We         used with large pre-trained language models [40].\nassume the items \ud835\udc51\ud835\udc56 to be textual documents like reviews or item\ndescriptions. While we don\u2019t assume there to be any overlap in the              3.2.3 Training Retrieval Models. We train bi-encoder and cross-\nusers making narrative queries or the collection of items C and the             encoder models for NDR on the generated synthetic dataset \u2013 com-\nuser-items interaction dataset I, we assume them to be from the                 monly used models in search tasks. Bi-encoders are commonly used\nsame broad domain, e.g., books, movies, points-of-interest.                     as scalable first-stage rankers from a large collection of items. On the\n                                                                                other hand, cross-encoders allow a richer interaction between query\n3.2    Proposed Method                                                          and item and are used as second-stage re-ranking models. For both\n                                                                                models, we use a pre-trained transformer language model architec-\nOur proposed method, Mint, for NDR, re-purposes a dataset of                    ture with 110M parameters, MPnet, a model similar to Bert [41].\n                                                           \ud835\udc41\ud835\udc62\nabundantly available user-item interactions, I = {(\ud835\udc62, {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 )} into          Bi-encoder models embed the query and item independently into\ntraining data for retrieval models by using LLMs as query gener-                high dimensional vectors: q\ud835\udc62 = MPNet(\ud835\udc5e\ud835\udc62 ), d\ud835\udc56 = MPNet(\ud835\udc51\ud835\udc56 ) and\n                                                                \ud835\udc41\ud835\udc62\nation models to author narrative queries \ud835\udc5e\ud835\udc62 : D = {(\ud835\udc5e\ud835\udc62 , {\ud835\udc51\ud835\udc56 }\ud835\udc56=1   )}.         rank items for the user based on the minimum L2 distance between\nThen, retrieval models are trained on the synthetic dataset D (Fig-\nure 3).                                                                         4 https://platform.openai.com/docs/models/gpt-3, text-davinci-003\n\n\n\n\n                                                                          779\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                   Mysore, McCallum, Zamani\n\n\nq\ud835\udc62 and d\ud835\udc56 . Embeddings are obtained by averaging token embeddings           dislikes). It also retains the users whose interests are summarizable\nfrom the final layer of MPNet, and the same model is used for both          by QGen. In the Yelp dataset, this results in 45,193 retained users.\nqueries and items. Cross-encoder models input both the query and            Now, 10,000 randomly selected users are chosen for generating syn-\nitem and output a score to be used for ranking\u0010 \ud835\udc60 = \ud835\udc53Cr ([\ud835\udc5e\ud835\udc62\u0011; \ud835\udc51\ud835\udc56 ]),       thetic narrative queries. For these users, a single randomly selected\nwhere \ud835\udc53Cr is parameterized as w\ud835\udc47 dropout W\ud835\udc47 MPNet(\u00b7) . We                   sentence from 10 of their reviews is included in the prompt (Figure\n                                                                            2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some\ntrain our bi-encoder model with a margin ranking loss: L\ud835\udc35\ud835\udc56 =\n\u00cd \u00cd\ud835\udc40                                \u2032                                       items are filtered out (\u00a73.2.2). Here, we exclude 40% of the items\n  \ud835\udc62 \ud835\udc56=1 max[\ud835\udc3f2(q\ud835\udc62 , d\ud835\udc56 ) \u2212 \ud835\udc3f2(q\ud835\udc62 , d\ud835\udc56 ) + \ud835\udeff, 0] with randomly sam-          for a user. This results in about 60,000 training samples for training\n                \u2032\npled negatives \ud835\udc51 and \ud835\udeff = 1. Our cross-encoders are trained with             BiEnc-Mint and CrEnc-Mint. These decisions were made manu-\n                            \u00cd \u00cd\ud835\udc40                \ud835\udc60\na cross-entropy loss: L\ud835\udc36\ud835\udc5f = \ud835\udc62 \ud835\udc56=1       log( \u00cd \ud835\udc52 \ud835\udc60 \u2032 ). For training, 4     ally by examining the resulting datasets and the cost of authoring\n                                                      \ud835\udc51\u2032 \ud835\udc52\nnegative example items \ud835\udc51 \u2032 are randomly sampled from ranks 100-             queries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230.\n300 from our trained bi-encoder. At test time, we retrieve the top\n200 items with our trained bi-encoder and re-rank them with the             4.1.3 Baselines. We compare BiEnc-Mint and CrEnc-Mint mod-\ncross-encoder - we evaluate both these components in experiments            els against several standard and performant retrieval model base-\nand refer to them as BiEnc-Mint and CrEnc-Mint.                             lines. These span zero-shot/unsupervised rankers, supervised bi-\n                                                                            encoders, unsupervised cross-encoders, and LLM baselines. BM25:\n4     EXPERIMENTS AND RESULTS                                               A standard unsupervised sparse retrieval baseline based on term\n                                                                            overlap between query and document, with strong generalization\nNext, we evaluate Mint on a publicly available test collection for\n                                                                            performance across tasks and domains [38]. Contriver: A BERT-base\nNDR and present a series of ablations.\n                                                                            bi-encoder model pre-trained for zero-shot retrieval with weakly su-\n                                                                            pervised query-document pairs [22]. MPNet-1B: A strong Sentence-\n4.1     Experimental Setup                                                  Bert bi-encoder model initialized with MPNet-base and trained on\n4.1.1 Datasets. We perform evaluations on an NDR dataset for                1 billion supervised query-document pairs aggregated from numer-\npoint-of-interest (POI) recommendation Pointrec [1]. Pointrec               ous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned\ncontains 112 realistic narrative queries (130 words long) obtained          on supervised question-passage pairs from MSMarco. UPR: A two-\nfrom discussion forums on Reddit and items pooled from baseline             stage approach that retrieves items with a Contriver bi-encoder\nrankers. The items are annotated on a graded relevance scale by             and re-ranks the top 200 items with a query-likelihood model using\ncrowd-workers and/or discussion forum members and further vali-             a FlanT5 model with 3B parameters [14, 40]. This may be seen\ndated by the dataset authors. The item collection C in Pointrec             as an unsupervised \u201ccross-encoder\u201d model. Grounded LLM: A re-\ncontains 700k POIs with metadata (category, city) and noisy text            cently proposed two-stage approach which autoregressively gener-\nsnippets describing the POI obtained from the Bing search engine.           ates ten pseudo-relevant items using an LLM (175B InstructGPT)\nFor test time ranking, we only rank the candidate items in the city         prompted with the narrative query and generates recommenda-\nand request category (e.g., \u201cRestaurants\u201d) of the query available in        tions grounded in C by retrieving the nearest neighbors for each\nPointrec - this follows prior practice to exclude clearly irrelevant        generated item using a bi-encoder [19]. We include one few-shot\nitems [1, 26]. We use user-item interaction datasets from Yelp to           example of a narrative query and recommended items in the prompt\ngenerate synthetic queries for training.5 Note also that we limit our       to the LLM. We run this baseline three times and report average\nevaluations to Pointrec since it presents the only publicly avail-          performance across runs. We report NDCG at 5 and 10, MAP, MRR,\nable, manually annotated, and candidate pooled test collection for          and Recall at 100 and 200. Finally, our reported results should be\nNDR, to our knowledge. Other datasets for NDR use document col-             considered lower bounds on realistic performance due to the un-\nlections that are no longer publicly accessible [24], contain sparse        judged documents (about 70% at \ud835\udc58 = 10) in our test collections\nand noisy relevance judgments due to them being determined with             [10].\nautomatic rules applied to discussion threads [18, 24], lack pooling\nto gather candidates for judging relevance [18, 24], or lack realistic      4.2    Results\nnarrative queries [21]. We leave the development of more robust\n                                                                            Table 1 presents the performance of the proposed method compared\ntest collections and evaluation methods for NDR to future work.\n                                                                            against baselines. Here, bold numbers indicate the best-performing\n4.1.2 Implementation Details. Next, we describe important details           model, and superscripts indicate statistical significance computed\nfor Mint and leave finer details of the model and training to our           with two-sided t-tests at \ud835\udc5d < 0.05.\ncode release. To sample user interactions for generating synthetic             Here, we first note the performance of baseline approaches. We\nqueries from the Yelp dataset, we exclude POIs and users with               see BM25 outperformed by Contriver, a transformer bi-encoder\nfewer than ten reviews to ensure that users were regular users of           model trained for zero-shot retrieval; this mirrors prior work [22].\nthe site with well represented interests. This follows common prior         Next, we see supervised bi-encoder models trained on similar pas-\npractice in preparing user-item interaction datasets for use [27].          sage (MPNet-1B) and question-answer (BERT-MSM) pairs outper-\nThen we retain users who deliver an average rating greater than             form a weakly supervised model (Contriver) by smaller margins.\n3/5 and with 10-30 above-average reviews. This desirably biases             Finally, the Grounded LLM outperforms all bi-encoder baselines, in-\nour data to users who commonly describe their likings (rather than          dicating strong few-shot generalization and mirroring prior results\n                                                                            [19]. Examining the Mint models, we first note that the BiEnc-\n5 https://www.yelp.com/dataset                                              Mint sees statistically significant improvement compared to BM25\n\n\n\n\n                                                                      780\n\fLarge Language Model Augmented Narrative Driven Recommendations                                       RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\nTable 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts\ndenote statistically significant improvements compared to specific baseline models.\n\n                                                                                        Pointrec\n                              Model     Parameters      NDCG@5     NDCG@10        MAP        MRR          Recall@100      Recall@200\n                             1 BM25          -          0.2682     0.2464         0.1182     0.2685       0.4194          0.5429\n                         2 Contriver       110M         0.2924     0.2776         0.1660     0.3355       0.4455          0.5552\n                        3 MPNet-1B         110M         0.3038     0.2842         0.1621     0.3566       0.4439          0.5657\n                      4 BERT-MSM           110M         0.3117     0.2886         0.1528     0.3320       0.4679          0.5816\n                  5 Grounded LLM        175B+110M       0.3558     0.3251         0.1808     0.3861       0.4797          0.5797\n                               6 UPR     110M+3B        0.3586     0.3242         0.1712     0.4013       0.4489          0.5552\n                       BiEnc-Mint          110M         0.34891    0.32631        0.18901    0.39821      0.49141         0.6221\n                       CrEnc-Mint         2\u00d7110M        0.372512   0.348912       0.219214   0.43171      0.5448123       0.6221\n\n\nand outperforms the best bi-encoder baselines by 11-13% on preci-                6B LLM for Item Queries. We find a smaller 6B LLM to result\nsion measures and 5-7% on recall measures. Specifically, we see a             in poor quality data when used to generate narrative queries con-\n                                                                                                \ud835\udc41\ud835\udc62\nmodel trained for question-answering (BERT-MSM) underperform                  ditioned on {\ud835\udc51\ud835\udc56 }\ud835\udc56=1  . Here we simplify the text generation task \u2013\nBiEnc-Mint, indicating the challenge of the NDR task. Further,                using a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56 . This\nBiEnc-Mint, trained on 5 orders of magnitude lesser data than                 experiment also mirrors the setup for generating synthetic queries\nMPNet-1B, sees improved performance \u2013 indicating the quality of               for search tasks [7, 15]. Here, we use 3-few shot examples and sam-\ndata obtained from Mint. Furthermore, BiEnc-Mint also performs                ple one item per user for generating \ud835\udc5e\ud835\udc62 . Given the lower cost of\nat par with a 175B LLM while offering the inference efficiency of a           using a smaller LLM, we use all 45,193 users in our Yelp dataset\nsmall-parameter bi-encoder. Next, we see CrEnc-Mint outperform                rather than a smaller random sample. From Table 2, we see that this\nthe baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM                   results in higher quality queries than using smaller LLMs for gen-\nby 4-21% on precision measures and 7-13% on recall measures \u2013                                                      \ud835\udc41\ud835\udc62\n                                                                              erating narrative queries from {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 . The resulting BiEnc model\ndemonstrating the value of Mint for training NDR models.                      underperforms the BiEnc-Mint, indicating the value of generating\n                                                                              complex queries conditioned on multiple items as in Mint for NDR.\n4.3    Ablations                                                              We see that CrEnc approaches the performance of CrEnc-Mint\u2013\nIn Table 2, we ablate various design choices in Mint. Different               note, however, that this approach uses the performant BiEnc-Mint\nchoices result in different training sets for the BiEnc and CrEnc             for sampling negatives and first stage ranking. We leave further\nmodels. Also, note that in reporting ablation performance for CrEnc,          exploration of using small parameter LLMs for data augmentation\nwe still use the performant BiEnc-Mint model for obtaining nega-              for NDR models to future work.\ntive examples for training and first-stage ranking. Without high-\nquality negative examples, we found CrEnc to result in much poorer            5    CONCLUSIONS\nperformance.                                                                  In this paper, we present Mint, a data augmentation method for the\n   No item filtering. Since synthetic queries are unlikely to rep-            narrative-driven recommendation (NDR) task. Mint re-purposes\n                                                                  \ud835\udc41\ud835\udc62\nresent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56 }\ud835\udc56=1             historical user-item interaction datasets for NDR by using a 175B pa-\nwhich have a low likelihood of being generated from the document              rameter large language model to author long-form narrative queries\n(\u00a73.2.2). Without this step, we expect the training set for training          while conditioning on the text of items liked by users. We evaluate\nretrieval models to be larger and noisier. In Table 2, we see that            bi-encoder and cross-encoder models trained on data from Mint on\nexcluding this step leads to a lower performance for BiEnc and                the publicly available Pointrec test collection for narrative-driven\nCrEnc, indicating that the quality of data obtained is important for          point of interest recommendation. We demonstrate that the result-\nperformance.                                                                  ing models outperform several strong baselines and ablated models\n   6B LLM for QGen. Mint relies on using an expensive 175B pa-                and match or outperform a 175B LLM directly used for NDR in a\nrameter InstructGPT model for QGen. Here, we investigate the                  1-shot setup.\n                                    \ud835\udc41\ud835\udc62\nefficacy for generating \ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56 }\ud835\udc56=1 with a 6B parameter Instruct-            However, Mint also presents some limitations. Given our use of\nGPT model (text-curie-001). We use an identical setup to the                  historical interaction datasets for generating synthetic training data\n175B LLM for this. In Table 2, we see that training on the synthetic          and the prevalence of popular interests in these datasets longer,\nnarrative queries of the smaller LLM results in worse models \u2013 of-            tailed interests are unlikely to be present in the generated syn-\nten underperforming the baselines in Table 1. This indicates the              thetic datasets. In turn, causing retrieval models to likely see poorer\ninability of a smaller model to generate complex narrative queries            performance on these requests. Our use of LLMs to generate syn-\nwhile conditioning on a set of user items. This necessity of a larger         thetic queries also causes the queries to be repetitive in structure,\nLLM for generating queries in complex retrieval tasks has been                likely causing novel longer-tail queries to be poorly served. These\nobserved in prior work [15, 23].                                              limitations may be addressed in future work.\n\n\n\n\n                                                                      781\n\fRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore                                                                                               Mysore, McCallum, Zamani\n\n\n                                             Table 2: Mint ablated for different design choices on Pointrec.\n\n                                                                                                      Pointrec\n                            Ablation                                NDCG@5          NDCG@10      MAP         MRR        Recall@100        Recall@200\n                            BiEnc-Mint                              0.3489          0.3263       0.1890      0.3982     0.5263            0.6221\n                            \u2212 No item filtering                     0.2949          0.2766       0.1634      0.3505     0.4979            0.5951\n                            \u2212 6B LLM for QGen                       0.2336          0.2293       0.1125      0.2287     0.426             0.5435\n                            \u2212 6B LLM for Item Queries               0.3012          0.2875       0.1721      0.3384     0.4800            0.5909\n                            CrEnc-Mint                              0.3725          0.3489       0.2192      0.4317     0.5448            0.6221\n                            \u2212 No item filtering                     0.3570          0.3379       0.2071      0.4063     0.5366            0.6221\n                            \u2212 6B LLM for QGen                       0.2618          0.2421       0.1341      0.3118     0.4841            0.6221\n                            \u2212 6B LLM for Item Queries               0.3792          0.3451       0.2128      0.4098     0.5546            0.6221\n\n\n   Besides this, other avenues also present rich future work. While                                Society: 14th International Conference, iConference 2019, Washington, DC, USA,\nMint leverages a 175B LLM for generating synthetic queries, smaller                                March 31\u2013April 3, 2019, Proceedings 14. Springer, 503\u2013515.\n                                                                                               [5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven\nparameter LLMs may be explored for this purpose - perhaps by                                       Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender\ntraining dedicated QGen models. Mint may also be expanded to                                       Systems (Como, Italy) (RecSys \u201917). Association for Computing Machinery, New\n                                                                                                   York, NY, USA, 238\u2013242. https://doi.org/10.1145/3109859.3109893\nexplore more active strategies for sampling items and users for                                [6] Toine Bogers and Marijn Koolen. 2018. \u201cI\u2019m looking for something like. . . \u201d:\nwhom narrative queries are authored - this may allow more effi-                                    Combining Narratives and Example Items for Narrative-driven Book Recommen-\ncient use of large parameter LLMs while ensuring higher quality                                    dation. In Knowledge-aware and Conversational Recommender Systems Workshop.\n                                                                                                   CEUR Workshop Proceedings.\ntraining datasets. Next, the generation of synthetic queries from                              [7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nsets of documents may be explored for a broader range of retrieval                                 InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings\ntasks beyond NDR given its promise to generate larger training                                     of the 45th International ACM SIGIR Conference on Research and Development\n                                                                                                   in Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing\nsets \u2013 a currently underexplored direction. Finally, given the lack of                             Machinery, New York, NY, USA, 2387\u20132392. https://doi.org/10.1145/3477495.\nlarger-scale test collections for NDR and the effectiveness of LLMs                                3531863\n                                                                                               [8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,\nfor authoring narrative queries from user-item interaction, fruitful                               Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-\nfuture work may also explore the creation of larger-scale datasets                                 pervised Training of Efficient Rankers. arXiv:2301.02998\nin a mixed-initiative setup to robustly evaluate models for NDR.                               [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\n                                                                                                   Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n                                                                                                   Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\n                                                                                                   Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nACKNOWLEDGMENTS                                                                                    Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nWe thank anonymous reviewers for their invaluable feedback. This                                   Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n                                                                                                   Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nwork was partly supported by the Center for Intelligent Informa-                                   In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-\ntion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of                                  zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nNaval Research contract number N000142212688, an Amazon Alexa                                      Inc., 1877\u20131901. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n                                                                                                   1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nPrize grant, and the Chan Zuckerberg Initiative under the project                             [10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nScientific Knowledge Base Construction. Any opinions, findings                                     Information. In Proceedings of the 27th Annual International ACM SIGIR Conference\n                                                                                                   on Research and Development in Information Retrieval (Sheffield, United Kingdom)\nand conclusions or recommendations expressed here are those of                                     (SIGIR \u201904). Association for Computing Machinery, New York, NY, USA, 25\u201332.\nthe authors and do not necessarily reflect those of the sponsors.                                  https://doi.org/10.1145/1008992.1009000\n                                                                                              [11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-\n                                                                                                   CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing\nREFERENCES                                                                                         Cold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con-\n                                                                                                   ference on Research and Development in Information Retrieval (Virtual Event,\n [1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:\n                                                                                                   China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA,\n     A Test Collection for Narrative-Driven Point of Interest Recommendation. In\n                                                                                                   1251\u20131260. https://doi.org/10.1145/3397271.3401038\n     Proceedings of the 44th International ACM SIGIR Conference on Research and\n                                                                                              [12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun\n     Development in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As-\n                                                                                                   Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data\n     sociation for Computing Machinery, New York, NY, USA, 2478\u20132484. https:\n                                                                                                   Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n     //doi.org/10.1145/3404835.3463243\n                                                                                                   USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n [2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and\n                                                                                                   1012\u20131020. https://doi.org/10.1145/3543507.3583341\n     Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in\n                                                                                              [13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study\n     Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference\n                                                                                                   for Understanding Users\u2019 Attitudes Towards a Conversational Agent for News\n     on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/\n                                                                                                   Recommendation. In Proceedings of the 4th Conference on Conversational User\n     doi/10.1145/3406522.3446021\n                                                                                                   Interfaces (Glasgow, United Kingdom) (CUI \u201922). Association for Computing\n [3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018.\n                                                                                                   Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/\n     \u201cWhat was this Movie About this Chick?\u201d A Comparative Study of Relevance\n                                                                                                   3543829.3544530\n     Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter-\n                                                                                              [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\n     national Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings\n                                                                                                   Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\n     13. Springer, 323\u2013334.\n                                                                                                   instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n [4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019.\n                                                                                              [15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\n     \u201cLooking for an amazing game I can relax and sink hours into...\u201d: A Study of\n                                                                                                   Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot\n     Relevance Aspects in Video Game Discovery. In Information in Contemporary\n\n\n\n\n                                                                                        782\n\fLarge Language Model Augmented Narrative Driven Recommendations                                                          RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\n\n     Dense Retrieval From 8 Examples. In The Eleventh International Conference on                  Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,\n     Learning Representations. https://openreview.net/forum?id=gmL46YMpu2J                         A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\n[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.                       Inc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/\n     Google News Personalization: Scalable Online Collaborative Filtering. In Pro-                 b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n     ceedings of the 16th International Conference on World Wide Web (Banff, Alberta,         [34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,\n     Canada) (WWW \u201907). Association for Computing Machinery, New York, NY, USA,                    and Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces\n     271\u2013280. https://doi.org/10.1145/1242572.1242610                                              That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on\n[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,                Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR\n     Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi                \u201921). Association for Computing Machinery, New York, NY, USA, 261\u2013265. https:\n     Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of                     //doi.org/10.1145/3406522.3446035\n     the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys              [35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues\n     \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https:                 Bouchard. 2023. Improving Content Retrievability in Search with Controllable\n     //doi.org/10.1145/1864708.1864770                                                             Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating                     USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n     Narrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th                  3182\u20133192. https://doi.org/10.1145/3543507.3583261\n     International Conference on Intelligent User Interfaces (Marina del Ray, California)     [36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin.\n     (IUI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https:               2022. On Natural Language User Profiles for Transparent and Scrutable Rec-\n     //doi.org/10.1145/3301275.3302287                                                             ommendation. In Proceedings of the 45th International ACM SIGIR Conference\n[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot                   on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR\n     Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).             \u201922). Association for Computing Machinery, New York, NY, USA, 2863\u20132874.\n[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context                   https://doi.org/10.1145/3477495.3531873\n     Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom-                 [37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\n     mender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing                     using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\n     Machinery, New York, NY, USA, 9\u201316. https://doi.org/10.1145/2507157.2507187                   pirical Methods in Natural Language Processing. Association for Computational\n[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M               Linguistics. https://arxiv.org/abs/1908.10084\n     Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In               [38] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance\n     TREC.                                                                                         Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389.\n[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-                  https://doi.org/10.1561/1500000019\n     janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-              [39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin\n     mation Retrieval with Contrastive Learning. Transactions on Machine Learning                  Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.\n     Research (2022). https://openreview.net/forum?id=jKN1pXi7b0                                   UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation\n[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,                of Rerankers. arXiv:2303.00807 [cs.IR]\n     Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as            [40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\n     Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820                      Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\n[24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo                      with Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\n     Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016.                     Empirical Methods in Natural Language Processing. Association for Computational\n     Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul-               Linguistics, Abu Dhabi, United Arab Emirates, 3781\u20133797. https://aclanthology.\n     tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa            org/2022.emnlp-main.249\n     Gon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato,            [41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked\n     and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351\u2013370.                    and Permuted Pre-training for Language Understanding. In Advances in Neural\n[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski,                   Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_\n     Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data                  files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf\n     for Conversational Music Recommendation Using Random Walks and Language                  [42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via\n     Models. arXiv:2301.11489                                                                      Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual\n[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of-                International ACM SIGIR Conference on Research and Development in Information\n     Interest Recommendation by Mining Users\u2019 Preference Transition. In Proceedings                Retrieval (Salvador, Brazil) (SIGIR \u201905). Association for Computing Machinery,\n     of the 22nd ACM International Conference on Information & Knowledge Manage-                   New York, NY, USA, 449\u2013456. https://doi.org/10.1145/1076034.1076111\n     ment (San Francisco, California, USA) (CIKM \u201913). Association for Computing Ma-          [43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic\n     chinery, New York, NY, USA, 733\u2013738. https://doi.org/10.1145/2505515.2505639                  Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender\n[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An                           Systems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association for\n     Experimental Evaluation of Point-of-Interest Recommendation in Location-Based                 Computing Machinery, New York, NY, USA, 86\u201394. https://doi.org/10.1145/\n     Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010\u20131021. https://doi.                 3240323.3240369\n     org/10.14778/3115404.3115407                                                             [44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.\n[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and                Controllable Gradient Item Retrieval. In Web Conference.\n     Lucas Dixon. 2021. Augmenting the user-item graph with textual similarity                [45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\n     models. arXiv preprint arXiv:2109.09358 (2021).                                               and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug-\n[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue                        mentation. In Proceedings of the 25th ACM SIGKDD International Conference\n     Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In                on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). As-\n     Proceedings of the 17th Conference of the European Chapter of the Association for             sociation for Computing Machinery, New York, NY, USA, 548\u2013556. https:\n     Computational Linguistics. Association for Computational Linguistics, Dubrovnik,              //doi.org/10.1145/3292500.3330873\n     Croatia, 2799\u20132829. https://aclanthology.org/2023.eacl-main.206                          [46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized\n[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear                   Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM\n     Critiquing for Conversational Recommender Systems. In The Web Conference.                     Conference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association\n[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot             for Computing Machinery, New York, NY, USA, 502\u2013505. https://doi.org/10.\n     Neural Passage Retrieval via Domain-targeted Synthetic Question Generation.                   1145/3523227.3547394\n     In Proceedings of the 16th Conference of the European Chapter of the Associa-            [47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng.\n     tion for Computational Linguistics: Main Volume. Association for Computational                2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users\n     Linguistics, Online, 1075\u20131088. https://doi.org/10.18653/v1/2021.eacl-main.92                 in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\n[32] Sheshera Mysore, Tim O\u2019Gorman, Andrew McCallum, and Hamed Zamani. 2021.                       USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n     CSFCube - A Test Collection of Computer Science Research Articles for Faceted                 1396\u20131404. https://doi.org/10.1145/3543507.3583538\n     Query by Example. In Thirty-fifth Conference on Neural Information Processing            [48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con-\n     Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv.              versational information seeking. arXiv preprint arXiv:2201.08808 (2022).\n     2103.12906                                                                               [49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela                  Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer-\n     Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John                        ence on Research and Development in Information Retrieval (Virtual Event, China)\n     Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda                     (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 881\u2013890.\n     Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.                    https://doi.org/10.1145/3397271.3401180\n     Training language models to follow instructions with human feedback. In\n\n\n\n\n                                                                                        783\n\f", "pdf_url": "https://drive.google.com/drive/folders/1ihkCemv6KY7552m09rpTT7ctsyV6j7QH", "references": ["\nAbhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google News Personalization: Scalable Online Collaborative Filtering. In Proceedings of the 16th International Conference on World Wide Web (Banff, Alberta, Canada) (WWW \u201907) . Association for Computing Machinery, New York, NY, USA, 271\u2013280. https://doi.org/10.1145/1242572.1242610 ", " James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https://doi.org/10.1145/1864708.1864770 ", " Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating Narrative-Driven Movie Recommendations on Reddit. In Proceedings of"], "publish_date": "2023-09-18", "approved": false}}]}}