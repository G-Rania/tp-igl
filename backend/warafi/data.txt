                 Large Language Model Augmented Narrative Driven
                                Recommendations
                  Sheshera Mysore                                              Andrew McCallum                                              Hamed Zamani
          smysore@cs.umass.edu                                           mccallum@cs.umass.edu                                       hzamani@cs.umass.edu
    University of Massachusetts Amherst                             University of Massachusetts Amherst                        University of Massachusetts Amherst
                    USA                                                             USA                                                        USA
ABSTRACT                                                                                              interactions are effective, users soliciting recommendations often
Narrative-driven recommendation (NDR) presents an information                                         start with a vague idea about their desired target items or may
access problem where users solicit recommendations with verbose                                       desire recommendations depending on the context of use, often
descriptions of their preferences and context, for example, travelers                                 missing in historical interaction data (Figure 1). In these scenarios,
soliciting recommendations for points of interest while describ-                                      it is common for users to solicit recommendations through long-
ing their likes/dislikes and travel circumstances. These requests                                     form narrative queries describing their broad interests and context.
are increasingly important with the rise of natural language-based                                    Information access tasks like these have been studied as narrative-
conversational interfaces for search and recommendation systems.                                      driven recommendations (NDR) for items ranging from books [5]
However, NDR lacks abundant training data for models, and current                                     and movies [18], to points of interest [1]. Bogers and Koolen [5]
platforms commonly do not support these requests. Fortunately,                                        note these narrative requests to be common on discussion forums
classical user-item interaction datasets contain rich textual data,                                   and several subreddits1 , but, there is a lack of support for these
e.g., reviews, which often describe user preferences and context                                      complex natural language queries in current recommenders.
â€“ this may be used to bootstrap training for NDR models. In this                                          However, with the emergence of conversational interfaces for
work, we explore using large language models (LLMs) for data                                          information access tasks, support for complex NDR tasks is likely
augmentation to train NDR models. We use LLMs for authoring                                           to become necessary. In this context, recent work has noted an
synthetic narrative queries from user-item interactions with few-                                     increase in complex and subjective natural language requests com-
shot prompting and train retrieval models for NDR on synthetic                                        pared to more conventional search interfaces [13, 34]. Furthermore,
queries and user-item interaction data. Our experiments demon-                                        the emergence of large language models (LLM) with strong lan-
strate that this is an effective strategy for training small-parameter                                guage understanding capabilities presents the potential for fulfilling
retrieval models that outperform other retrieval and LLM baselines                                    such complex requests [9, 33]. This work explores the potential for
for narrative-driven recommendation.                                                                  re-purposing historical user-item recommendation datasets, tra-
                                                                                                      ditionally used for training collaborative filtering recommenders,
CCS CONCEPTS                                                                                          with LLMs to support NDR.
                                                                                                          Specifically, given a userâ€™s interactions, ğ·ğ‘¢ , with items and
â€¢ Information systems â†’ Recommender systems; Users and inter-
                                                                                                      their accompanying text documents (e.g., reviews, descriptions)
active retrieval; â€¢ Computing methodologies â†’ Natural language                                                   ğ‘ğ‘¢
generation.                                                                                           ğ·ğ‘¢ = {ğ‘‘ğ‘– }ğ‘–=1  , selected from a user-item interaction dataset I, we
                                                                                                      prompt InstructGPT, a 175B parameter LLM, to author a synthetic
ACM Reference Format:                                                                                 narrative query ğ‘ğ‘¢ based on ğ·ğ‘¢ (Figure 2). Since we expect the
Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large
                                                                                                      query ğ‘ğ‘¢ to be noisy and not fully representative of all the user
Language Model Augmented Narrative Driven Recommendations. In Sev-
enteenth ACM Conference on Recommender Systems (RecSys â€™23), Septem-
                                                                                                      reviews, ğ·ğ‘¢ is filtered to retain only a fraction of the reviews based
ber 18â€“22, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages.                               on a language-model assigned likelihood of ğ‘ğ‘¢ given a user doc-
https://doi.org/10.1145/3604915.3608829                                                               ument, ğ‘‘ğ‘– . Then, a pre-trained LM based retrieval model (110M
                                                                                                      parameters) is fine-tuned for retrieval on the synthetic queries and
1     INTRODUCTION                                                                                    filtered reviews.
                                                                                                          Our approach, which we refer to as Mint2 , follows from the
Recommender systems personalized to users are an important com-
                                                                                                      observation that while narrative queries and suggestions are often
ponent of several industry-scale platforms [16, 17, 46]. These sys-
                                                                                                      made in online discussion forums, and could serve as training data,
tems function by inferring usersâ€™ interests from their prior inter-
                                                                                                      the number of these posts and the diversity of domains for which
actions on the platform and making recommendations based on
                                                                                                      they are available is significantly smaller than the size and diversity
these inferred interests. While recommendations based on historical
                                                                                                      of passively gathered user-item interaction datasets. E.g. while
Permission to make digital or hard copies of all or part of this work for personal or                 Bogers and Koolen [5] note nearly 25,000 narrative requests for
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation             books on the LibraryThing discussion forum, a publicly available
on the first page. Copyrights for components of this work owned by others than the                    user-item interaction dataset for Goodreads contains interactions
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
                                                                                                      with nearly 2.2M books by 460k users [43] .
and/or a fee. Request permissions from permissions@acm.org.                                               We empirically evaluate Mint in a publicly available test collec-
RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore                                               tion for point of interest recommendation: pointrec [1]. To train
Â© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
                                                                                                      1 r/MovieSuggestions, r/booksuggestions, r/Animesuggest
ACM ISBN 979-8-4007-0241-9/23/09. . . $15.00
https://doi.org/10.1145/3604915.3608829                                                               2 Mint: Data augMentation with INteraction narraTives.




                                                                                                777
RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore                                                                  Mysore, McCallum, Zamani




Figure 1: An example narrative query soliciting point of                      Figure 2: The format of the prompt used in Mint for
interest recommendations. The query describes the users                       generating synthetic narrative queries from user-item
preferences and the context of their request.                                 interaction with a large language model.


our NDR models, we generate synthetic training data based on                  Besides creating queries for ad-hoc retrieval tasks, concurrent
user-item interaction datasets from Yelp. Models (110M parameters)         work of Leszczynski et al. [25] has also explored the creation of syn-
trained with Mint significantly outperform several baseline models         thetic conversational search datasets from music recommendation
and match the performance of significantly larger LLM baselines            datasets with LLMs. The synthetic queries and user documents are
autoregressively generating recommendations. Code and synthetic            then used to train bi-encoder retrieval models for conversational
datasets are available:3                                                   search. Our work resembles this in creating synthetic queries from
                                                                           sets of user items found in recommendation interaction datasets.
2    RELATED WORK                                                          However, it differs in the task of focus, creating long-form narra-
Data Augmentation for Information Access. A line of recent                 tive queries for NDR. Finally, our work also builds on the recent
work has explored using language models to generate synthetic              perspective of Radlinski et al. [36] who make a case for natural
queries for data augmentation to train models for information re-          language user profiles driving recommenders â€“ narrative requests
trieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of     tie closely to natural language user profiles. Our work presents a
interest, a pre-trained language model is used to create synthetic         step toward these systems.
queries for the document collection. An optional filtering step ex-           Finally, while our work explores data augmentation from user-
cludes noisy queries, and finally, a bi-encoder or a cross-encoder is      item interactions for a retrieval-oriented NDR task, prior work has
trained for the retrieval task. While earlier work of Ma et al. [31]       also explored data augmentation of the user-item graph for training
train a custom query generation model on web-text datasets, more           collaborative filtering models. This work has often explored aug-
recent work has leveraged large language models for zero/few-shot          mentation to improve recommendation performance for minority
question generation [7, 8, 15, 23]. In generating synthetic queries,       [12, 47] or cold-start users [11, 28, 45]. And has leveraged genera-
this work indicates the effectiveness of smaller parameter LLMs            tive models [11, 45] and text similarity models [28] for augmenting
(up to 6B parameters) for generating synthetic queries in simpler          the user-item graph.
information-retrieval tasks [7, 8, 23], and finds larger models (100B         Complex Queries in Information Access. With the advent
parameters and above) to be necessary for harder tasks such as             of performant models for text understanding, focus on complex
argument retrieval [15, 23]. Similar to this work, we explore the          and interactive information access tasks has seen a resurgence
generation of synthetic queries with LLMs for a retrieval task. Un-        [2, 29, 32, 48]. NDR presents an example of this â€“ NDR was first
like this work, we demonstrate a data augmentation method for              formalized in Bogers and Koolen [5] for the case of book recommen-
creating effective training data from sets of user documents found in      dation and subsequently studied in other domains [3, 4, 6]. Bogers
recommendation datasets rather than individual documents. Other            and Koolen [5] systematically examined narrative requests posted
work in this space has also explored training more efficient multi-        by users on discussion forums. They defined NDR as a task requir-
vector models from synthetic queries instead of more expensive             ing item recommendation based on a long-form narrative query
cross-encoder models [39] and generating queries with a diverse            and prior-user item interactions. While this formulation resembles
range of intents than the ones available in implicit feedback datasets     personalized search [42] and query-driven recommendation [20],
to enhance item retrievability [35].                                       the length and complexity of requests differentiate these from NDR.
                                                                           Other work has also demonstrated the effectiveness of re-ranking
                                                                           initial recommendations from collaborative filtering approaches
3 https://github.com/iesl/narrative-driven-rec-mint/




                                                                     778
Large Language Model Augmented Narrative Driven Recommendations                                          RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore




Figure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering
models for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with
a large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic
queries and user items.


based on the narrative query [18]. More recent work of Afzali et al.            3.2.1 Narrative Queries from LLMs. To author a narrative query ğ‘ğ‘¢
[1] formulate the NDR task without access to the prior interactions             for a user in I, we make use of the 175B parameter InstructGPT4
of a user while also noting the value of contextual cues contained              model as our query generation model QGen. We include the text
                                                                                                         ğ‘ğ‘¢
in the narrative request. In our work, we focus on this latter for-             of interacted items {ğ‘‘ğ‘– }ğ‘–=1 in the prompt for QGen, and instruct it
mulation of NDR, given the lack of focus on effectively using the               to author a narrative query (Figure 2). To improve the coherence
rich narrative queries in most prior work. Further, we demonstrate              of generated queries and obtain correctly formatted outputs, we
the usefulness of data augmentation from LLMs and user-item                     manually author narrative queries for 3 topically diverse users
interaction datasets lacking narrative queries.                                 based on their interacted items and include it in the prompt for
   Besides this, a range of work has explored more complex, long-               QGen. The same three few shot examples are used for the whole
form, and interactive query formulations for information access;                dataset I, and the three users were chosen from I. Generating
these resemble queries in NDR. Arguello et al. [2] define the tip of            narrative queries based on user interactions may also be considered
tongue retrieval task, a known-item search task where user queries              a form of multi-document summarization for generating a natural
describe the rich context of items while being unable to recall item            language user profile [36].
metadata itself. Mysore et al. [32] formulate an aspect conditional
query-by example task where results must match specific aspects of              3.2.2 Filtering Items for Synthetic Queries. Since we expect user
a long natural language query. And finally, a vibrant body of work              items to capture multiple aspects of their interests and generated
has explored conversational critiquing of recommenders where nat-               queries to only capture a subset of these interests, we only retain
                                                                                                                    ğ‘ğ‘¢
ural language feedback helps tune the recommendations received                  some of the items present in {ğ‘‘ğ‘– }ğ‘–=1   before using it for training re-
by users [30, 44, 49].                                                          trieval models. For this, we use a pre-trained language model to com-
                                                                                pute the likelihood of the query given each user item, ğ‘ƒğ¿ğ‘€ (ğ‘ğ‘¢ |ğ‘‘ğ‘– ),
3 METHOD                                                                        and only retain the top ğ‘€ highly scoring item for ğ‘ğ‘¢ , this re-
                                                                                sults in ğ‘€ training samples per user for our NDR retrieval models:
3.1 Problem Setup                                                                          ğ‘€ }. In our experiments, we use FlanT5 with 3B parame-
                                                                                {(ğ‘ğ‘¢ , ğ‘‘ğ‘– )ğ‘–=1
In our work, we define narrative-driven recommendation (NDR) to                 ters [14] for computing and follow Sachan et al. [40] for computing
be a ranking task, where given a narrative query ğ‘ made by a user               ğ‘ƒğ¿ğ‘€ (ğ‘ğ‘¢ |ğ‘‘ğ‘– ). Note that our use of ğ‘ƒğ¿ğ‘€ (ğ‘ğ‘¢ |ğ‘‘ğ‘– ) represents a query-
ğ‘¢, a ranking system ğ‘“ must generate a ranking ğ‘… over a collection               likelihood model classically used for ad-hoc search and recently
of items C. Further, we assume access to a user-item interaction                shown to be an effective unsupervised re-ranking method when
                                                              ğ‘ğ‘¢
dataset I consisting of user interactions with items (ğ‘¢, {ğ‘‘ğ‘– }ğ‘–=1 ). We         used with large pre-trained language models [40].
assume the items ğ‘‘ğ‘– to be textual documents like reviews or item
descriptions. While we donâ€™t assume there to be any overlap in the              3.2.3 Training Retrieval Models. We train bi-encoder and cross-
users making narrative queries or the collection of items C and the             encoder models for NDR on the generated synthetic dataset â€“ com-
user-items interaction dataset I, we assume them to be from the                 monly used models in search tasks. Bi-encoders are commonly used
same broad domain, e.g., books, movies, points-of-interest.                     as scalable first-stage rankers from a large collection of items. On the
                                                                                other hand, cross-encoders allow a richer interaction between query
3.2    Proposed Method                                                          and item and are used as second-stage re-ranking models. For both
                                                                                models, we use a pre-trained transformer language model architec-
Our proposed method, Mint, for NDR, re-purposes a dataset of                    ture with 110M parameters, MPnet, a model similar to Bert [41].
                                                           ğ‘ğ‘¢
abundantly available user-item interactions, I = {(ğ‘¢, {ğ‘‘ğ‘– }ğ‘–=1 )} into          Bi-encoder models embed the query and item independently into
training data for retrieval models by using LLMs as query gener-                high dimensional vectors: qğ‘¢ = MPNet(ğ‘ğ‘¢ ), dğ‘– = MPNet(ğ‘‘ğ‘– ) and
                                                                ğ‘ğ‘¢
ation models to author narrative queries ğ‘ğ‘¢ : D = {(ğ‘ğ‘¢ , {ğ‘‘ğ‘– }ğ‘–=1   )}.         rank items for the user based on the minimum L2 distance between
Then, retrieval models are trained on the synthetic dataset D (Fig-
ure 3).                                                                         4 https://platform.openai.com/docs/models/gpt-3, text-davinci-003




                                                                          779
RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore                                                                   Mysore, McCallum, Zamani


qğ‘¢ and dğ‘– . Embeddings are obtained by averaging token embeddings           dislikes). It also retains the users whose interests are summarizable
from the final layer of MPNet, and the same model is used for both          by QGen. In the Yelp dataset, this results in 45,193 retained users.
queries and items. Cross-encoder models input both the query and            Now, 10,000 randomly selected users are chosen for generating syn-
item and output a score to be used for ranking ğ‘  = ğ‘“Cr ([ğ‘ğ‘¢; ğ‘‘ğ‘– ]),       thetic narrative queries. For these users, a single randomly selected
where ğ‘“Cr is parameterized as wğ‘‡ dropout Wğ‘‡ MPNet(Â·) . We                   sentence from 10 of their reviews is included in the prompt (Figure
                                                                            2) to QGen, i.e., ğ‘ğ‘¢ = 10. After generating synthetic queries, some
train our bi-encoder model with a margin ranking loss: Lğµğ‘– =
Ã Ãğ‘€                                â€²                                       items are filtered out (Â§3.2.2). Here, we exclude 40% of the items
  ğ‘¢ ğ‘–=1 max[ğ¿2(qğ‘¢ , dğ‘– ) âˆ’ ğ¿2(qğ‘¢ , dğ‘– ) + ğ›¿, 0] with randomly sam-          for a user. This results in about 60,000 training samples for training
                â€²
pled negatives ğ‘‘ and ğ›¿ = 1. Our cross-encoders are trained with             BiEnc-Mint and CrEnc-Mint. These decisions were made manu-
                            Ã Ãğ‘€                ğ‘ 
a cross-entropy loss: Lğ¶ğ‘Ÿ = ğ‘¢ ğ‘–=1       log( Ã ğ‘’ ğ‘  â€² ). For training, 4     ally by examining the resulting datasets and the cost of authoring
                                                      ğ‘‘â€² ğ‘’
negative example items ğ‘‘ â€² are randomly sampled from ranks 100-             queries. The expense of generating ğ‘ğ‘¢ was about USD 230.
300 from our trained bi-encoder. At test time, we retrieve the top
200 items with our trained bi-encoder and re-rank them with the             4.1.3 Baselines. We compare BiEnc-Mint and CrEnc-Mint mod-
cross-encoder - we evaluate both these components in experiments            els against several standard and performant retrieval model base-
and refer to them as BiEnc-Mint and CrEnc-Mint.                             lines. These span zero-shot/unsupervised rankers, supervised bi-
                                                                            encoders, unsupervised cross-encoders, and LLM baselines. BM25:
4     EXPERIMENTS AND RESULTS                                               A standard unsupervised sparse retrieval baseline based on term
                                                                            overlap between query and document, with strong generalization
Next, we evaluate Mint on a publicly available test collection for
                                                                            performance across tasks and domains [38]. Contriver: A BERT-base
NDR and present a series of ablations.
                                                                            bi-encoder model pre-trained for zero-shot retrieval with weakly su-
                                                                            pervised query-document pairs [22]. MPNet-1B: A strong Sentence-
4.1     Experimental Setup                                                  Bert bi-encoder model initialized with MPNet-base and trained on
4.1.1 Datasets. We perform evaluations on an NDR dataset for                1 billion supervised query-document pairs aggregated from numer-
point-of-interest (POI) recommendation Pointrec [1]. Pointrec               ous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned
contains 112 realistic narrative queries (130 words long) obtained          on supervised question-passage pairs from MSMarco. UPR: A two-
from discussion forums on Reddit and items pooled from baseline             stage approach that retrieves items with a Contriver bi-encoder
rankers. The items are annotated on a graded relevance scale by             and re-ranks the top 200 items with a query-likelihood model using
crowd-workers and/or discussion forum members and further vali-             a FlanT5 model with 3B parameters [14, 40]. This may be seen
dated by the dataset authors. The item collection C in Pointrec             as an unsupervised â€œcross-encoderâ€ model. Grounded LLM: A re-
contains 700k POIs with metadata (category, city) and noisy text            cently proposed two-stage approach which autoregressively gener-
snippets describing the POI obtained from the Bing search engine.           ates ten pseudo-relevant items using an LLM (175B InstructGPT)
For test time ranking, we only rank the candidate items in the city         prompted with the narrative query and generates recommenda-
and request category (e.g., â€œRestaurantsâ€) of the query available in        tions grounded in C by retrieving the nearest neighbors for each
Pointrec - this follows prior practice to exclude clearly irrelevant        generated item using a bi-encoder [19]. We include one few-shot
items [1, 26]. We use user-item interaction datasets from Yelp to           example of a narrative query and recommended items in the prompt
generate synthetic queries for training.5 Note also that we limit our       to the LLM. We run this baseline three times and report average
evaluations to Pointrec since it presents the only publicly avail-          performance across runs. We report NDCG at 5 and 10, MAP, MRR,
able, manually annotated, and candidate pooled test collection for          and Recall at 100 and 200. Finally, our reported results should be
NDR, to our knowledge. Other datasets for NDR use document col-             considered lower bounds on realistic performance due to the un-
lections that are no longer publicly accessible [24], contain sparse        judged documents (about 70% at ğ‘˜ = 10) in our test collections
and noisy relevance judgments due to them being determined with             [10].
automatic rules applied to discussion threads [18, 24], lack pooling
to gather candidates for judging relevance [18, 24], or lack realistic      4.2    Results
narrative queries [21]. We leave the development of more robust
                                                                            Table 1 presents the performance of the proposed method compared
test collections and evaluation methods for NDR to future work.
                                                                            against baselines. Here, bold numbers indicate the best-performing
4.1.2 Implementation Details. Next, we describe important details           model, and superscripts indicate statistical significance computed
for Mint and leave finer details of the model and training to our           with two-sided t-tests at ğ‘ < 0.05.
code release. To sample user interactions for generating synthetic             Here, we first note the performance of baseline approaches. We
queries from the Yelp dataset, we exclude POIs and users with               see BM25 outperformed by Contriver, a transformer bi-encoder
fewer than ten reviews to ensure that users were regular users of           model trained for zero-shot retrieval; this mirrors prior work [22].
the site with well represented interests. This follows common prior         Next, we see supervised bi-encoder models trained on similar pas-
practice in preparing user-item interaction datasets for use [27].          sage (MPNet-1B) and question-answer (BERT-MSM) pairs outper-
Then we retain users who deliver an average rating greater than             form a weakly supervised model (Contriver) by smaller margins.
3/5 and with 10-30 above-average reviews. This desirably biases             Finally, the Grounded LLM outperforms all bi-encoder baselines, in-
our data to users who commonly describe their likings (rather than          dicating strong few-shot generalization and mirroring prior results
                                                                            [19]. Examining the Mint models, we first note that the BiEnc-
5 https://www.yelp.com/dataset                                              Mint sees statistically significant improvement compared to BM25




                                                                      780
Large Language Model Augmented Narrative Driven Recommendations                                       RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore


Table 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts
denote statistically significant improvements compared to specific baseline models.

                                                                                        Pointrec
                              Model     Parameters      NDCG@5     NDCG@10        MAP        MRR          Recall@100      Recall@200
                             1 BM25          -          0.2682     0.2464         0.1182     0.2685       0.4194          0.5429
                         2 Contriver       110M         0.2924     0.2776         0.1660     0.3355       0.4455          0.5552
                        3 MPNet-1B         110M         0.3038     0.2842         0.1621     0.3566       0.4439          0.5657
                      4 BERT-MSM           110M         0.3117     0.2886         0.1528     0.3320       0.4679          0.5816
                  5 Grounded LLM        175B+110M       0.3558     0.3251         0.1808     0.3861       0.4797          0.5797
                               6 UPR     110M+3B        0.3586     0.3242         0.1712     0.4013       0.4489          0.5552
                       BiEnc-Mint          110M         0.34891    0.32631        0.18901    0.39821      0.49141         0.6221
                       CrEnc-Mint         2Ã—110M        0.372512   0.348912       0.219214   0.43171      0.5448123       0.6221


and outperforms the best bi-encoder baselines by 11-13% on preci-                6B LLM for Item Queries. We find a smaller 6B LLM to result
sion measures and 5-7% on recall measures. Specifically, we see a             in poor quality data when used to generate narrative queries con-
                                                                                                ğ‘ğ‘¢
model trained for question-answering (BERT-MSM) underperform                  ditioned on {ğ‘‘ğ‘– }ğ‘–=1  . Here we simplify the text generation task â€“
BiEnc-Mint, indicating the challenge of the NDR task. Further,                using a 6B LLM to generate queries for individual items ğ‘‘ğ‘– . This
BiEnc-Mint, trained on 5 orders of magnitude lesser data than                 experiment also mirrors the setup for generating synthetic queries
MPNet-1B, sees improved performance â€“ indicating the quality of               for search tasks [7, 15]. Here, we use 3-few shot examples and sam-
data obtained from Mint. Furthermore, BiEnc-Mint also performs                ple one item per user for generating ğ‘ğ‘¢ . Given the lower cost of
at par with a 175B LLM while offering the inference efficiency of a           using a smaller LLM, we use all 45,193 users in our Yelp dataset
small-parameter bi-encoder. Next, we see CrEnc-Mint outperform                rather than a smaller random sample. From Table 2, we see that this
the baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM                   results in higher quality queries than using smaller LLMs for gen-
by 4-21% on precision measures and 7-13% on recall measures â€“                                                      ğ‘ğ‘¢
                                                                              erating narrative queries from {ğ‘‘ğ‘– }ğ‘–=1 . The resulting BiEnc model
demonstrating the value of Mint for training NDR models.                      underperforms the BiEnc-Mint, indicating the value of generating
                                                                              complex queries conditioned on multiple items as in Mint for NDR.
4.3    Ablations                                                              We see that CrEnc approaches the performance of CrEnc-Mintâ€“
In Table 2, we ablate various design choices in Mint. Different               note, however, that this approach uses the performant BiEnc-Mint
choices result in different training sets for the BiEnc and CrEnc             for sampling negatives and first stage ranking. We leave further
models. Also, note that in reporting ablation performance for CrEnc,          exploration of using small parameter LLMs for data augmentation
we still use the performant BiEnc-Mint model for obtaining nega-              for NDR models to future work.
tive examples for training and first-stage ranking. Without high-
quality negative examples, we found CrEnc to result in much poorer            5    CONCLUSIONS
performance.                                                                  In this paper, we present Mint, a data augmentation method for the
   No item filtering. Since synthetic queries are unlikely to rep-            narrative-driven recommendation (NDR) task. Mint re-purposes
                                                                  ğ‘ğ‘¢
resent all the items of a user, Mint excludes user items {ğ‘‘ğ‘– }ğ‘–=1             historical user-item interaction datasets for NDR by using a 175B pa-
which have a low likelihood of being generated from the document              rameter large language model to author long-form narrative queries
(Â§3.2.2). Without this step, we expect the training set for training          while conditioning on the text of items liked by users. We evaluate
retrieval models to be larger and noisier. In Table 2, we see that            bi-encoder and cross-encoder models trained on data from Mint on
excluding this step leads to a lower performance for BiEnc and                the publicly available Pointrec test collection for narrative-driven
CrEnc, indicating that the quality of data obtained is important for          point of interest recommendation. We demonstrate that the result-
performance.                                                                  ing models outperform several strong baselines and ablated models
   6B LLM for QGen. Mint relies on using an expensive 175B pa-                and match or outperform a 175B LLM directly used for NDR in a
rameter InstructGPT model for QGen. Here, we investigate the                  1-shot setup.
                                    ğ‘ğ‘¢
efficacy for generating ğ‘ğ‘¢ for {ğ‘‘ğ‘– }ğ‘–=1 with a 6B parameter Instruct-            However, Mint also presents some limitations. Given our use of
GPT model (text-curie-001). We use an identical setup to the                  historical interaction datasets for generating synthetic training data
175B LLM for this. In Table 2, we see that training on the synthetic          and the prevalence of popular interests in these datasets longer,
narrative queries of the smaller LLM results in worse models â€“ of-            tailed interests are unlikely to be present in the generated syn-
ten underperforming the baselines in Table 1. This indicates the              thetic datasets. In turn, causing retrieval models to likely see poorer
inability of a smaller model to generate complex narrative queries            performance on these requests. Our use of LLMs to generate syn-
while conditioning on a set of user items. This necessity of a larger         thetic queries also causes the queries to be repetitive in structure,
LLM for generating queries in complex retrieval tasks has been                likely causing novel longer-tail queries to be poorly served. These
observed in prior work [15, 23].                                              limitations may be addressed in future work.




                                                                      781
RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore                                                                                               Mysore, McCallum, Zamani


                                             Table 2: Mint ablated for different design choices on Pointrec.

                                                                                                      Pointrec
                            Ablation                                NDCG@5          NDCG@10      MAP         MRR        Recall@100        Recall@200
                            BiEnc-Mint                              0.3489          0.3263       0.1890      0.3982     0.5263            0.6221
                            âˆ’ No item filtering                     0.2949          0.2766       0.1634      0.3505     0.4979            0.5951
                            âˆ’ 6B LLM for QGen                       0.2336          0.2293       0.1125      0.2287     0.426             0.5435
                            âˆ’ 6B LLM for Item Queries               0.3012          0.2875       0.1721      0.3384     0.4800            0.5909
                            CrEnc-Mint                              0.3725          0.3489       0.2192      0.4317     0.5448            0.6221
                            âˆ’ No item filtering                     0.3570          0.3379       0.2071      0.4063     0.5366            0.6221
                            âˆ’ 6B LLM for QGen                       0.2618          0.2421       0.1341      0.3118     0.4841            0.6221
                            âˆ’ 6B LLM for Item Queries               0.3792          0.3451       0.2128      0.4098     0.5546            0.6221


   Besides this, other avenues also present rich future work. While                                Society: 14th International Conference, iConference 2019, Washington, DC, USA,
Mint leverages a 175B LLM for generating synthetic queries, smaller                                March 31â€“April 3, 2019, Proceedings 14. Springer, 503â€“515.
                                                                                               [5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven
parameter LLMs may be explored for this purpose - perhaps by                                       Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender
training dedicated QGen models. Mint may also be expanded to                                       Systems (Como, Italy) (RecSys â€™17). Association for Computing Machinery, New
                                                                                                   York, NY, USA, 238â€“242. https://doi.org/10.1145/3109859.3109893
explore more active strategies for sampling items and users for                                [6] Toine Bogers and Marijn Koolen. 2018. â€œIâ€™m looking for something like. . . â€:
whom narrative queries are authored - this may allow more effi-                                    Combining Narratives and Example Items for Narrative-driven Book Recommen-
cient use of large parameter LLMs while ensuring higher quality                                    dation. In Knowledge-aware and Conversational Recommender Systems Workshop.
                                                                                                   CEUR Workshop Proceedings.
training datasets. Next, the generation of synthetic queries from                              [7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.
sets of documents may be explored for a broader range of retrieval                                 InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings
tasks beyond NDR given its promise to generate larger training                                     of the 45th International ACM SIGIR Conference on Research and Development
                                                                                                   in Information Retrieval (Madrid, Spain) (SIGIR â€™22). Association for Computing
sets â€“ a currently underexplored direction. Finally, given the lack of                             Machinery, New York, NY, USA, 2387â€“2392. https://doi.org/10.1145/3477495.
larger-scale test collections for NDR and the effectiveness of LLMs                                3531863
                                                                                               [8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,
for authoring narrative queries from user-item interaction, fruitful                               Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-
future work may also explore the creation of larger-scale datasets                                 pervised Training of Efficient Rankers. arXiv:2301.02998
in a mixed-initiative setup to robustly evaluate models for NDR.                               [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
                                                                                                   Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
                                                                                                   Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
                                                                                                   Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,
ACKNOWLEDGMENTS                                                                                    Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
We thank anonymous reviewers for their invaluable feedback. This                                   Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
                                                                                                   Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
work was partly supported by the Center for Intelligent Informa-                                   In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-
tion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of                                  zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,
Naval Research contract number N000142212688, an Amazon Alexa                                      Inc., 1877â€“1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/
                                                                                                   1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
Prize grant, and the Chan Zuckerberg Initiative under the project                             [10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete
Scientific Knowledge Base Construction. Any opinions, findings                                     Information. In Proceedings of the 27th Annual International ACM SIGIR Conference
                                                                                                   on Research and Development in Information Retrieval (Sheffield, United Kingdom)
and conclusions or recommendations expressed here are those of                                     (SIGIR â€™04). Association for Computing Machinery, New York, NY, USA, 25â€“32.
the authors and do not necessarily reflect those of the sponsors.                                  https://doi.org/10.1145/1008992.1009000
                                                                                              [11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-
                                                                                                   CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing
REFERENCES                                                                                         Cold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con-
                                                                                                   ference on Research and Development in Information Retrieval (Virtual Event,
 [1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:
                                                                                                   China) (SIGIR â€™20). Association for Computing Machinery, New York, NY, USA,
     A Test Collection for Narrative-Driven Point of Interest Recommendation. In
                                                                                                   1251â€“1260. https://doi.org/10.1145/3397271.3401038
     Proceedings of the 44th International ACM SIGIR Conference on Research and
                                                                                              [12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun
     Development in Information Retrieval (Virtual Event, Canada) (SIGIR â€™21). As-
                                                                                                   Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data
     sociation for Computing Machinery, New York, NY, USA, 2478â€“2484. https:
                                                                                                   Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,
     //doi.org/10.1145/3404835.3463243
                                                                                                   USA) (WWW â€™23). Association for Computing Machinery, New York, NY, USA,
 [2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and
                                                                                                   1012â€“1020. https://doi.org/10.1145/3543507.3583341
     Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in
                                                                                              [13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study
     Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference
                                                                                                   for Understanding Usersâ€™ Attitudes Towards a Conversational Agent for News
     on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/
                                                                                                   Recommendation. In Proceedings of the 4th Conference on Conversational User
     doi/10.1145/3406522.3446021
                                                                                                   Interfaces (Glasgow, United Kingdom) (CUI â€™22). Association for Computing
 [3] Toine Bogers, Maria GÃ¤de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018.
                                                                                                   Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/
     â€œWhat was this Movie About this Chick?â€ A Comparative Study of Relevance
                                                                                                   3543829.3544530
     Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter-
                                                                                              [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
     national Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings
                                                                                                   Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling
     13. Springer, 323â€“334.
                                                                                                   instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).
 [4] Toine Bogers, Maria GÃ¤de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019.
                                                                                              [15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
     â€œLooking for an amazing game I can relax and sink hours into...â€: A Study of
                                                                                                   Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot
     Relevance Aspects in Video Game Discovery. In Information in Contemporary




                                                                                        782
Large Language Model Augmented Narrative Driven Recommendations                                                          RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore


     Dense Retrieval From 8 Examples. In The Eleventh International Conference on                  Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,
     Learning Representations. https://openreview.net/forum?id=gmL46YMpu2J                         A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,
[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.                       Inc., 27730â€“27744. https://proceedings.neurips.cc/paper_files/paper/2022/file/
     Google News Personalization: Scalable Online Collaborative Filtering. In Pro-                 b1efde53be364a73914f58805a001731-Paper-Conference.pdf
     ceedings of the 16th International Conference on World Wide Web (Banff, Alberta,         [34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,
     Canada) (WWW â€™07). Association for Computing Machinery, New York, NY, USA,                    and Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces
     271â€“280. https://doi.org/10.1145/1242572.1242610                                              That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on
[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,                Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR
     Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi                â€™21). Association for Computing Machinery, New York, NY, USA, 261â€“265. https:
     Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of                     //doi.org/10.1145/3406522.3446035
     the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys              [35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues
     â€™10). Association for Computing Machinery, New York, NY, USA, 293â€“296. https:                 Bouchard. 2023. Improving Content Retrievability in Search with Controllable
     //doi.org/10.1145/1864708.1864770                                                             Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,
[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating                     USA) (WWW â€™23). Association for Computing Machinery, New York, NY, USA,
     Narrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th                  3182â€“3192. https://doi.org/10.1145/3543507.3583261
     International Conference on Intelligent User Interfaces (Marina del Ray, California)     [36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin.
     (IUI â€™19). Association for Computing Machinery, New York, NY, USA, 1â€“11. https:               2022. On Natural Language User Profiles for Transparent and Scrutable Rec-
     //doi.org/10.1145/3301275.3302287                                                             ommendation. In Proceedings of the 45th International ACM SIGIR Conference
[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot                   on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR
     Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).             â€™22). Association for Computing Machinery, New York, NY, USA, 2863â€“2874.
[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context                   https://doi.org/10.1145/3477495.3531873
     Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom-                 [37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
     mender Systems (Hong Kong, China) (RecSys â€™13). Association for Computing                     using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-
     Machinery, New York, NY, USA, 9â€“16. https://doi.org/10.1145/2507157.2507187                   pirical Methods in Natural Language Processing. Association for Computational
[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M               Linguistics. https://arxiv.org/abs/1908.10084
     Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In               [38] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance
     TREC.                                                                                         Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333â€“389.
[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-                  https://doi.org/10.1561/1500000019
     janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-              [39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin
     mation Retrieval with Contrastive Learning. Transactions on Machine Learning                  Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.
     Research (2022). https://openreview.net/forum?id=jKN1pXi7b0                                   UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation
[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,                of Rerankers. arXiv:2303.00807 [cs.IR]
     Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as            [40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau
     Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820                      Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval
[24] Marijn Koolen, Toine Bogers, Maria GÃ¤de, Mark Hall, Iris Hendrickx, Hugo                      with Zero-Shot Question Generation. In Proceedings of the 2022 Conference on
     Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016.                     Empirical Methods in Natural Language Processing. Association for Computational
     Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul-               Linguistics, Abu Dhabi, United Arab Emirates, 3781â€“3797. https://aclanthology.
     tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa            org/2022.emnlp-main.249
     GonÃ§alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato,            [41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked
     and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351â€“370.                    and Permuted Pre-training for Language Understanding. In Advances in Neural
[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski,                   Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_
     Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data                  files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf
     for Conversational Music Recommendation Using Random Walks and Language                  [42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via
     Models. arXiv:2301.11489                                                                      Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual
[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of-                International ACM SIGIR Conference on Research and Development in Information
     Interest Recommendation by Mining Usersâ€™ Preference Transition. In Proceedings                Retrieval (Salvador, Brazil) (SIGIR â€™05). Association for Computing Machinery,
     of the 22nd ACM International Conference on Information & Knowledge Manage-                   New York, NY, USA, 449â€“456. https://doi.org/10.1145/1076034.1076111
     ment (San Francisco, California, USA) (CIKM â€™13). Association for Computing Ma-          [43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic
     chinery, New York, NY, USA, 733â€“738. https://doi.org/10.1145/2505515.2505639                  Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender
[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An                           Systems (Vancouver, British Columbia, Canada) (RecSys â€™18). Association for
     Experimental Evaluation of Point-of-Interest Recommendation in Location-Based                 Computing Machinery, New York, NY, USA, 86â€“94. https://doi.org/10.1145/
     Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010â€“1021. https://doi.                 3240323.3240369
     org/10.14778/3115404.3115407                                                             [44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.
[28] Federico LÃ³pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and                Controllable Gradient Item Retrieval. In Web Conference.
     Lucas Dixon. 2021. Augmenting the user-item graph with textual similarity                [45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,
     models. arXiv preprint arXiv:2109.09358 (2021).                                               and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug-
[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue                        mentation. In Proceedings of the 25th ACM SIGKDD International Conference
     Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In                on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD â€™19). As-
     Proceedings of the 17th Conference of the European Chapter of the Association for             sociation for Computing Machinery, New York, NY, USA, 548â€“556. https:
     Computational Linguistics. Association for Computational Linguistics, Dubrovnik,              //doi.org/10.1145/3292500.3330873
     Croatia, 2799â€“2829. https://aclanthology.org/2023.eacl-main.206                          [46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized
[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear                   Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM
     Critiquing for Conversational Recommender Systems. In The Web Conference.                     Conference on Recommender Systems (Seattle, WA, USA) (RecSys â€™22). Association
[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot             for Computing Machinery, New York, NY, USA, 502â€“505. https://doi.org/10.
     Neural Passage Retrieval via Domain-targeted Synthetic Question Generation.                   1145/3523227.3547394
     In Proceedings of the 16th Conference of the European Chapter of the Associa-            [47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng.
     tion for Computational Linguistics: Main Volume. Association for Computational                2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users
     Linguistics, Online, 1075â€“1088. https://doi.org/10.18653/v1/2021.eacl-main.92                 in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,
[32] Sheshera Mysore, Tim Oâ€™Gorman, Andrew McCallum, and Hamed Zamani. 2021.                       USA) (WWW â€™23). Association for Computing Machinery, New York, NY, USA,
     CSFCube - A Test Collection of Computer Science Research Articles for Faceted                 1396â€“1404. https://doi.org/10.1145/3543507.3583538
     Query by Example. In Thirty-fifth Conference on Neural Information Processing            [48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con-
     Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv.              versational information seeking. arXiv preprint arXiv:2201.08808 (2022).
     2103.12906                                                                               [49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based
[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela                  Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer-
     Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John                        ence on Research and Development in Information Retrieval (Virtual Event, China)
     Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda                     (SIGIR â€™20). Association for Computing Machinery, New York, NY, USA, 881â€“890.
     Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.                    https://doi.org/10.1145/3397271.3401180
     Training language models to follow instructions with human feedback. In




                                                                                        783
